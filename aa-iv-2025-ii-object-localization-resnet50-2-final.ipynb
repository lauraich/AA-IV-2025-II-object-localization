{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"collapsed_sections":["5fb56c48","10e7d377","aa19b15d","936367f4","b50e9616","7c3b77ae","f8b680c7","6b239aa5","596c9ea4","e3cdb2f4"],"gpuType":"T4","provenance":[],"toc_visible":true},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":112034,"databundleVersionId":13352295,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":130.408632,"end_time":"2025-09-04T04:16:54.499232","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-04T04:14:44.090600","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"08aae853b9744c288e5eed5564c19210":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_61fdd179158948c990f3437aa165acbc","max":219,"min":0,"orientation":"horizontal","style":"IPY_MODEL_31367be3b99a48fba7ff6db544fbe41d","value":219}},"31367be3b99a48fba7ff6db544fbe41d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a1c6fece878487b947b5c67eb42d64f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61fdd179158948c990f3437aa165acbc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64cb1f5bddf64fe094d33023a96b430b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a1c6fece878487b947b5c67eb42d64f","placeholder":"​","style":"IPY_MODEL_e7a2edc93e834f8d8f3f9e579f2eb892","value":" 219/219 [00:01&lt;00:00, 153.60it/s]"}},"66bc55d656d84d6f8b2ede7602f0dd03":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de8148cf162a4959860c2dae286d033b","placeholder":"​","style":"IPY_MODEL_d5840b73017b4fe48b19fdca20aa024f","value":"100%"}},"782ae4b088f24aed888b5c7e4b4d229e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"943252b5542c467f87f95df33e170aa5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66bc55d656d84d6f8b2ede7602f0dd03","IPY_MODEL_08aae853b9744c288e5eed5564c19210","IPY_MODEL_64cb1f5bddf64fe094d33023a96b430b"],"layout":"IPY_MODEL_782ae4b088f24aed888b5c7e4b4d229e"}},"d5840b73017b4fe48b19fdca20aa024f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de8148cf162a4959860c2dae286d033b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7a2edc93e834f8d8f3f9e579f2eb892":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"5fb56c48","cell_type":"markdown","source":"# Importación de librerias requeridas","metadata":{"id":"5fb56c48","papermill":{"duration":0.014093,"end_time":"2025-09-04T04:14:48.305226","exception":false,"start_time":"2025-09-04T04:14:48.291133","status":"completed"},"tags":[]}},{"id":"ab8a8af5","cell_type":"code","source":"#!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:10:47.670530Z","iopub.execute_input":"2025-09-26T02:10:47.671165Z","iopub.status.idle":"2025-09-26T02:10:47.674953Z","shell.execute_reply.started":"2025-09-26T02:10:47.671137Z","shell.execute_reply":"2025-09-26T02:10:47.674226Z"},"id":"ab8a8af5","papermill":{"duration":0.018442,"end_time":"2025-09-04T04:14:48.337166","exception":false,"start_time":"2025-09-04T04:14:48.318724","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":1},{"id":"7fbd8dc6","cell_type":"code","source":"# -*- coding: utf-8 -*-\nfrom __future__ import print_function, division\n\n# ===========================\n# CONFIGURACIÓN INICIAL\n# ===========================\n\n# tqdm es una librería para mostrar barras de progreso en ciclos (loops).\n# \"tqdm.auto\" detecta si estamos en un notebook (como Kaggle o Jupyter)\n# o en una terminal, y se adapta automáticamente sin mostrar advertencias.\nfrom tqdm.auto import tqdm\ntqdm.pandas()   # Integra tqdm con pandas → se ven barras de progreso en operaciones de pandas.\n\n# ===========================\n# MANEJO DE WEIGHTS & BIASES (wandb)\n# ===========================\n# wandb es una herramienta para registrar experimentos de machine learning.\n# En Kaggle a veces genera errores o no queremos usarlo.\n# Con esta configuración lo desactivamos por defecto y creamos un \"plan B\"\n# para que el código siga funcionando aunque wandb falle o no esté instalado.\n\nimport os\nos.environ.setdefault(\"WANDB_DISABLED\", \"true\")  # Kaggle lo desactiva automáticamente\n\ntry:\n    import wandb  # Intentamos importar wandb\nexcept Exception as e:\n    # Si falla la importación, creamos una clase de \"simulación\"\n    # que actúa como reemplazo básico (stub).\n    # Así, cuando en el código se llame a wandb.init() o wandb.log(),\n    # no se producirá un error.\n    class _WandbStub:\n        def init(self, *args, **kwargs):\n            class _Ctx:\n                def __enter__(self): return self\n                def __exit__(self, exc_type, exc, tb): pass\n            return _Ctx()\n        def log(self, *args, **kwargs): pass\n        def watch(self, *args, **kwargs): pass\n        def finish(self, *args, **kwargs): pass\n\n    wandb = _WandbStub()  # Reemplazamos wandb por el \"stub\"\n\n# ===========================\n# IMPORTACIÓN DE LIBRERÍAS\n# ===========================\n# Estas librerías cubren diferentes tareas:\n# - Numpy/Pandas: análisis de datos\n# - PyTorch/Torchvision: redes neuronales\n# - Albumentations: aumentación de imágenes\n# - Scikit-learn: partición de datos\n# - OpenCV/Skimage: procesamiento de imágenes\n# - Matplotlib: visualización\n\nimport numpy as np\nimport pandas as pd\nfrom numpy.typing import NDArray\nfrom functools import reduce\nfrom itertools import islice, chain\nimport math, copy\n\nfrom PIL import Image\n\nimport torch\nfrom torch import nn, Tensor\nfrom torch.optim import Optimizer\nimport torch.nn.functional as F\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom torchsummary import summary  # Muestra la arquitectura de la red\n\n# Albumentations: librería de aumentación de datos para imágenes\nimport albumentations as A\n\nfrom sklearn.model_selection import train_test_split\nfrom multiprocessing import cpu_count  # Para paralelizar procesos\n\nimport os.path as osp\nfrom skimage import io, transform\nimport matplotlib.pyplot as plt\nimport typing as ty\nimport cv2\n\nplt.ion()  # Activa el \"modo interactivo\" → las gráficas se actualizan automáticamente.\n\n# ===========================\n# EXPLORACIÓN DE DATOS EN KAGGLE\n# ===========================\n# Kaggle guarda los datasets en la carpeta \"/kaggle/input\".\n# El siguiente bloque recorre esa carpeta y muestra los primeros 10 archivos encontrados.\n# Esto ayuda a verificar qué datos tenemos disponibles sin abrir manualmente el explorador.\nfor root, dirs, filenames in os.walk('/kaggle/input'):\n    for i, filepath in enumerate(filenames):\n        if i >= 10:  # Solo mostramos hasta 10 archivos para no saturar la salida\n            print()\n            break\n        print(osp.join(root, filepath))\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-09-26T02:10:47.676205Z","iopub.execute_input":"2025-09-26T02:10:47.676456Z","iopub.status.idle":"2025-09-26T02:10:59.952792Z","shell.execute_reply.started":"2025-09-26T02:10:47.676432Z","shell.execute_reply":"2025-09-26T02:10:59.952107Z"},"id":"7fbd8dc6","papermill":{"duration":47.755478,"end_time":"2025-09-04T04:15:36.105241","exception":false,"start_time":"2025-09-04T04:14:48.349763","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/aa-iv-2025-ii-object-localization/sample_submission.csv\n/kaggle/input/aa-iv-2025-ii-object-localization/train.csv\n/kaggle/input/aa-iv-2025-ii-object-localization/test.csv\n/kaggle/input/aa-iv-2025-ii-object-localization/images/videoplayback-1-_mp4-6_jpg.rf.e2195c50e4aa68ffc18f41c80fd7d235.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_4861_mp4-43_jpg.rf.4271f075e21c21ee8dc01731c6a7ea89.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_4861_mp4-56_jpg.rf.2c4bd7a2dd787d0344f2b49af88f21f1.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_3100_mp4-24_jpg.rf.894c40eafa77cd73ff50a69982e3f924.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_4860_mp4-45_jpg.rf.29bb394fd84df979b2a6096746751f42.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_4861_mp4-9_jpg.rf.bcc352b97426c7378bcd8004247f4433.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/video_CDC-YOUTUBE_mp4-41_jpg.rf.4f56be4b40c9775509474d515489f5a5.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_4921-2_mp4-124_jpg.rf.60e2e62f7f6c331d5960bf81261d0d8c.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_0871_MOV-30_jpg.rf.4bca57028e6bc86bf4ed9604468dc336.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_4860_mp4-4_jpg.rf.4ebe7129b4b9ebff2910ba3953e16a6b.jpg\n\n","output_type":"stream"}],"execution_count":2},{"id":"af138bac","cell_type":"markdown","source":"## **Definición de Funciones y Clases**\n\n-   **Visualización de bounding boxes**: Funciones para dibujar cajas delimitadoras y etiquetas de clase sobre imágenes, usando OpenCV.\n-   **Dataset personalizado**:  `maskDataset`  extiende  `torch.utils.data.Dataset`  para manejar imágenes, bounding boxes y clases, permitiendo transformaciones y resize.\n-   **Transformaciones**: Wrappers para convertir imágenes a tensores (`ToTensor`), normalizar canales (`Normalizer`), y aplicar augmentations con Albumentations o torchvision.\n-   **Feature extractor**:  `ResNetFeatureExtractor`  encapsula un modelo ResNet, eliminando la capa final y añadiendo flatten y dropout para extraer embeddings.\n-   **Métricas y pérdidas**: Funciones para calcular IoU, accuracy y una función de pérdida multitarea (clasificación + regresión de bbox).\n-   **Entrenamiento y evaluación**: Funciones para el ciclo de entrenamiento, evaluación periódica y logging de métricas.","metadata":{}},{"id":"d8734fee-000c-4f43-af68-524ca54112ed","cell_type":"markdown","source":"## DEFINICION DE ARQUITECTURA CNN\n\nEl Siguiente fragmento de código define las dos \"cabezas\" principales del modelo multitarea para detección de objetos: la cabeza de clasificación y la cabeza de regresión de bounding box. Ambas están implementadas como secuencias de capas densas (fully connected) con normalización, activaciones y dropout para mejorar la capacidad de generalización y la estabilidad durante el entrenamiento.\n\nLa  **cabeza de clasificación**  (`cls_head`) recibe el vector de características extraído por el backbone (ResNet50_2) y lo transforma a través de varias capas lineales. Primero, reduce la dimensionalidad a 768 unidades, aplica normalización por lotes (BatchNorm1d), una activación SiLU y dropout para evitar el sobreajuste. Este patrón se repite con una reducción a 256 unidades y un dropout más alto, finalizando con una capa lineal que produce los logits para cada clase.\n\nSe utilizaron valores elevados de dropout en la cabeza de clasificación porque esta alcanzaba una alta precisión en pocas iteraciones, lo que indicaba un riesgo de sobreajuste. Al incrementar el dropout, se ralentiza el aprendizaje de la clasificación, permitiendo que la cabeza de regresión tenga más tiempo para optimizarse  Así, se mejora la generalización del modelo y se evita que la clasificación domine el proceso de aprendizaje antes de que la regresión converja adecuadamente.\n\nAdicionalmente, se dio mayor peso a la tarea de regresión en la función de pérdida, utilizando un parámetro alpha de 0.7, y se asignaron learning rates diferenciados para el backbone, la cabeza de clasificación y la cabeza de regresión, favoreciendo un ajuste más fino y específico para cada componente del modelo.\n\n\nLa función de activación **SiLU** (Sigmoid Linear Unit), se utiliza en vez de **ReLU** porque ofrece varias ventajas en redes profundas y tareas de visión por computadora. SiLU es una función suave, esto es el resultado de experimentacion usando diferentes funciones de activacion, con Relu se obtivieron resultados mas bajos\n\n\nLa  **cabeza de regresión**  (`reg_head`) está diseñada para predecir las coordenadas de bbox. Comienza disminuyendo el vector de entrada a la mitad (1024 unidades), seguido de normalización, activación ReLU y dropout. Luego, reduce progresivamente la dimensionalidad a 512 y 256 unidades, manteniendo la normalización y la activación, y finalmente produce cuatro valores que corresponden a las coordenadas normalizadas  `[xmin, ymin, xmax, ymax]`. \n\n\nLa cantidad de capas en la arquitectura se definió como resultado de diferentes experimentos y comparación de resultados, buscando el mejor equilibrio entre capacidad de representación y generalización del modelo.\n\nEn la función de pérdida, además del alpha que pondera la contribución entre la tarea de clasificación y regresión, se asignaron pesos [0.35, 0.65] a las clases con el fin de mitigar el desbalance en la distribución de etiquetas y favorecer el aprendizaje de la clase minoritaria. ","metadata":{}},{"id":"d01a5651-2d4b-401b-aac9-94812e3defeb","cell_type":"code","source":"def get_output_shape(model: nn.Module, image_dim: ty.Tuple[int, int, int]):\n    # ===========================================================\n    # UTILIDAD: INFERIR LA FORMA DE SALIDA DEL BACKBONE\n    # -----------------------------------------------------------\n    # - Recibe un 'model' (ej. nuestro extractor VGG16) ya movido a 'device'.\n    # - 'image_dim' debe incluir el batch (B, C, H, W). En el uso típico abajo\n    #   se pasa como [1, *input_shape] → [1, 3, 640, 640].\n    # - Crea un tensor aleatorio con esa forma, lo pasa por el modelo\n    #   y devuelve la 'shape' resultante (solo para inspección).\n    # ===========================================================\n    return model(torch.rand(*(image_dim)).to(device)).data.shape\n\n\nclass Model(nn.Module):\n    def __init__(self, input_shape: ty.Tuple[int, int, int] = (3, 256, 256), n_classes: int = 2):\n        \"\"\"\n        MODELO MULTI-TAREA (CLASIFICACIÓN + REGRESIÓN DE BBOX)\n\n        Entradas\n        --------\n        input_shape : (C, H, W)\n            Tamaño esperado de la imagen (usamos 3×640×640).\n            Formato PyTorch: canal primero (C,H,W).\n        n_classes : int\n            Nº de clases para clasificación (en este curso: 2 → no-mask / mask).\n\n        Descripción\n        -----------\n        El modelo tiene:\n          • Un 'backbone' convolucional (preentrenado) que extrae un vector de\n            características (features) a partir de la imagen.\n          • Una cabeza de CLASIFICACIÓN (cls_head) que toma ese vector y produce\n            'logits' por clase (forma [B, n_classes]).\n          • Una cabeza de REGRESIÓN (reg_head) que toma el mismo vector y predice\n            las 4 coordenadas de la caja (forma [B, 4]).\n        \"\"\"\n        super().__init__()\n\n        self.input_shape = input_shape\n        # Backbone preentrenado \n        self.backbone = pretrained_model_resnet\n\n        # Inferimos cuántas características (F) salen del backbone para este input.\n        # Se usa un batch sintético de 1 imagen: [1, C, H, W].\n        backbone_output_shape = get_output_shape(self.backbone, [1, *input_shape])\n        # Aplastamos todas las dimensiones de salida para obtener F (nº de features).\n        backbone_output_features = reduce(lambda x, y: x*y, backbone_output_shape)\n\n        # ---------------------------\n        # CABEZA DE CLASIFICACIÓN \n        # ---------------------------\n        # Toma el vector de features (F) y produce 'logits' de tamaño n_classes.\n        self.cls_head = nn.Sequential(\n            nn.Linear(backbone_output_features, 768),\n            nn.BatchNorm1d(768),\n            nn.SiLU(),\n            nn.Dropout(0.5),\n\n            nn.Linear(768, 256),\n            nn.BatchNorm1d(256),\n            nn.SiLU(),\n            nn.Dropout(0.6),\n\n            nn.Linear(256, n_classes)  # logits\n        )\n\n        # ---------------------------\n        # CABEZA DE REGRESIÓN (BBOX)\n        # ---------------------------\n        # Predice 4 valores: [xmin, ymin, xmax, ymax] en la MISMA escala que tus etiquetas.\n        self.reg_head = nn.Sequential(\n            nn.Linear(backbone_output_features, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n\n            nn.Linear(256, 4)\n        )\n\n    def forward(self, x: Tensor) -> ty.Dict[str, Tensor]:\n        # ===========================================================\n        # FLUJO HACIA ADELANTE\n        # x: tensor de imágenes [B, 3, 640, 640]\n        # 1) Extraemos features con el backbone → [B, F]\n        # 2) cls_head(features) → logits de clase [B, n_classes]\n        # 3) reg_head(features) → bbox [B, 4]\n        # 4) Devolvemos un diccionario con ambas salidas\n        # ===========================================================\n        features = self.backbone(x)\n        cls_logits = self.cls_head(features)\n        pred_bbox = self.reg_head(features)\n        predictions = {'bbox': pred_bbox, 'class_id': cls_logits}\n        return predictions\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:10:59.953968Z","iopub.execute_input":"2025-09-26T02:10:59.954342Z","iopub.status.idle":"2025-09-26T02:10:59.963820Z","shell.execute_reply.started":"2025-09-26T02:10:59.954325Z","shell.execute_reply":"2025-09-26T02:10:59.963280Z"},"trusted":true},"outputs":[],"execution_count":3},{"id":"39f35654","cell_type":"code","source":"\n# ---------------------------------------------------------------------------\n# FIRMAS DE TRANSFORMACIONES (TIPADO)\n# ---------------------------------------------------------------------------\n# Cada transformación debe recibir y devolver un diccionario de numpy arrays\n# con claves como 'image', 'bbox' y/o 'class_id'. Esto ayuda a documentar\n# la interfaz esperada por el pipeline de datos.\ntransform_func_inp_signature = ty.Dict[str, NDArray[np.float64]]\ntransform_func_signature = ty.Callable[\n    [transform_func_inp_signature],  # entrada: sample dict\n    transform_func_inp_signature     # salida: sample dict (misma estructura)\n]\n\n\n# ===========================\n# VISUALIZACIÓN Y BBOX\n# ===========================\ndef draw_bbox(img, bbox, color,thickness: int = 3):\n    # Dibuja un único cuadro delimitador (bounding box) sobre una imagen.\n    xmin, ymin, xmax, ymax = bbox\n    img = cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color, thickness)\n    return img\n\ndef normalize_bbox(bbox, h: int, w: int):\n    \"\"\"Escala las coordenadas normalizadas al tamaño real de la imagen.\"\"\"\n    return [\n        int(bbox[0] * w),  # xmin\n        int(bbox[1] * h),  # ymin\n        int(bbox[2] * w),  # xmax\n        int(bbox[3] * h),  # ymax,\n    ]\n\ndef draw_bboxes(imgs, bboxes, colors,thickness):\n    \"\"\"Dibuja múltiples cuadros delimitadores en imágenes, escalando según h y w.\"\"\"\n    for i, (img, bbox, color) in enumerate(zip(imgs, bboxes, colors)):\n        imgs[i] = draw_bbox(img, bbox, color,thickness)\n    return imgs\n\ndef draw_classes(imgs, classes, colors, origin, prefix: str ='',fontScale : int = 2):\n    \"\"\"Dibuja las clases en las imágenes.\"\"\"\n    for i, (img, class_id, color) in enumerate(zip(imgs, classes, colors)):\n        if type(c)==list:\n            name_class_=id2obj[classes[i]]\n        else:\n            name_class_=id2obj[classes[i][0]]\n        imgs[i] = cv2.putText(\n            img, f'{prefix}{name_class_}',\n            origin, cv2.FONT_HERSHEY_SIMPLEX,\n            fontScale , color, 2, cv2.LINE_AA\n        )\n    return imgs\n\ndef draw_predictions(imgs, classes, bboxes, colors, origin,thickness,fontScale):\n    \"\"\"Combina las funciones anteriores para dibujar cuadros delimitadores y clases en las imágenes.\"\"\"\n    assert all(len(x) > 0 for x in [imgs, classes, bboxes, colors])\n    if len(colors) == 1:\n        colors = [colors[0] for _ in imgs]\n    imgs = draw_bboxes(imgs, bboxes, colors,thickness)\n    imgs = draw_classes(imgs, classes, colors, origin,\"\",fontScale)\n    return imgs\n\n# ===========================\n# DATASET Y MANEJO DE DATOS\n# ===========================\n\ndef extract_any_int(name: str) -> int:\n    \"\"\"Extrae el último grupo de dígitos de un nombre de archivo.\"\"\"\n    base, _ = os.path.splitext(name)\n    nums = re.findall(r'\\d+', base)\n    return int(nums[-1]) if nums else -1\n\nclass maskDataset(Dataset):\n    \"\"\"Dataset personalizado para imágenes y bounding boxes.\"\"\"\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        root_dir: str,\n        labeled: bool = True,\n        transform: ty.Optional[ty.List[transform_func_signature]] = None,\n        output_size: ty.Optional[tuple] = None\n    ) -> None:\n        self.df = df\n        self.root_dir = root_dir\n        self.transform = transform\n        self.labeled = labeled\n        self.output_size = output_size\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, idx: int) -> transform_func_signature:\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        img_name = os.path.join(self.root_dir, self.df.filename.iloc[idx])\n        image = io.imread(img_name)\n        if image is None:\n            raise FileNotFoundError(f\"Image not found: {img_name}\")\n        if image.ndim == 2:\n            image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n        elif image.shape[2] == 4:\n            image = image[:, :, :3]\n        if self.output_size:\n            image = cv2.resize(image, self.output_size)\n        sample = {'image': image}\n        if self.labeled:\n            img_class = self.df.class_id.iloc[idx]\n            img_bbox = self.df.iloc[idx, 1:5]\n            img_bbox = np.array([img_bbox]).astype('float')\n            img_class = np.array([img_class]).astype('int')\n            sample.update({'bbox': img_bbox, 'class_id': img_class})\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n\n# ===========================\n# TRANSFORMACIONES\n# ===========================\n\nclass ToTensor(object):\n    \"\"\"Convierte ndarrays en sample a Tensors.\"\"\"\n    def __call__(self, sample):\n        image = sample['image']\n        image = image.transpose((2, 0, 1))\n        image = torch.from_numpy(image).float()\n        sample.update({'image': image})\n        return sample\n\nclass Normalizer(object):\n    \"\"\"Normaliza la imagen por canal usando mean y std.\"\"\"\n    def __init__(self, stds, means):\n        self.stds = stds\n        self.means = means\n    def __call__(self, sample):\n        image = sample['image']\n        for channel in range(3):\n            image[channel] = (image[channel] - means[channel]) / stds[channel]\n        sample['image'] = image\n        return sample\n\nclass TVTransformWrapper(object):\n    \"\"\"Wrapper para transforms de torchvision.\"\"\"\n    def __init__(self, transform: torch.nn.Module):\n        self.transform = transform\n    def __call__(self, sample):\n        sample['image'] = self.transform(sample['image'])\n        return sample\n\nclass AlbumentationsWrapper(object):\n    \"\"\"Wrapper para transforms de Albumentations.\"\"\"\n    def __init__(self, transform):\n        self.transform = transform\n    def __call__(self, sample):\n        transformed = self.transform(\n            image=sample['image'],\n            bboxes=sample['bbox'],\n        )\n        sample['image'] = transformed['image']\n        sample['bbox'] = np.array(transformed['bboxes'])\n        return sample\n\n# ===========================\n# ARQUITECTURA Y UTILIDADES DE MODELO\n# ===========================\n\ndef get_output_shape(model: nn.Module, image_dim: ty.Tuple[int, int, int]):\n    \"\"\"Inferir la forma de salida del backbone.\"\"\"\n    return model(torch.rand(*(image_dim)).to(device)).data.shape\n\nclass ResNetFeatureExtractor(nn.Module):\n    \"\"\"Extractor de features basado en ResNet.\"\"\"\n    def __init__(self, model):\n        super(ResNetFeatureExtractor, self).__init__()\n        self.features = nn.Sequential(*list(model.children())[:-1])\n        self.flatten = nn.Flatten()\n        self.dropout = nn.Dropout(0.7)\n    def forward(self, x):\n        out = self.features(x)\n        out = self.flatten(out)\n        out = self.dropout(out)\n        return out\n\n\n# ===========================\n# MÉTRICAS Y PÉRDIDAS\n# ===========================\n\ndef iou(y_true: Tensor, y_pred: Tensor):\n    \"\"\"Calcula el IoU (Intersection over Union) promedio entre cajas verdaderas y predichas.\"\"\"\n    pairwise_iou = torchvision.ops.box_iou(y_true.squeeze(), y_pred.squeeze())\n    result = torch.trace(pairwise_iou) / pairwise_iou.size()[0]\n    return result\n\ndef accuracy(y_true: Tensor, y_pred: Tensor) -> Tensor:\n    \"\"\"Accuracy para clasificación binaria con 2 logits.\"\"\"\n    if y_true.dim() > 1:\n        y_true = y_true.squeeze(-1)\n    y_true = y_true.long()\n    pred = torch.argmax(y_pred, dim=-1)\n    if pred.shape != y_true.shape:\n        y_true = y_true.view_as(pred)\n    return (pred == y_true).float().mean()\n\ndef loss_fn(y_true, y_preds, alpha=0.7):\n    \"\"\"Pérdida multitarea: clasificación + regresión.\"\"\"\n    logits = y_preds['class_id']\n    N, K = logits.shape[0], logits.shape[1]\n    if K > 1:\n        y = y_true['class_id']\n        if y.dim() == 2 and y.size(1) == 1:\n            y = y.squeeze(1)\n        if y.dim() == 2 and y.size(1) == K:\n            y = y.argmax(1)\n        y = y.long()\n        class_weights = torch.tensor([0.35, 0.65], dtype=torch.float32).to(logits.device)\n        cls_loss = F.cross_entropy(logits, y, weight=class_weights)\n    else:\n        y = y_true['class_id']\n        if y.dim() == 1:\n            y = y.unsqueeze(1)\n        y = y.float()\n        if y.shape != logits.shape:\n            y = y.view_as(logits)\n        bce = torch.nn.BCEWithLogitsLoss()\n        cls_loss = bce(logits, y)\n    reg_pred = y_preds['bbox'].float()\n    reg_true = y_true['bbox'].float()\n    if reg_pred.shape != reg_true.shape:\n        reg_true = reg_true.view_as(reg_pred)\n    reg_loss = F.mse_loss(reg_pred, reg_true)\n    total = (1 - alpha) * cls_loss + alpha * reg_loss\n    return {'loss': total, 'cls_loss': cls_loss, 'reg_loss': reg_loss}\n\n# ===========================\n# ENTRENAMIENTO Y EVALUACIÓN\n# ===========================\n\ndef printer(logs: ty.Dict[str, ty.Any]):\n    \"\"\"Callback de logging: imprime cada 10 iteraciones.\"\"\"\n    if logs['iters'] % 10 != 0:\n        return\n    print('Iteration #: ', logs['iters'])\n    for name, value in logs.items():\n        if name == 'iters':\n            continue\n        if type(value) in [float, int]:\n            value = round(value, 4)\n        elif type(value) is torch.Tensor:\n            value = torch.round(value, decimals=4)\n        print(f'\\t{name} = {value}')\n    print()\n\ndef evaluate(\n    logs: ty.Dict[str, ty.Any],\n    labels: ty.Dict[str, Tensor],\n    preds: ty.Dict[str, Tensor],\n    eval_set: str,\n    metrics: ty.Dict[str, ty.Callable[[Tensor, Tensor], Tensor]],\n    losses: ty.Optional[ty.Dict[str, Tensor]] = None,\n) -> ty.Dict[str, ty.Any]:\n    \"\"\"Callback de evaluación (uso dentro del training loop).\"\"\"\n    if losses is not None:\n        for loss_name, loss_value in losses.items():\n            logs[f'{eval_set}_{loss_name}'] = loss_value\n    for task_name, label in labels.items():\n        for metric_name, metric in metrics[task_name]:\n            value = metric(label, preds[task_name])\n            logs[f'{eval_set}_{metric_name}'] = value\n    return logs\n\ndef step(\n    model: Model,\n    optimizer: Optimizer,\n    batch: maskDataset,\n    loss_fn: ty.Callable[[ty.Dict[str, torch.Tensor]], torch.Tensor],\n    device: str,\n    train: bool = False,\n) -> ty.Tuple[ty.Dict[str, Tensor], ty.Dict[str, Tensor]]:\n    \"\"\"Un paso (train o eval): forward, loss, backward, optimizer.step().\"\"\"\n    if train:\n        optimizer.zero_grad()\n    img = batch.pop('image').to(device)\n    for k in list(batch.keys()):\n        batch[k] = batch[k].to(device)\n    preds = model(img.float())\n    losses = loss_fn(batch, preds)\n    final_loss = losses['loss']\n    if train:\n        final_loss.backward()\n        optimizer.step()\n    return losses, preds\n\ndef train(\n    model: Model,\n    optimizer: Optimizer,\n    dataset: DataLoader,\n    eval_datasets: ty.List[ty.Tuple[str, DataLoader]],\n    loss_fn: ty.Callable[[ty.Dict[str, torch.Tensor]], torch.Tensor],\n    metrics: ty.Dict[str, ty.Callable[[Tensor, Tensor], Tensor]],\n    callbacks: ty.List[ty.Callable[[ty.Dict[ty.Any, ty.Any]], None]],\n    device: str,\n    train_steps: 100,\n    eval_steps: 10,\n) -> Model:\n    \"\"\"Training loop genérico (clasificación binaria + regresión de bbox).\"\"\"\n    model = model.to(device)\n    iters = 0\n    iterator = iter(dataset)\n    assert train_steps > eval_steps, 'Train steps should be greater than the eval steps'\n    while iters <= train_steps:\n        logs = dict()\n        logs['iters'] = iters\n        try:\n            batch = next(iterator)\n        except StopIteration:\n            iterator = iter(dataset)\n            batch = next(iterator)\n        losses, preds = step(model, optimizer, batch, loss_fn, device, train=True)\n        logs = evaluate(logs, batch, preds, 'train', metrics, losses)\n        if iters % eval_steps == 0:\n            model.eval()\n            with torch.no_grad():\n                for name, dataset in eval_datasets:\n                    for batch in dataset:\n                        losses, preds = step(model, optimizer, batch, loss_fn, device, train=False)\n                        logs = evaluate(logs, batch, preds, name, metrics, losses)\n            model.train()\n        for callback in callbacks:\n            callback(logs)\n        iters += 1\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:10:59.964624Z","iopub.execute_input":"2025-09-26T02:10:59.964906Z","iopub.status.idle":"2025-09-26T02:11:00.001742Z","shell.execute_reply.started":"2025-09-26T02:10:59.964880Z","shell.execute_reply":"2025-09-26T02:11:00.001010Z"},"trusted":true},"outputs":[],"execution_count":4},{"id":"10e7d377","cell_type":"markdown","source":"## **Preparación del Dataset**\n\n-   **Configuración de device**: Selección automática de GPU si está disponible.\n-   **Carga y preprocesamiento**: Lectura del CSV de anotaciones, mapeo de clases a IDs, selección de columnas relevantes.\n-   **Análisis exploratorio**: Estadísticas de tamaños, canales y formas de las imágenes; verificación de balance de clases y validez de bounding boxes.\n-   **Normalización de bounding boxes**: Las coordenadas se escalan a [0,1] para facilitar el entrenamiento.","metadata":{"id":"10e7d377","papermill":{"duration":0.012543,"end_time":"2025-09-04T04:15:36.130516","exception":false,"start_time":"2025-09-04T04:15:36.117973","status":"completed"},"tags":[]}},{"id":"a386a534","cell_type":"code","source":"# ===========================\n# Configuración básica en PyTorch\n# ===========================\n\ntorch.manual_seed(32)\n# Fija la semilla para que los resultados sean reproducibles.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Usando {device}')\n# Selecciona GPU si está disponible, de lo contrario usa CPU.\n\ntest = torch.ones((100, 100)).to(device)\n# Crea un tensor de prueba en el dispositivo seleccionado (CPU o GPU).\n\ndel test\n# Elimina el tensor de la memoria.\n\ntorch.cuda.empty_cache()\n# Limpia la memoria de la GPU, dejándola lista para entrenar modelos.\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:11:00.003496Z","iopub.execute_input":"2025-09-26T02:11:00.003805Z","iopub.status.idle":"2025-09-26T02:11:00.264835Z","shell.execute_reply.started":"2025-09-26T02:11:00.003789Z","shell.execute_reply":"2025-09-26T02:11:00.264271Z"},"id":"a386a534","outputId":"de8a2251-015b-490c-fe98-46ca686ff279","papermill":{"duration":0.234842,"end_time":"2025-09-04T04:15:36.377754","exception":false,"start_time":"2025-09-04T04:15:36.142912","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Usando cuda\n","output_type":"stream"}],"execution_count":5},{"id":"78adb081","cell_type":"code","source":"# ===========================\n# CONFIGURACIÓN DE DIRECTORIOS Y PARÁMETROS\n# ===========================\n\nDATA_DIR = '/kaggle/input/aa-iv-2025-ii-object-localization'  # Carpeta donde Kaggle guarda el dataset (solo lectura)\nWORK_DIR = 'workdir'                                          # Carpeta de trabajo (aquí se guardan outputs y resultados)\nBATCH_SIZE = 32                                               # Tamaño de lote (batch) para el entrenamiento del modelo\n\n# Ruta donde están guardadas las imágenes\nimg_dir = osp.join(DATA_DIR, \"images\")\n\n# ===========================\n# CARGA DEL DATASET\n# ===========================\n\n# Leemos el archivo CSV de entrenamiento que contiene:\n# - nombre de la imagen\n# - coordenadas de la caja delimitadora (bounding box: xmin, ymin, xmax, ymax)\n# - clase del objeto\ndf = pd.read_csv(osp.join(DATA_DIR, \"train.csv\"))\n\n# ===========================\n# MAPEO DE CLASES A IDs\n# ===========================\n\n# Diccionario para convertir las clases (texto) en identificadores numéricos\n# Diccionario que asigna un número a cada clase:\n# - \"no-mask\" → 0\n# - \"mask\"    → 1\nobj2id  = {'no-mask':0,'mask':1}\n\n# Diccionario inverso: convertir IDs numéricos en nombres de clases\nid2obj  = {0:'no-mask',1:'mask'}\n\n# Crear nueva columna \"class_id\" en el DataFrame con el valor numérico de la clase\ndf[\"class_id\"] = df[\"class\"].map(obj2id)\n\n# ===========================\n# SELECCIÓN DE COLUMNAS ÚTILES\n# ===========================\n\n# Definimos qué columnas necesitamos realmente del dataset\ncolumns_f=['filename','xmin','ymin','xmax','ymax','class','class_id']\n\n# Nos quedamos únicamente con esas columnas\ndf = df[columns_f].copy()\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:11:00.265534Z","iopub.execute_input":"2025-09-26T02:11:00.265707Z","iopub.status.idle":"2025-09-26T02:11:00.291440Z","shell.execute_reply.started":"2025-09-26T02:11:00.265692Z","shell.execute_reply":"2025-09-26T02:11:00.290821Z"},"id":"78adb081","papermill":{"duration":0.040481,"end_time":"2025-09-04T04:15:36.431096","exception":false,"start_time":"2025-09-04T04:15:36.390615","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":6},{"id":"aa19b15d","cell_type":"markdown","source":"# Exploremos un poco los datos","metadata":{"id":"aa19b15d","papermill":{"duration":0.0747,"end_time":"2025-09-04T04:15:36.518545","exception":false,"start_time":"2025-09-04T04:15:36.443845","status":"completed"},"tags":[]}},{"id":"0aef8121","cell_type":"code","source":"df\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"execution":{"iopub.status.busy":"2025-09-26T02:11:00.292166Z","iopub.execute_input":"2025-09-26T02:11:00.292446Z","iopub.status.idle":"2025-09-26T02:11:00.314988Z","shell.execute_reply.started":"2025-09-26T02:11:00.292425Z","shell.execute_reply":"2025-09-26T02:11:00.314289Z"},"id":"0aef8121","outputId":"aa01f39b-c2e1-4969-c299-a39e380f538e","papermill":{"duration":0.034573,"end_time":"2025-09-04T04:15:36.565600","exception":false,"start_time":"2025-09-04T04:15:36.531027","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                              filename  xmin  ymin  xmax  \\\n0    video_CDC-YOUTUBE_mp4-63_jpg.rf.2f4f64f6ef712f...   315   249   468   \n1    IMG_4860_mp4-36_jpg.rf.01a053cabddff2cdd19f04e...   257   237   299   \n2    IMG_1491_mp4-12_jpg.rf.9df64033aebef44b8bb9a6a...   291   245   582   \n3    IMG_4861_mp4-64_jpg.rf.74ab6d1da8a1fa9b8fbb576...   231   229   577   \n4    IMG_9950-1-_mp4-83_jpg.rf.74dca33810c23ba144d8...   107   168   515   \n..                                                 ...   ...   ...   ...   \n214  videoplayback-1-_mp4-58_jpg.rf.bfdf3258d74f87d...   408   168   465   \n215  video_CDC-YOUTUBE_mp4-36_jpg.rf.5d17748e659665...   181   232   350   \n216  IMG_4861_mp4-38_jpg.rf.880a11c3ebf59b3d0cf988f...   112   179   413   \n217  How-to-Properly-Wear-a-Face-Mask-_-UC-San-Dieg...   268   134   382   \n218  videoplayback-1-_mp4-28_jpg.rf.ffdc6b183c1a5f9...   396    85   479   \n\n     ymax    class  class_id  \n0     374  no-mask         0  \n1     264  no-mask         0  \n2     449     mask         1  \n3     420  no-mask         0  \n4     469  no-mask         0  \n..    ...      ...       ...  \n214   212  no-mask         0  \n215   356     mask         1  \n216   438  no-mask         0  \n217   422  no-mask         0  \n218   153  no-mask         0  \n\n[219 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>xmax</th>\n      <th>ymax</th>\n      <th>class</th>\n      <th>class_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>video_CDC-YOUTUBE_mp4-63_jpg.rf.2f4f64f6ef712f...</td>\n      <td>315</td>\n      <td>249</td>\n      <td>468</td>\n      <td>374</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>IMG_4860_mp4-36_jpg.rf.01a053cabddff2cdd19f04e...</td>\n      <td>257</td>\n      <td>237</td>\n      <td>299</td>\n      <td>264</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>IMG_1491_mp4-12_jpg.rf.9df64033aebef44b8bb9a6a...</td>\n      <td>291</td>\n      <td>245</td>\n      <td>582</td>\n      <td>449</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>IMG_4861_mp4-64_jpg.rf.74ab6d1da8a1fa9b8fbb576...</td>\n      <td>231</td>\n      <td>229</td>\n      <td>577</td>\n      <td>420</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>IMG_9950-1-_mp4-83_jpg.rf.74dca33810c23ba144d8...</td>\n      <td>107</td>\n      <td>168</td>\n      <td>515</td>\n      <td>469</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>214</th>\n      <td>videoplayback-1-_mp4-58_jpg.rf.bfdf3258d74f87d...</td>\n      <td>408</td>\n      <td>168</td>\n      <td>465</td>\n      <td>212</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>215</th>\n      <td>video_CDC-YOUTUBE_mp4-36_jpg.rf.5d17748e659665...</td>\n      <td>181</td>\n      <td>232</td>\n      <td>350</td>\n      <td>356</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>216</th>\n      <td>IMG_4861_mp4-38_jpg.rf.880a11c3ebf59b3d0cf988f...</td>\n      <td>112</td>\n      <td>179</td>\n      <td>413</td>\n      <td>438</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>217</th>\n      <td>How-to-Properly-Wear-a-Face-Mask-_-UC-San-Dieg...</td>\n      <td>268</td>\n      <td>134</td>\n      <td>382</td>\n      <td>422</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>218</th>\n      <td>videoplayback-1-_mp4-28_jpg.rf.ffdc6b183c1a5f9...</td>\n      <td>396</td>\n      <td>85</td>\n      <td>479</td>\n      <td>153</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>219 rows × 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"id":"748af35e","cell_type":"code","source":"# ===========================\n# CARGA DE UNA IMAGEN DE EJEMPLO\n# ===========================\n\n# Construimos la ruta completa a un archivo de imagen específico.\nimg_filename = osp.join(DATA_DIR, \"images\", 'IMG_1493_mp4-21_jpg.rf.c5a3e237451e64e0674d5b0a6d556c25.jpg')\n\n# 1) Lectura de la imagen con OpenCV (cv2)\n# ----------------------------------------\n# OpenCV lee las imágenes en formato BGR (Blue, Green, Red) por defecto.\nimg1 = cv2.imread(img_filename)\n\n# Convertimos de BGR a RGB para que los colores sean correctos al visualizar con matplotlib.\nimg1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n\n# 2) Lectura de la misma imagen con skimage (io.imread)\n# -----------------------------------------------------\n# La función `io.imread` de scikit-image lee las imágenes directamente en formato RGB,\n# por lo tanto no es necesario hacer la conversión de colores.\nimg2 = io.imread(img_filename)\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:11:00.315723Z","iopub.execute_input":"2025-09-26T02:11:00.316003Z","iopub.status.idle":"2025-09-26T02:11:00.395172Z","shell.execute_reply.started":"2025-09-26T02:11:00.315982Z","shell.execute_reply":"2025-09-26T02:11:00.394615Z"},"id":"748af35e","papermill":{"duration":0.092589,"end_time":"2025-09-04T04:15:36.671568","exception":false,"start_time":"2025-09-04T04:15:36.578979","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":8},{"id":"3f9edbdd","cell_type":"code","source":"# Mostramos la forma original de la imagen: (alto, ancho, canales) → (H, W, C)\nprint(img1.shape)\n\n# Transponemos para pasar a formato (canales, alto, ancho) → (C, H, W),\n# que es el requerido por PyTorch. -> como se vió en clase, pytorch trabaja\n# con Channel first, no Channel last.\nprint(img1.transpose((2,0,1)).shape)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:11:00.396089Z","iopub.execute_input":"2025-09-26T02:11:00.396298Z","iopub.status.idle":"2025-09-26T02:11:00.400690Z","shell.execute_reply.started":"2025-09-26T02:11:00.396281Z","shell.execute_reply":"2025-09-26T02:11:00.399965Z"},"id":"3f9edbdd","outputId":"52867d8f-9b01-4340-e8c7-4aa4e12d91a9","papermill":{"duration":0.019703,"end_time":"2025-09-04T04:15:36.704436","exception":false,"start_time":"2025-09-04T04:15:36.684733","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"(640, 640, 3)\n(3, 640, 640)\n","output_type":"stream"}],"execution_count":9},{"id":"fb8ba88b","cell_type":"code","source":"# ===========================\n# ANÁLISIS EXPLORATORIO DE IMÁGENES\n# ===========================\n\n# Obtenemos la lista de nombres de archivos de las imágenes desde el DataFrame\nlist_image = list(df.filename)\n\n# Inicializamos listas vacías donde guardaremos información de cada imagen\ndata_shape = []   # Guardará la forma completa de la imagen (alto, ancho, canales)\ndata_dim = []     # Guardará el número de dimensiones (ej: 2 para escala de grises, 3 para RGB)\ndata_w = []       # Guardará el ancho de la imagen\ndata_h = []       # Guardará la altura de la imagen\n\n# Recorremos todas las imágenes con una barra de progreso (tqdm)\nfor i in tqdm(list_image):  # Puede tardar unos ~40 segundos en recorrer todo\n    # Construimos la ruta completa de la imagen\n    ruta_imagen = osp.join(img_dir, i)\n\n    # Leemos la imagen con skimage → obtenemos forma y número de dimensiones\n    imagen = io.imread(ruta_imagen)\n    shapes = imagen.shape      # Ejemplo: (300, 400, 3)\n    dimen = imagen.ndim        # Ejemplo: 3 si es RGB, 2 si es escala de grises\n\n    # Leemos la imagen con PIL → obtenemos ancho y alto\n    imagen = Image.open(ruta_imagen)\n    w, h = imagen.size         # size devuelve (ancho, alto)\n\n    # Guardamos toda la información en las listas\n    data_w.append(w)\n    data_h.append(h)\n    data_shape.append(shapes)\n    data_dim.append(dimen)\n\n# Construimos un DataFrame con toda la información recopilada\ndata_w_h = pd.DataFrame(\n    [list_image, data_shape, data_dim, data_w, data_h]\n).T.rename(columns={0:'filename', 1:'shapes', 2:'ndim', 3:'w', 4:'h'})\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["943252b5542c467f87f95df33e170aa5","66bc55d656d84d6f8b2ede7602f0dd03","08aae853b9744c288e5eed5564c19210","64cb1f5bddf64fe094d33023a96b430b","782ae4b088f24aed888b5c7e4b4d229e","de8148cf162a4959860c2dae286d033b","d5840b73017b4fe48b19fdca20aa024f","61fdd179158948c990f3437aa165acbc","31367be3b99a48fba7ff6db544fbe41d","3a1c6fece878487b947b5c67eb42d64f","e7a2edc93e834f8d8f3f9e579f2eb892"]},"execution":{"iopub.status.busy":"2025-09-26T02:11:00.401491Z","iopub.execute_input":"2025-09-26T02:11:00.401727Z","iopub.status.idle":"2025-09-26T02:11:02.133215Z","shell.execute_reply.started":"2025-09-26T02:11:00.401711Z","shell.execute_reply":"2025-09-26T02:11:02.132251Z"},"id":"fb8ba88b","outputId":"5463bb69-02a2-4e08-9f92-bcbe16a10dea","papermill":{"duration":1.572186,"end_time":"2025-09-04T04:15:38.289664","exception":false,"start_time":"2025-09-04T04:15:36.717478","status":"completed"},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/219 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27760779431a46eeb07bb1c44f552ed0"}},"metadata":{}}],"execution_count":10},{"id":"110ecc9e","cell_type":"code","source":"# Contamos cuántas veces aparece cada forma (alto, ancho, canales) en el dataset.\ndata_w_h['shapes'].value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":147},"execution":{"iopub.status.busy":"2025-09-26T02:11:02.136049Z","iopub.execute_input":"2025-09-26T02:11:02.136264Z","iopub.status.idle":"2025-09-26T02:11:02.145173Z","shell.execute_reply.started":"2025-09-26T02:11:02.136248Z","shell.execute_reply":"2025-09-26T02:11:02.144390Z"},"id":"110ecc9e","outputId":"8b518412-131c-4214-cf51-889cf838af3a","papermill":{"duration":0.023761,"end_time":"2025-09-04T04:15:38.400866","exception":false,"start_time":"2025-09-04T04:15:38.377105","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"shapes\n(640, 640, 3)    219\nName: count, dtype: int64"},"metadata":{}}],"execution_count":11},{"id":"af279e13","cell_type":"code","source":"# Contamos cuántas veces aparece cada clase en el dataset (en formato de texto).\n# Esto muestra la distribución de imágenes entre las clases \"mask\" y \"no-mask\".\ndf['class'].value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"execution":{"iopub.status.busy":"2025-09-26T02:11:02.145785Z","iopub.execute_input":"2025-09-26T02:11:02.146001Z","iopub.status.idle":"2025-09-26T02:11:02.165820Z","shell.execute_reply.started":"2025-09-26T02:11:02.145984Z","shell.execute_reply":"2025-09-26T02:11:02.165052Z"},"id":"af279e13","outputId":"9e098ca9-e27c-4d0a-aa72-9722d70d2a04","papermill":{"duration":0.022073,"end_time":"2025-09-04T04:15:38.474474","exception":false,"start_time":"2025-09-04T04:15:38.452401","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"class\nno-mask    135\nmask        84\nName: count, dtype: int64"},"metadata":{}}],"execution_count":12},{"id":"fa5b5272","cell_type":"code","source":"# Verificamos si existen errores en las coordenadas de las cajas delimitadoras (bounding boxes).\n\n# Caso 1: xmin >= xmax\n# Esto indicaría que el lado izquierdo de la caja está a la derecha del lado derecho → caja inválida.\n\n# Caso 2: ymin >= ymax\n# Esto indicaría que la parte superior de la caja está por debajo de la parte inferior → caja inválida.\ndf[df['xmin']>=df['xmax']].shape, df[df['ymin']>=df['ymax']].shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:11:02.166637Z","iopub.execute_input":"2025-09-26T02:11:02.166894Z","iopub.status.idle":"2025-09-26T02:11:02.186663Z","shell.execute_reply.started":"2025-09-26T02:11:02.166853Z","shell.execute_reply":"2025-09-26T02:11:02.185916Z"},"id":"fa5b5272","outputId":"ea629280-4bf6-4dac-c9c0-8bddfa209a83","papermill":{"duration":0.022136,"end_time":"2025-09-04T04:15:38.510376","exception":false,"start_time":"2025-09-04T04:15:38.488240","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"((0, 7), (0, 7))"},"metadata":{}}],"execution_count":13},{"id":"936367f4","cell_type":"markdown","source":"# Normalizamos los bounding box","metadata":{"id":"936367f4","papermill":{"duration":0.013344,"end_time":"2025-09-04T04:15:38.569981","exception":false,"start_time":"2025-09-04T04:15:38.556637","status":"completed"},"tags":[]}},{"id":"ad59f78f","cell_type":"code","source":"# Mostramos estadísticas básicas (min, max, promedio, etc.)\n# de las coordenadas de las bounding boxes: ymin, ymax, xmin, xmax.\n# Sirve para verificar que las cajas estén dentro de los rangos esperados\n# y detectar valores anómalos en las anotaciones.\nprint(df[[\"ymin\", \"ymax\", \"xmin\", \"xmax\"]].describe())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:11:02.187524Z","iopub.execute_input":"2025-09-26T02:11:02.187758Z","iopub.status.idle":"2025-09-26T02:11:02.219332Z","shell.execute_reply.started":"2025-09-26T02:11:02.187734Z","shell.execute_reply":"2025-09-26T02:11:02.218445Z"},"id":"ad59f78f","outputId":"38912311-2d91-4352-e8f5-0d41de7ed1ff","papermill":{"duration":0.032165,"end_time":"2025-09-04T04:15:38.615671","exception":false,"start_time":"2025-09-04T04:15:38.583506","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"             ymin        ymax        xmin        xmax\ncount  219.000000  219.000000  219.000000  219.000000\nmean   175.296804  371.278539  217.068493  446.849315\nstd     67.509690   97.632620  108.656136  104.015128\nmin      3.000000  145.000000    0.000000  146.000000\n25%    134.000000  333.000000  154.000000  382.500000\n50%    168.000000  394.000000  204.000000  466.000000\n75%    224.000000  431.500000  274.500000  513.000000\nmax    420.000000  640.000000  557.000000  640.000000\n","output_type":"stream"}],"execution_count":14},{"id":"710a05e9","cell_type":"code","source":"# ===========================\n# NORMALIZACIÓN DE COORDENADAS DE LAS BOUNDING BOXES\n# ===========================\n\n# Definimos la altura y el ancho reales de las imágenes del dataset.\n# En este caso todas son de 640 x 640 píxeles.\nh_real = 640\nw_real = 640\n\n# Normalizamos las coordenadas de las cajas delimitadoras dividiéndolas\n# entre la altura o el ancho correspondiente.\n# De esta forma los valores quedan entre 0 y 1, lo que facilita el entrenamiento.\ndf[[\"ymin\", \"ymax\"]] = df[[\"ymin\", \"ymax\"]].div(h_real, axis=0)\ndf[[\"xmin\", \"xmax\"]] = df[[\"xmin\", \"xmax\"]].div(w_real, axis=0)","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:11:02.220304Z","iopub.execute_input":"2025-09-26T02:11:02.220612Z","iopub.status.idle":"2025-09-26T02:11:02.233231Z","shell.execute_reply.started":"2025-09-26T02:11:02.220586Z","shell.execute_reply":"2025-09-26T02:11:02.232414Z"},"id":"710a05e9","papermill":{"duration":0.022784,"end_time":"2025-09-04T04:15:38.652071","exception":false,"start_time":"2025-09-04T04:15:38.629287","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":15},{"id":"5fdd773e","cell_type":"code","source":"# Estadisticos normalizados\nprint(df[[\"ymin\", \"ymax\", \"xmin\", \"xmax\"]].describe())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:11:02.234182Z","iopub.execute_input":"2025-09-26T02:11:02.234455Z","iopub.status.idle":"2025-09-26T02:11:02.263090Z","shell.execute_reply.started":"2025-09-26T02:11:02.234431Z","shell.execute_reply":"2025-09-26T02:11:02.262129Z"},"id":"5fdd773e","outputId":"aaf84d8f-0a4b-4717-e445-92063ed468af","papermill":{"duration":0.029962,"end_time":"2025-09-04T04:15:38.695783","exception":false,"start_time":"2025-09-04T04:15:38.665821","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"             ymin        ymax        xmin        xmax\ncount  219.000000  219.000000  219.000000  219.000000\nmean     0.273901    0.580123    0.339170    0.698202\nstd      0.105484    0.152551    0.169775    0.162524\nmin      0.004687    0.226562    0.000000    0.228125\n25%      0.209375    0.520312    0.240625    0.597656\n50%      0.262500    0.615625    0.318750    0.728125\n75%      0.350000    0.674219    0.428906    0.801562\nmax      0.656250    1.000000    0.870313    1.000000\n","output_type":"stream"}],"execution_count":16},{"id":"f58755d8","cell_type":"code","source":"# Ahora visualizamos el df con los bbox normalizados\ndf","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"execution":{"iopub.status.busy":"2025-09-26T02:11:02.263813Z","iopub.execute_input":"2025-09-26T02:11:02.264092Z","iopub.status.idle":"2025-09-26T02:11:02.281299Z","shell.execute_reply.started":"2025-09-26T02:11:02.264067Z","shell.execute_reply":"2025-09-26T02:11:02.280646Z"},"id":"f58755d8","outputId":"dee35548-610d-4bcf-8f21-44140ba94dab","papermill":{"duration":0.02797,"end_time":"2025-09-04T04:15:38.737618","exception":false,"start_time":"2025-09-04T04:15:38.709648","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                              filename      xmin      ymin  \\\n0    video_CDC-YOUTUBE_mp4-63_jpg.rf.2f4f64f6ef712f...  0.492188  0.389062   \n1    IMG_4860_mp4-36_jpg.rf.01a053cabddff2cdd19f04e...  0.401562  0.370312   \n2    IMG_1491_mp4-12_jpg.rf.9df64033aebef44b8bb9a6a...  0.454688  0.382812   \n3    IMG_4861_mp4-64_jpg.rf.74ab6d1da8a1fa9b8fbb576...  0.360938  0.357812   \n4    IMG_9950-1-_mp4-83_jpg.rf.74dca33810c23ba144d8...  0.167187  0.262500   \n..                                                 ...       ...       ...   \n214  videoplayback-1-_mp4-58_jpg.rf.bfdf3258d74f87d...  0.637500  0.262500   \n215  video_CDC-YOUTUBE_mp4-36_jpg.rf.5d17748e659665...  0.282813  0.362500   \n216  IMG_4861_mp4-38_jpg.rf.880a11c3ebf59b3d0cf988f...  0.175000  0.279687   \n217  How-to-Properly-Wear-a-Face-Mask-_-UC-San-Dieg...  0.418750  0.209375   \n218  videoplayback-1-_mp4-28_jpg.rf.ffdc6b183c1a5f9...  0.618750  0.132812   \n\n         xmax      ymax    class  class_id  \n0    0.731250  0.584375  no-mask         0  \n1    0.467187  0.412500  no-mask         0  \n2    0.909375  0.701562     mask         1  \n3    0.901563  0.656250  no-mask         0  \n4    0.804688  0.732812  no-mask         0  \n..        ...       ...      ...       ...  \n214  0.726562  0.331250  no-mask         0  \n215  0.546875  0.556250     mask         1  \n216  0.645312  0.684375  no-mask         0  \n217  0.596875  0.659375  no-mask         0  \n218  0.748437  0.239063  no-mask         0  \n\n[219 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>xmax</th>\n      <th>ymax</th>\n      <th>class</th>\n      <th>class_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>video_CDC-YOUTUBE_mp4-63_jpg.rf.2f4f64f6ef712f...</td>\n      <td>0.492188</td>\n      <td>0.389062</td>\n      <td>0.731250</td>\n      <td>0.584375</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>IMG_4860_mp4-36_jpg.rf.01a053cabddff2cdd19f04e...</td>\n      <td>0.401562</td>\n      <td>0.370312</td>\n      <td>0.467187</td>\n      <td>0.412500</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>IMG_1491_mp4-12_jpg.rf.9df64033aebef44b8bb9a6a...</td>\n      <td>0.454688</td>\n      <td>0.382812</td>\n      <td>0.909375</td>\n      <td>0.701562</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>IMG_4861_mp4-64_jpg.rf.74ab6d1da8a1fa9b8fbb576...</td>\n      <td>0.360938</td>\n      <td>0.357812</td>\n      <td>0.901563</td>\n      <td>0.656250</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>IMG_9950-1-_mp4-83_jpg.rf.74dca33810c23ba144d8...</td>\n      <td>0.167187</td>\n      <td>0.262500</td>\n      <td>0.804688</td>\n      <td>0.732812</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>214</th>\n      <td>videoplayback-1-_mp4-58_jpg.rf.bfdf3258d74f87d...</td>\n      <td>0.637500</td>\n      <td>0.262500</td>\n      <td>0.726562</td>\n      <td>0.331250</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>215</th>\n      <td>video_CDC-YOUTUBE_mp4-36_jpg.rf.5d17748e659665...</td>\n      <td>0.282813</td>\n      <td>0.362500</td>\n      <td>0.546875</td>\n      <td>0.556250</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>216</th>\n      <td>IMG_4861_mp4-38_jpg.rf.880a11c3ebf59b3d0cf988f...</td>\n      <td>0.175000</td>\n      <td>0.279687</td>\n      <td>0.645312</td>\n      <td>0.684375</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>217</th>\n      <td>How-to-Properly-Wear-a-Face-Mask-_-UC-San-Dieg...</td>\n      <td>0.418750</td>\n      <td>0.209375</td>\n      <td>0.596875</td>\n      <td>0.659375</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>218</th>\n      <td>videoplayback-1-_mp4-28_jpg.rf.ffdc6b183c1a5f9...</td>\n      <td>0.618750</td>\n      <td>0.132812</td>\n      <td>0.748437</td>\n      <td>0.239063</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>219 rows × 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":17},{"id":"7553fdb9","cell_type":"code","source":"# ===========================\n# PARTICIÓN ENTRENAMIENTO / VALIDACIÓN (estratificada)\n# ===========================\n# Dividimos el DataFrame 'df' en dos subconjuntos:\n#  - train_df: datos para entrenar el modelo (75%)\n#  - val_df:   datos para validar el modelo (25%)\n#\n# Parámetros clave:\n#  - stratify=df['class_id']  → mantiene la misma proporción de clases en\n#    train y val (muy importante si el dataset está desbalanceado).\n#  - test_size=0.25           → 25% de los datos va a validación.\n#  - random_state=42           → semilla para reproducibilidad del split.\ntrain_df, val_df = train_test_split(\n    df, stratify=df['class_id'], test_size=0.25, random_state=42\n)\n\n# Tamaños resultantes de cada partición\nprint(train_df.shape)\nprint(val_df.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:11:02.282302Z","iopub.execute_input":"2025-09-26T02:11:02.282580Z","iopub.status.idle":"2025-09-26T02:11:02.304769Z","shell.execute_reply.started":"2025-09-26T02:11:02.282558Z","shell.execute_reply":"2025-09-26T02:11:02.304095Z"},"id":"7553fdb9","outputId":"988d21e4-eff3-4f47-dcc2-d37bfc78d8bd","papermill":{"duration":0.024319,"end_time":"2025-09-04T04:15:38.775982","exception":false,"start_time":"2025-09-04T04:15:38.751663","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"(164, 7)\n(55, 7)\n","output_type":"stream"}],"execution_count":18},{"id":"2970c565","cell_type":"markdown","source":"**Importante**: El set de entrenamiento debe tener información acerca de la clase y las coordenadas correspondientes a los bbox","metadata":{"id":"2970c565","papermill":{"duration":0.013711,"end_time":"2025-09-04T04:15:38.803971","exception":false,"start_time":"2025-09-04T04:15:38.790260","status":"completed"},"tags":[]}},{"id":"22da4a0a","cell_type":"code","source":"# ===========================\n# DISTRIBUCIÓN DE CLASES EN TRAIN (en %)\n# ===========================\n# ahora verificamos que la distribución de las clases se mantengan en el train\n# Útil para verificar que el split estratificado mantuvo el balance de clases.\ntrain_df['class'].value_counts(normalize=True) * 100","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"execution":{"iopub.status.busy":"2025-09-26T02:11:02.305653Z","iopub.execute_input":"2025-09-26T02:11:02.305928Z","iopub.status.idle":"2025-09-26T02:11:02.330206Z","shell.execute_reply.started":"2025-09-26T02:11:02.305907Z","shell.execute_reply":"2025-09-26T02:11:02.329278Z"},"id":"22da4a0a","outputId":"a295b73c-66fa-4e53-a1a7-88fcca721817","papermill":{"duration":0.023832,"end_time":"2025-09-04T04:15:38.842276","exception":false,"start_time":"2025-09-04T04:15:38.818444","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"class\nno-mask    61.585366\nmask       38.414634\nName: proportion, dtype: float64"},"metadata":{}}],"execution_count":19},{"id":"ce7b7cef","cell_type":"code","source":"h, w, c = 256, 256, 3","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:11:02.331049Z","iopub.execute_input":"2025-09-26T02:11:02.331376Z","iopub.status.idle":"2025-09-26T02:11:02.345441Z","shell.execute_reply.started":"2025-09-26T02:11:02.331347Z","shell.execute_reply":"2025-09-26T02:11:02.344682Z"},"id":"ce7b7cef","papermill":{"duration":0.019333,"end_time":"2025-09-04T04:15:39.020618","exception":false,"start_time":"2025-09-04T04:15:39.001285","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":20},{"id":"5863ed66","cell_type":"code","source":"# ===========================\n# VISUALIZACIÓN DE MUESTRAS CON CAJAS Y CLASES\n# ===========================\n\n# Carpeta raíz donde están las imágenes del split de entrenamiento\ntrain_root_dir = osp.join(DATA_DIR, \"images\")#, \"train\"\n\n# Instanciamos el Dataset con el DataFrame de train y forzamos tamaño de salida (w, h)\ntrain_ds = maskDataset(train_df, root_dir=train_root_dir,output_size=(w,h))\n\n# Número de imágenes a mostrar y desde qué índice empezar\nnum_imgs = 6\nstart_idx = 0\n\n# Tomamos 'num_imgs' muestras consecutivas a partir de 'start_idx'\nsamples = [train_ds[i] for i in range(start_idx, num_imgs)]\n\n# Extraemos por separado las imágenes, bboxes y clases de cada sample\nimgs = [s['image'] for s in samples]\n# Convertimos las cajas normalizadas [0,1] a píxeles con (w,h) de salida\nbboxes = [normalize_bbox(s['bbox'].squeeze(),h,w) for s in samples]\nclasses = [s['class_id'] for s in samples]\n\n# Dibujamos predicciones: cajas + etiquetas\n# - colors: lista con un color (BGR) que se reutiliza para todas las imágenes\n# - origin: punto (x,y) para el texto (10% del ancho y alto)\n# - thickness y fontScale: grosor de línea y tamaño de fuente\nimgs = draw_predictions(imgs, classes, bboxes, [(0, 150, 0)], (int(w*0.1), int(h*0.1)),thickness = 1,fontScale=1)#(150, 10)\n\n# Creamos una figura grande y colocamos cada imagen en una subgráfica\nfig = plt.figure(figsize=(30, num_imgs))\n\nfor i, img in enumerate(imgs):\n    fig.add_subplot(1, num_imgs, i+1)\n    plt.imshow(img)\n\n# Mostramos el collage de imágenes con sus cajas y clases\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"execution":{"iopub.status.busy":"2025-09-26T02:11:02.346291Z","iopub.execute_input":"2025-09-26T02:11:02.346755Z"},"id":"5863ed66","outputId":"0d25600b-8e11-4509-befe-c26dab85a79b","papermill":{"duration":1.008793,"end_time":"2025-09-04T04:15:40.043517","exception":false,"start_time":"2025-09-04T04:15:39.034724","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"It1hBIifjsPm","cell_type":"markdown","source":"# Transfer Learning RESNET50_2\n\n\nUtilizar  **ResNet50_2**  (Wide ResNet-50-2) para localizar personas con tapabocas en imágenes es una excelente opción por varias razones técnicas:\n\n-   **Arquitectura Profunda y Robusta**: ResNet50_2 es una red residual profunda con más filtros por capa que la ResNet50 estándar, lo que le permite aprender representaciones más ricas y complejas de las imágenes.\n    \n    \n-   **Reconocimiento de Patrones Faciales**: Las capas profundas pueden identificar patrones faciales y distinguir entre personas con y sin tapabocas, incluso en condiciones de iluminación o poses variadas.\n    \n","metadata":{"id":"It1hBIifjsPm"}},{"id":"5OTnw_pvjut6","cell_type":"code","source":"from torchvision.models import wide_resnet50_2, Wide_ResNet50_2_Weights\n\n# modelo Wide ResNet50-2\nweights_resnet50_2 = Wide_ResNet50_2_Weights.IMAGENET1K_V2\nmodel_resnet50_2 = wide_resnet50_2(weights=weights_resnet50_2)","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:11:03.356054Z","iopub.execute_input":"2025-09-26T02:11:03.356268Z","iopub.status.idle":"2025-09-26T02:11:09.091565Z","shell.execute_reply.started":"2025-09-26T02:11:03.356251Z","shell.execute_reply":"2025-09-26T02:11:09.090745Z"},"id":"5OTnw_pvjut6","trusted":true},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/wide_resnet50_2-9ba9bcbe.pth\" to /root/.cache/torch/hub/checkpoints/wide_resnet50_2-9ba9bcbe.pth\n100%|██████████| 263M/263M [00:04<00:00, 67.8MB/s] \n","output_type":"stream"}],"execution_count":22},{"id":"j99nEZIcrxJn","cell_type":"code","source":"# Instantiate the ResNet feature extractor\npretrained_model_resnet = ResNetFeatureExtractor(model_resnet50_2).to(device)\npretrained_model_resnet.eval()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:11:09.092325Z","iopub.execute_input":"2025-09-26T02:11:09.092535Z","iopub.status.idle":"2025-09-26T02:11:09.197554Z","shell.execute_reply.started":"2025-09-26T02:11:09.092518Z","shell.execute_reply":"2025-09-26T02:11:09.196688Z"},"id":"j99nEZIcrxJn","outputId":"e44d2b9b-ead1-47af-c3a3-4a3216662cce","trusted":true},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"ResNetFeatureExtractor(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (5): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (6): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (7): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n  )\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (dropout): Dropout(p=0.7, inplace=False)\n)"},"metadata":{}}],"execution_count":23},{"id":"GhKtk4sllbDA","cell_type":"code","source":"summary(pretrained_model_resnet, (3, 640, 640))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:11:09.198366Z","iopub.execute_input":"2025-09-26T02:11:09.198711Z","iopub.status.idle":"2025-09-26T02:11:09.831121Z","shell.execute_reply.started":"2025-09-26T02:11:09.198683Z","shell.execute_reply":"2025-09-26T02:11:09.830373Z"},"id":"GhKtk4sllbDA","outputId":"047df6ac-095c-4899-db00-32326c10d828","trusted":true},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 320, 320]           9,408\n       BatchNorm2d-2         [-1, 64, 320, 320]             128\n              ReLU-3         [-1, 64, 320, 320]               0\n         MaxPool2d-4         [-1, 64, 160, 160]               0\n            Conv2d-5        [-1, 128, 160, 160]           8,192\n       BatchNorm2d-6        [-1, 128, 160, 160]             256\n              ReLU-7        [-1, 128, 160, 160]               0\n            Conv2d-8        [-1, 128, 160, 160]         147,456\n       BatchNorm2d-9        [-1, 128, 160, 160]             256\n             ReLU-10        [-1, 128, 160, 160]               0\n           Conv2d-11        [-1, 256, 160, 160]          32,768\n      BatchNorm2d-12        [-1, 256, 160, 160]             512\n           Conv2d-13        [-1, 256, 160, 160]          16,384\n      BatchNorm2d-14        [-1, 256, 160, 160]             512\n             ReLU-15        [-1, 256, 160, 160]               0\n       Bottleneck-16        [-1, 256, 160, 160]               0\n           Conv2d-17        [-1, 128, 160, 160]          32,768\n      BatchNorm2d-18        [-1, 128, 160, 160]             256\n             ReLU-19        [-1, 128, 160, 160]               0\n           Conv2d-20        [-1, 128, 160, 160]         147,456\n      BatchNorm2d-21        [-1, 128, 160, 160]             256\n             ReLU-22        [-1, 128, 160, 160]               0\n           Conv2d-23        [-1, 256, 160, 160]          32,768\n      BatchNorm2d-24        [-1, 256, 160, 160]             512\n             ReLU-25        [-1, 256, 160, 160]               0\n       Bottleneck-26        [-1, 256, 160, 160]               0\n           Conv2d-27        [-1, 128, 160, 160]          32,768\n      BatchNorm2d-28        [-1, 128, 160, 160]             256\n             ReLU-29        [-1, 128, 160, 160]               0\n           Conv2d-30        [-1, 128, 160, 160]         147,456\n      BatchNorm2d-31        [-1, 128, 160, 160]             256\n             ReLU-32        [-1, 128, 160, 160]               0\n           Conv2d-33        [-1, 256, 160, 160]          32,768\n      BatchNorm2d-34        [-1, 256, 160, 160]             512\n             ReLU-35        [-1, 256, 160, 160]               0\n       Bottleneck-36        [-1, 256, 160, 160]               0\n           Conv2d-37        [-1, 256, 160, 160]          65,536\n      BatchNorm2d-38        [-1, 256, 160, 160]             512\n             ReLU-39        [-1, 256, 160, 160]               0\n           Conv2d-40          [-1, 256, 80, 80]         589,824\n      BatchNorm2d-41          [-1, 256, 80, 80]             512\n             ReLU-42          [-1, 256, 80, 80]               0\n           Conv2d-43          [-1, 512, 80, 80]         131,072\n      BatchNorm2d-44          [-1, 512, 80, 80]           1,024\n           Conv2d-45          [-1, 512, 80, 80]         131,072\n      BatchNorm2d-46          [-1, 512, 80, 80]           1,024\n             ReLU-47          [-1, 512, 80, 80]               0\n       Bottleneck-48          [-1, 512, 80, 80]               0\n           Conv2d-49          [-1, 256, 80, 80]         131,072\n      BatchNorm2d-50          [-1, 256, 80, 80]             512\n             ReLU-51          [-1, 256, 80, 80]               0\n           Conv2d-52          [-1, 256, 80, 80]         589,824\n      BatchNorm2d-53          [-1, 256, 80, 80]             512\n             ReLU-54          [-1, 256, 80, 80]               0\n           Conv2d-55          [-1, 512, 80, 80]         131,072\n      BatchNorm2d-56          [-1, 512, 80, 80]           1,024\n             ReLU-57          [-1, 512, 80, 80]               0\n       Bottleneck-58          [-1, 512, 80, 80]               0\n           Conv2d-59          [-1, 256, 80, 80]         131,072\n      BatchNorm2d-60          [-1, 256, 80, 80]             512\n             ReLU-61          [-1, 256, 80, 80]               0\n           Conv2d-62          [-1, 256, 80, 80]         589,824\n      BatchNorm2d-63          [-1, 256, 80, 80]             512\n             ReLU-64          [-1, 256, 80, 80]               0\n           Conv2d-65          [-1, 512, 80, 80]         131,072\n      BatchNorm2d-66          [-1, 512, 80, 80]           1,024\n             ReLU-67          [-1, 512, 80, 80]               0\n       Bottleneck-68          [-1, 512, 80, 80]               0\n           Conv2d-69          [-1, 256, 80, 80]         131,072\n      BatchNorm2d-70          [-1, 256, 80, 80]             512\n             ReLU-71          [-1, 256, 80, 80]               0\n           Conv2d-72          [-1, 256, 80, 80]         589,824\n      BatchNorm2d-73          [-1, 256, 80, 80]             512\n             ReLU-74          [-1, 256, 80, 80]               0\n           Conv2d-75          [-1, 512, 80, 80]         131,072\n      BatchNorm2d-76          [-1, 512, 80, 80]           1,024\n             ReLU-77          [-1, 512, 80, 80]               0\n       Bottleneck-78          [-1, 512, 80, 80]               0\n           Conv2d-79          [-1, 512, 80, 80]         262,144\n      BatchNorm2d-80          [-1, 512, 80, 80]           1,024\n             ReLU-81          [-1, 512, 80, 80]               0\n           Conv2d-82          [-1, 512, 40, 40]       2,359,296\n      BatchNorm2d-83          [-1, 512, 40, 40]           1,024\n             ReLU-84          [-1, 512, 40, 40]               0\n           Conv2d-85         [-1, 1024, 40, 40]         524,288\n      BatchNorm2d-86         [-1, 1024, 40, 40]           2,048\n           Conv2d-87         [-1, 1024, 40, 40]         524,288\n      BatchNorm2d-88         [-1, 1024, 40, 40]           2,048\n             ReLU-89         [-1, 1024, 40, 40]               0\n       Bottleneck-90         [-1, 1024, 40, 40]               0\n           Conv2d-91          [-1, 512, 40, 40]         524,288\n      BatchNorm2d-92          [-1, 512, 40, 40]           1,024\n             ReLU-93          [-1, 512, 40, 40]               0\n           Conv2d-94          [-1, 512, 40, 40]       2,359,296\n      BatchNorm2d-95          [-1, 512, 40, 40]           1,024\n             ReLU-96          [-1, 512, 40, 40]               0\n           Conv2d-97         [-1, 1024, 40, 40]         524,288\n      BatchNorm2d-98         [-1, 1024, 40, 40]           2,048\n             ReLU-99         [-1, 1024, 40, 40]               0\n      Bottleneck-100         [-1, 1024, 40, 40]               0\n          Conv2d-101          [-1, 512, 40, 40]         524,288\n     BatchNorm2d-102          [-1, 512, 40, 40]           1,024\n            ReLU-103          [-1, 512, 40, 40]               0\n          Conv2d-104          [-1, 512, 40, 40]       2,359,296\n     BatchNorm2d-105          [-1, 512, 40, 40]           1,024\n            ReLU-106          [-1, 512, 40, 40]               0\n          Conv2d-107         [-1, 1024, 40, 40]         524,288\n     BatchNorm2d-108         [-1, 1024, 40, 40]           2,048\n            ReLU-109         [-1, 1024, 40, 40]               0\n      Bottleneck-110         [-1, 1024, 40, 40]               0\n          Conv2d-111          [-1, 512, 40, 40]         524,288\n     BatchNorm2d-112          [-1, 512, 40, 40]           1,024\n            ReLU-113          [-1, 512, 40, 40]               0\n          Conv2d-114          [-1, 512, 40, 40]       2,359,296\n     BatchNorm2d-115          [-1, 512, 40, 40]           1,024\n            ReLU-116          [-1, 512, 40, 40]               0\n          Conv2d-117         [-1, 1024, 40, 40]         524,288\n     BatchNorm2d-118         [-1, 1024, 40, 40]           2,048\n            ReLU-119         [-1, 1024, 40, 40]               0\n      Bottleneck-120         [-1, 1024, 40, 40]               0\n          Conv2d-121          [-1, 512, 40, 40]         524,288\n     BatchNorm2d-122          [-1, 512, 40, 40]           1,024\n            ReLU-123          [-1, 512, 40, 40]               0\n          Conv2d-124          [-1, 512, 40, 40]       2,359,296\n     BatchNorm2d-125          [-1, 512, 40, 40]           1,024\n            ReLU-126          [-1, 512, 40, 40]               0\n          Conv2d-127         [-1, 1024, 40, 40]         524,288\n     BatchNorm2d-128         [-1, 1024, 40, 40]           2,048\n            ReLU-129         [-1, 1024, 40, 40]               0\n      Bottleneck-130         [-1, 1024, 40, 40]               0\n          Conv2d-131          [-1, 512, 40, 40]         524,288\n     BatchNorm2d-132          [-1, 512, 40, 40]           1,024\n            ReLU-133          [-1, 512, 40, 40]               0\n          Conv2d-134          [-1, 512, 40, 40]       2,359,296\n     BatchNorm2d-135          [-1, 512, 40, 40]           1,024\n            ReLU-136          [-1, 512, 40, 40]               0\n          Conv2d-137         [-1, 1024, 40, 40]         524,288\n     BatchNorm2d-138         [-1, 1024, 40, 40]           2,048\n            ReLU-139         [-1, 1024, 40, 40]               0\n      Bottleneck-140         [-1, 1024, 40, 40]               0\n          Conv2d-141         [-1, 1024, 40, 40]       1,048,576\n     BatchNorm2d-142         [-1, 1024, 40, 40]           2,048\n            ReLU-143         [-1, 1024, 40, 40]               0\n          Conv2d-144         [-1, 1024, 20, 20]       9,437,184\n     BatchNorm2d-145         [-1, 1024, 20, 20]           2,048\n            ReLU-146         [-1, 1024, 20, 20]               0\n          Conv2d-147         [-1, 2048, 20, 20]       2,097,152\n     BatchNorm2d-148         [-1, 2048, 20, 20]           4,096\n          Conv2d-149         [-1, 2048, 20, 20]       2,097,152\n     BatchNorm2d-150         [-1, 2048, 20, 20]           4,096\n            ReLU-151         [-1, 2048, 20, 20]               0\n      Bottleneck-152         [-1, 2048, 20, 20]               0\n          Conv2d-153         [-1, 1024, 20, 20]       2,097,152\n     BatchNorm2d-154         [-1, 1024, 20, 20]           2,048\n            ReLU-155         [-1, 1024, 20, 20]               0\n          Conv2d-156         [-1, 1024, 20, 20]       9,437,184\n     BatchNorm2d-157         [-1, 1024, 20, 20]           2,048\n            ReLU-158         [-1, 1024, 20, 20]               0\n          Conv2d-159         [-1, 2048, 20, 20]       2,097,152\n     BatchNorm2d-160         [-1, 2048, 20, 20]           4,096\n            ReLU-161         [-1, 2048, 20, 20]               0\n      Bottleneck-162         [-1, 2048, 20, 20]               0\n          Conv2d-163         [-1, 1024, 20, 20]       2,097,152\n     BatchNorm2d-164         [-1, 1024, 20, 20]           2,048\n            ReLU-165         [-1, 1024, 20, 20]               0\n          Conv2d-166         [-1, 1024, 20, 20]       9,437,184\n     BatchNorm2d-167         [-1, 1024, 20, 20]           2,048\n            ReLU-168         [-1, 1024, 20, 20]               0\n          Conv2d-169         [-1, 2048, 20, 20]       2,097,152\n     BatchNorm2d-170         [-1, 2048, 20, 20]           4,096\n            ReLU-171         [-1, 2048, 20, 20]               0\n      Bottleneck-172         [-1, 2048, 20, 20]               0\nAdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n         Flatten-174                 [-1, 2048]               0\n         Dropout-175                 [-1, 2048]               0\n================================================================\nTotal params: 66,834,240\nTrainable params: 66,834,240\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 4.69\nForward/backward pass size (MB): 2953.17\nParams size (MB): 254.95\nEstimated Total Size (MB): 3212.81\n----------------------------------------------------------------\n","output_type":"stream"}],"execution_count":24},{"id":"7c3b77ae","cell_type":"markdown","source":"# Normalización de imagen","metadata":{"id":"7c3b77ae","papermill":{"duration":0.026947,"end_time":"2025-09-04T04:16:16.411039","exception":false,"start_time":"2025-09-04T04:16:16.384092","status":"completed"},"tags":[]}},{"id":"ade6093c","cell_type":"code","source":"# ===========================\n# CÁLCULO DE MEDIA Y DESVIACIÓN ESTÁNDAR (por canal) DEL DATASET\n# ===========================\n# Objetivo: estimar las estadísticas de color (mean y std de R, G, B) para\n# usarlas luego en una normalización tipo torchvision.transforms.Normalize(mean, std).\n\ntrain_ds = maskDataset(train_df, root_dir=train_root_dir,output_size=(w,h))\n\n# Acumuladores para medias/STD por canal (R,G,B)\nmeans = np.zeros(3)\nstds = np.zeros(3)\nn_images = 0\n\n# Recorremos todas las imágenes del split de entrenamiento\nfor x in train_ds:\n    img = x['image']  # Imagen en formato HxWxC (RGB).\n                      # (Si la imagen estuviera en uint8 [0..255], las medias/STD saldrán en esa escala.)\n                      # .astype(np.float32) comentado: convertir a float puede ser útil para mayor precisión.\n    n_images += 1\n\n    # Para cada canal (0=R, 1=G, 2=B), calculamos la media y la STD de la imagen actual\n    for channel in range(3):\n        channel_pixels = img[..., channel]  # Todos los píxeles del canal\n        # Se acumula la media y la desviación estándar por imagen (promedio de medias, no ponderado por píxeles)\n        means[channel] += np.mean(channel_pixels)\n        stds[channel] += np.std(channel_pixels)\n\n# Promediamos sobre el número de imágenes para obtener la estimación final por canal\nmeans /= n_images\nstds /= n_images\n\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:11:09.832001Z","iopub.execute_input":"2025-09-26T02:11:09.832288Z","iopub.status.idle":"2025-09-26T02:11:10.803666Z","shell.execute_reply.started":"2025-09-26T02:11:09.832265Z","shell.execute_reply":"2025-09-26T02:11:10.802759Z"},"id":"ade6093c","papermill":{"duration":1.02611,"end_time":"2025-09-04T04:16:17.464488","exception":false,"start_time":"2025-09-04T04:16:16.438378","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":25},{"id":"429e8800","cell_type":"code","source":"# INSPECCIÓN DE ESTADÍSTICAS POR CANAL\n# ===========================\n# 'means': medias por canal [R, G, B] calculadas en el bloque anterior.\n# 'stds' : desviaciones estándar por canal [R, G, B].\n# Útil para configurar transforms.Normalize(mean, std).\nprint(means)\nprint(stds)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:11:10.804840Z","iopub.execute_input":"2025-09-26T02:11:10.805140Z","iopub.status.idle":"2025-09-26T02:11:10.810344Z","shell.execute_reply.started":"2025-09-26T02:11:10.805116Z","shell.execute_reply":"2025-09-26T02:11:10.809562Z"},"id":"429e8800","outputId":"dc395547-9cb9-48de-b0dd-9276189cdb0d","papermill":{"duration":0.03518,"end_time":"2025-09-04T04:16:17.527626","exception":false,"start_time":"2025-09-04T04:16:17.492446","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"[150.87213377 140.8888561  133.6496836 ]\n[62.79959127 61.64436314 59.85598115]\n","output_type":"stream"}],"execution_count":26},{"id":"f8b680c7","cell_type":"markdown","source":"# Transformación de imagenes\n\nSe hace uso de la librería de aumentación de imagenes en https://albumentations.ai/docs/examples/pytorch_classification/\n\n- Se utilizo solo la transformacion de Flip Horizontal y aplicamos un flitro tipo Sephia, se probaron diferentes tipos de transformaciones pero empeoraban las metricas del modelo por lo que despues de la experimentacion exsaustiva se opto por solo esas dos transformaciones","metadata":{"id":"f8b680c7","papermill":{"duration":0.027329,"end_time":"2025-09-04T04:16:17.582567","exception":false,"start_time":"2025-09-04T04:16:17.555238","status":"completed"},"tags":[]}},{"id":"FX7mnpUgy9vf","cell_type":"code","source":"# ===========================\n# DATA AUGMENTATION (Albumentations) + Wrapper para el pipeline\n# ===========================\n\ntrain_data_augmentations = A.Compose([\n    A.HorizontalFlip(p=1),    # Volteo horizontal con prob. 0.5.\n    #A.Rotate(limit=90, p=1, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_REFLECT_101), # Rotation by 90 degrees with 50% probability for each direction.\n\n    ],\n    bbox_params=A.BboxParams(\n        format='albumentations',   # Formato de bboxes esperado por Albumentations:\n                                   # [x_min, y_min, x_max, y_max] NORMALIZADO en [0,1].\n        label_fields=[],           # No se pasan etiquetas (category_ids) a las transforms.\n    )\n)\n\n# En nuestro pipeline, las transforms operan sobre 'sample' (dict).\n# Usamos un wrapper que aplica Albumentations sobre sample['image'] y sample['bbox'].\ndataaug_transforms = torchvision.transforms.Compose(\n    [\n        AlbumentationsWrapper(train_data_augmentations)  # Aplica A.Compose a 'image' y 'bbox'.\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:11:10.811169Z","iopub.execute_input":"2025-09-26T02:11:10.811377Z","iopub.status.idle":"2025-09-26T02:11:10.830382Z","shell.execute_reply.started":"2025-09-26T02:11:10.811352Z","shell.execute_reply":"2025-09-26T02:11:10.829564Z"},"id":"FX7mnpUgy9vf","trusted":true},"outputs":[],"execution_count":27},{"id":"1a19db37","cell_type":"code","source":"import shutil\nimport re  # Usaremos expresiones regulares para extraer números de cualquier nombre de archivo\n\n# ============================================================\n# 1) PREPARAR CARPETA DE SALIDA PARA IMÁGENES FINALES\n#    (BORRAR SI EXISTE Y CREAR DE NUEVO)\n# ============================================================\nif os.path.exists('data_final'):\n    shutil.rmtree('data_final')   # Eliminamos la carpeta anterior para empezar “limpio”\n\nos.mkdir('data_final')            # Carpeta donde guardaremos: imágenes aumentadas + originales\n\n# Dataset base SIN resize (conserva tamaño original).\ntrain_ds_da = maskDataset(train_df, root_dir=train_root_dir) \n\n# ============================================================\n# 2) CÁLCULO ROBUSTO DEL ÚLTIMO ÍNDICE A PARTIR DEL NOMBRE\n#    Funciona con .jpg o .jpeg y con nombres arbitrarios.\n#    Ej.: 'IMG_4921-2...jpg' → extrae '4921' y usa el ÚLTIMO grupo de dígitos.\n# ============================================================\ndef extract_any_int(name: str) -> int:\n    base, _ = os.path.splitext(name)  # separa nombre y extensión\n    nums = re.findall(r'\\d+', base)   # encuentra todos los grupos de dígitos\n    return int(nums[-1]) if nums else -1  # toma el último grupo si existe; si no, -1\n\nlast_index = train_ds_da.df.filename.apply(extract_any_int).max()\nif last_index < 0:    # si ningún archivo tenía dígitos, empezamos desde 0\n    last_index = 0\nindex = int(last_index) + 1   # primer índice nuevo para imágenes sintéticas\n\n# ============================================================\n# 3) GENERAR IMÁGENES AUMENTADAS Y SUS ANOTACIONES\n#    - Recorremos el dataset de train\n#    - Aplicamos data augmentation (p. ej., flip horizontal)\n#    - Guardamos imagen aumentada\n#    - Registramos fila con filename, class_id y bbox\n# ============================================================\nrows = []\nfor j in range(0,1):  # Cantidad de imágenes sintéticas por imagen original (aquí: 1)\n    iterador = iter(train_ds_da)\n    for i in range(len(train_ds_da)):\n        x = next(iterador)                 # sample original: {'image', 'bbox', 'class_id', ...}\n        x_transformed = copy.deepcopy(x)   # copiamos para no modificar el original\n        x_transformed = dataaug_transforms(x_transformed)  # aplicamos augmentations\n\n        # Construimos nombre único para la imagen aumentada\n        filename = f\"image_id_{index}_t{j}.jpeg\"\n\n        # Recuperamos la imagen aumentada (numpy HxWxC, RGB)\n        image = x_transformed['image']  # .astype('uint8') opcional si fuera necesario\n\n        # Guardamos en disco (cv2 usa BGR, por eso convertimos de RGB→BGR)\n        cv2.imwrite(\"data_final/\"+filename, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n\n        # Registramos anotación:\n        #   - filename de la nueva imagen\n        #   - class_id (tal como viene en el sample)\n        #   - bbox (xmin, ymin, xmax, ymax)\n        # Importante: aquí se asume que las bboxes se mantienen en el mismo formato\n        # (p. ej., normalizadas [0,1]) que maneja el resto del pipeline.\n        row = [filename, *x_transformed[\"class_id\"], *x_transformed['bbox'].squeeze()]\n        rows.append(row)\n        index += 1\n\n# Construimos DataFrame con las anotaciones de las imágenes aumentadas\naug_df = pd.DataFrame(rows, columns=['filename', 'class_id', 'xmin', 'ymin', 'xmax', 'ymax',])\n\n# ============================================================\n# 4) COPIAR TAMBIÉN LAS IMÁGENES ORIGINALES A 'data_final'\n#    (tendremos en una misma carpeta originales + aumentadas)\n# ============================================================\nsource = train_root_dir\ndestination = 'data_final'\nallfiles = os.listdir(source)\n\nfor f in allfiles:\n    if f in train_df['filename'].values:   # solo las del split de entrenamiento\n        src_path = os.path.join(source, f)\n        dst_path = os.path.join(destination, f)\n        shutil.copy(src_path, dst_path)\n\n# ============================================================\n# 5) UNIR ANOTACIONES: ORIGINALES + AUMENTADAS\n#    Y AGREGAR LA COLUMNA 'class' A PARTIR DE 'class_id'\n# ============================================================\ndataframe_with_dataaugmentation = pd.concat([train_df, aug_df], ignore_index=True)\ndataframe_with_dataaugmentation['class'] = dataframe_with_dataaugmentation['class_id'].replace(id2obj)\n\n# Mostramos el DataFrame final con todas las anotaciones\ndataframe_with_dataaugmentation\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"execution":{"iopub.status.busy":"2025-09-26T02:11:10.831207Z","iopub.execute_input":"2025-09-26T02:11:10.831489Z","iopub.status.idle":"2025-09-26T02:11:12.358535Z","shell.execute_reply.started":"2025-09-26T02:11:10.831466Z","shell.execute_reply":"2025-09-26T02:11:12.357764Z"},"id":"1a19db37","outputId":"47c4b9a8-102a-47b5-a2fd-c3916ce27e65","papermill":{"duration":1.520945,"end_time":"2025-09-04T04:16:19.259217","exception":false,"start_time":"2025-09-04T04:16:17.738272","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"                                              filename      xmin      ymin  \\\n0    IMG_4921-2_mp4-124_jpg.rf.60e2e62f7f6c331d5960...  0.000000  0.326562   \n1    IMG_3099_mp4-26_jpg.rf.44828067865615f50965e95...  0.189062  0.250000   \n2    videoplayback-1-_mp4-0_jpg.rf.2b8492685ce5a86f...  0.667188  0.298438   \n3    video_CDC-YOUTUBE_mp4-31_jpg.rf.9dcb8f35940393...  0.428125  0.389062   \n4    Apple-Tests-Face-ID-Feature-While-Wearing-a-Ma...  0.245312  0.278125   \n..                                                 ...       ...       ...   \n323                     image_id_5295680176929_t0.jpeg  0.365625  0.462500   \n324                     image_id_5295680176930_t0.jpeg  0.001563  0.264062   \n325                     image_id_5295680176931_t0.jpeg  0.145312  0.296875   \n326                     image_id_5295680176932_t0.jpeg  0.268750  0.259375   \n327                     image_id_5295680176933_t0.jpeg  0.385938  0.390625   \n\n         xmax      ymax    class  class_id  \n0    0.501563  0.781250  no-mask         0  \n1    0.718750  0.648438  no-mask         0  \n2    0.731250  0.351562  no-mask         0  \n3    0.598437  0.528125     mask         1  \n4    0.662500  0.553125  no-mask         0  \n..        ...       ...      ...       ...  \n323  0.404687  0.485938     mask         1  \n324  0.515625  0.710938  no-mask         0  \n325  0.707812  0.818750     mask         1  \n326  0.500000  0.562500     mask         1  \n327  0.873437  0.690625     mask         1  \n\n[328 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>xmax</th>\n      <th>ymax</th>\n      <th>class</th>\n      <th>class_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>IMG_4921-2_mp4-124_jpg.rf.60e2e62f7f6c331d5960...</td>\n      <td>0.000000</td>\n      <td>0.326562</td>\n      <td>0.501563</td>\n      <td>0.781250</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>IMG_3099_mp4-26_jpg.rf.44828067865615f50965e95...</td>\n      <td>0.189062</td>\n      <td>0.250000</td>\n      <td>0.718750</td>\n      <td>0.648438</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>videoplayback-1-_mp4-0_jpg.rf.2b8492685ce5a86f...</td>\n      <td>0.667188</td>\n      <td>0.298438</td>\n      <td>0.731250</td>\n      <td>0.351562</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>video_CDC-YOUTUBE_mp4-31_jpg.rf.9dcb8f35940393...</td>\n      <td>0.428125</td>\n      <td>0.389062</td>\n      <td>0.598437</td>\n      <td>0.528125</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Apple-Tests-Face-ID-Feature-While-Wearing-a-Ma...</td>\n      <td>0.245312</td>\n      <td>0.278125</td>\n      <td>0.662500</td>\n      <td>0.553125</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>323</th>\n      <td>image_id_5295680176929_t0.jpeg</td>\n      <td>0.365625</td>\n      <td>0.462500</td>\n      <td>0.404687</td>\n      <td>0.485938</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>324</th>\n      <td>image_id_5295680176930_t0.jpeg</td>\n      <td>0.001563</td>\n      <td>0.264062</td>\n      <td>0.515625</td>\n      <td>0.710938</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>325</th>\n      <td>image_id_5295680176931_t0.jpeg</td>\n      <td>0.145312</td>\n      <td>0.296875</td>\n      <td>0.707812</td>\n      <td>0.818750</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>326</th>\n      <td>image_id_5295680176932_t0.jpeg</td>\n      <td>0.268750</td>\n      <td>0.259375</td>\n      <td>0.500000</td>\n      <td>0.562500</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>327</th>\n      <td>image_id_5295680176933_t0.jpeg</td>\n      <td>0.385938</td>\n      <td>0.390625</td>\n      <td>0.873437</td>\n      <td>0.690625</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>328 rows × 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":28},{"id":"f943ec49","cell_type":"code","source":"# ===========================\n# VERIFICACIÓN RÁPIDA DE FORMAS (nº de filas y columnas)\n# ===========================\n# Muestra un par de tuplas:\n#  - Primero: shape de train_df  → (n_filas_train, n_columnas)\n#  - Segundo: shape de dataframe_with_dataaugmentation → (n_filas_total, n_columnas)\n#\n\ntrain_df.shape, dataframe_with_dataaugmentation.shape\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:11:12.362903Z","iopub.execute_input":"2025-09-26T02:11:12.363138Z","iopub.status.idle":"2025-09-26T02:11:12.368471Z","shell.execute_reply.started":"2025-09-26T02:11:12.363120Z","shell.execute_reply":"2025-09-26T02:11:12.367688Z"},"id":"f943ec49","outputId":"22897b47-eab5-41e9-e7e7-51f45beeb20a","papermill":{"duration":0.034728,"end_time":"2025-09-04T04:16:19.322027","exception":false,"start_time":"2025-09-04T04:16:19.287299","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"((164, 7), (328, 7))"},"metadata":{}}],"execution_count":29},{"id":"fd81ea46","cell_type":"code","source":"# ===========================\n# PIPELINE DE TRANSFORMACIONES\n# - common_transforms: pasos comunes a train y valid/test\n# - train_data_augmentations: augmentations solo para entrenamiento\n# - train_transforms: augmentations + comunes (en ese orden)\n# - eval_transforms: solo comunes (sin augmentations)\n# ===========================\n\ncommon_transforms = [\n    ToTensor(),               # Convierte imagen de numpy (H,W,C) → torch.Tensor (C,H,W), float32\n    Normalizer(               # Normaliza por canal: (x - mean) / std\n        means=means,          \n        stds=stds,            \n    )\n]\n\ntrain_data_augmentations = A.Compose([\n    A.ToSepia(p=1) # Apply sepia filter\n\n    ],\n    bbox_params=A.BboxParams(\n        format='albumentations',  # Formato esperado: [xmin, ymin, xmax, ymax] NORMALIZADO en [0,1]\n        label_fields=[],          \n    )\n)\n\n# En entrenamiento: primero augmentations (operan sobre numpy HxWxC),\n# luego ToTensor() y Normalizer() (operan sobre tensor CxHxW).\ntrain_transforms = torchvision.transforms.Compose(\n    [\n        AlbumentationsWrapper(train_data_augmentations),  # Aplica A.Compose a 'image' y 'bbox'\n    ] + common_transforms\n)\n\n# En validación/evaluación: NO se aplican augmentations, solo los pasos comunes\n# (ToTensor + Normalizer) para mantener consistencia.\neval_transforms = torchvision.transforms.Compose(common_transforms)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:11:12.369333Z","iopub.execute_input":"2025-09-26T02:11:12.369595Z","iopub.status.idle":"2025-09-26T02:11:12.391019Z","shell.execute_reply.started":"2025-09-26T02:11:12.369575Z","shell.execute_reply":"2025-09-26T02:11:12.390144Z"},"id":"fd81ea46","outputId":"aaefae22-473b-49af-e3ab-1fa0fcb911ed","papermill":{"duration":0.03529,"end_time":"2025-09-04T04:16:19.448084","exception":false,"start_time":"2025-09-04T04:16:19.412794","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/core/composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n  self._set_keys()\n","output_type":"stream"}],"execution_count":30},{"id":"fbc371fc","cell_type":"code","source":"# ===========================\n# DATASET + DATALOADER (entrenamiento)\n# ===========================\n# Creamos el Dataset a partir del DataFrame que une originales + aumentadas.\n# root_dir='data_final'  → carpeta donde guardamos todas las imágenes (originales y sintéticas).\n# transform=train_transforms → aplica (1) augmentations (AlbumentationsWrapper) y (2) pasos comunes (ToTensor + Normalizer).\n# output_size=(w,h)      → fuerza que todas las imágenes salgan con el mismo tamaño (p. ej., 640x640).\ntrain_ds = maskDataset(dataframe_with_dataaugmentation, root_dir='data_final', transform=train_transforms,output_size=(w,h)) #train_root_dir\n\n# DataLoader: empaqueta el dataset en lotes (batches) para entrenamiento.\n# batch_size=16 → cada iteración entrega 16 muestras (imágenes + labels si labeled=True).\ntrain_data = torch.utils.data.DataLoader(train_ds, batch_size=16)\n\n# Iteramos una sola vez sobre el DataLoader para inspeccionar la forma del tensor de imágenes.\n# Esperado en PyTorch (channel-first): [batch, channels, height, width] → (16, 3, h, w)\nfor x in train_data:\n    print(x['image'].size())  \n    break                     # 'break' para no consumir todo el DataLoader en esta comprobación\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:11:12.391957Z","iopub.execute_input":"2025-09-26T02:11:12.392659Z","iopub.status.idle":"2025-09-26T02:11:12.579410Z","shell.execute_reply.started":"2025-09-26T02:11:12.392629Z","shell.execute_reply":"2025-09-26T02:11:12.578469Z"},"id":"fbc371fc","outputId":"e160520a-be1b-4b8f-a56e-07c89098eb10","papermill":{"duration":0.140137,"end_time":"2025-09-04T04:16:19.616658","exception":false,"start_time":"2025-09-04T04:16:19.476521","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([16, 3, 256, 256])\n","output_type":"stream"}],"execution_count":31},{"id":"1fb027c9","cell_type":"markdown","source":"Nota: Se verifica que el tensor tenga forma [B,C,H,W]","metadata":{"id":"1fb027c9","papermill":{"duration":0.027035,"end_time":"2025-09-04T04:16:19.673315","exception":false,"start_time":"2025-09-04T04:16:19.646280","status":"completed"},"tags":[]}},{"id":"eb516cab","cell_type":"code","source":"# Libera la memoria **en caché** de CUDA que PyTorch reservó pero no está usando.\n# No borra tensores activos ni reduce memoria de objetos vivos.\n# Útil tras `del` de tensores grandes para evitar OOM, pero abusar puede bajar rendimiento.\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:11:12.580305Z","iopub.execute_input":"2025-09-26T02:11:12.580566Z","iopub.status.idle":"2025-09-26T02:11:12.661099Z","shell.execute_reply.started":"2025-09-26T02:11:12.580541Z","shell.execute_reply":"2025-09-26T02:11:12.660261Z"},"id":"eb516cab","papermill":{"duration":0.068246,"end_time":"2025-09-04T04:16:20.043869","exception":false,"start_time":"2025-09-04T04:16:19.975623","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":32},{"id":"ff8ee1ea","cell_type":"code","source":"# Imprime el tamaño del tensor de imagen que viene en x['image'].\nprint('image', x['image'].size())\n\n# Instancia el modelo con el tamaño de entrada (3, h, w) y 2 clases.\nmodel = Model(input_shape=(3, h, w), n_classes=2).to(device)\n\n# Mueve el tensor de imágenes al mismo device que el modelo (cuda o cpu).\nx['image'] = x['image'].to(device)\n\n# Forward: pasa el batch de imágenes por el modelo.\n# Salida esperada (diccionario):\n#   - preds['bbox']: tensor [B, 4] con las coordenadas predichas (en la misma escala que las etiquetas).\n#   - preds['class_id']: tensor [B, n_classes] con logits de clasificación.\npreds = model(x['image'])\n\n# Muestra el diccionario de predicciones.\npreds\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:11:12.662234Z","iopub.execute_input":"2025-09-26T02:11:12.662532Z","iopub.status.idle":"2025-09-26T02:11:13.179792Z","shell.execute_reply.started":"2025-09-26T02:11:12.662505Z","shell.execute_reply":"2025-09-26T02:11:13.178975Z"},"id":"ff8ee1ea","outputId":"7c6b613e-d1d2-42d1-fa9e-85b521d8825c","papermill":{"duration":0.719128,"end_time":"2025-09-04T04:16:20.852727","exception":false,"start_time":"2025-09-04T04:16:20.133599","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"image torch.Size([16, 3, 256, 256])\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"{'bbox': tensor([[-0.3548,  0.5903,  0.8692,  0.2882],\n         [-0.1962,  0.2306, -0.2824,  0.2189],\n         [-0.1053,  0.1951,  0.0911,  0.3712],\n         [ 0.5158,  1.2994, -0.2871, -0.0222],\n         [-0.0481,  0.1208,  0.2055, -0.4125],\n         [-0.1586,  0.6600, -0.0941,  0.0755],\n         [ 0.0841, -0.3063,  0.4624,  0.5121],\n         [-0.4052,  0.5987,  0.5357, -0.3930],\n         [-0.3567,  0.5051,  0.2428,  0.3642],\n         [ 0.6720,  0.6685,  0.3395,  0.6756],\n         [-0.9622,  0.0955,  0.4901, -0.2025],\n         [ 0.7583,  1.4584,  0.7085, -0.1757],\n         [-0.7663,  0.6415,  0.4360, -0.0367],\n         [-0.6181,  0.3590,  0.1921, -0.2193],\n         [-0.3429,  0.2445,  0.0462,  0.3567],\n         [ 0.3432,  0.1146,  0.2729,  0.0117]], device='cuda:0',\n        grad_fn=<AddmmBackward0>),\n 'class_id': tensor([[ 1.0195, -0.4723],\n         [ 0.4905, -0.2123],\n         [ 0.8049, -0.8269],\n         [ 0.1396,  0.5523],\n         [-0.0537,  0.3746],\n         [ 0.3589, -0.4843],\n         [-0.0852,  0.4523],\n         [ 0.6193, -0.8755],\n         [-0.1047, -1.0962],\n         [-0.2978,  0.4887],\n         [-0.2325,  0.2031],\n         [ 0.3142, -0.3440],\n         [-0.5505, -1.3684],\n         [-1.7856, -0.4921],\n         [-0.2707, -0.1448],\n         [-0.2245, -0.3710]], device='cuda:0', grad_fn=<AddmmBackward0>)}"},"metadata":{}}],"execution_count":33},{"id":"e3cdb2f4","cell_type":"markdown","source":"# Bucle de entrenamiento/ training loop","metadata":{"id":"e3cdb2f4","papermill":{"duration":0.027799,"end_time":"2025-09-04T04:16:21.390132","exception":false,"start_time":"2025-09-04T04:16:21.362333","status":"completed"},"tags":[]}},{"id":"628b21e9","cell_type":"markdown","source":"# Run","metadata":{"id":"628b21e9","papermill":{"duration":0.02834,"end_time":"2025-09-04T04:16:21.642510","exception":false,"start_time":"2025-09-04T04:16:21.614170","status":"completed"},"tags":[]}},{"id":"5d199b1c","cell_type":"code","source":"os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:11:13.180691Z","iopub.execute_input":"2025-09-26T02:11:13.181126Z","iopub.status.idle":"2025-09-26T02:11:13.184981Z","shell.execute_reply.started":"2025-09-26T02:11:13.181096Z","shell.execute_reply":"2025-09-26T02:11:13.184194Z"},"id":"5d199b1c","papermill":{"duration":0.036477,"end_time":"2025-09-04T04:16:21.708085","exception":false,"start_time":"2025-09-04T04:16:21.671608","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":34},{"id":"d17e039a","cell_type":"markdown","source":"\n- El batch size se definió como resultado de diferentes experimentos y comparación de resultados, con valores inferiores o superiores a 32 teniamos problemas de Memoria desbordada o las metricas se desmejoraban","metadata":{}},{"id":"02c3651a","cell_type":"code","source":"# ===========================\n# RUN: configuración rápida para lanzar entrenamiento/validación\n# ===========================\n\n# Hparams: hiperparámetros básicos del run\nbatch_size = 32\n\n# Data: datasets y transforms\n# - train: augmentations + ToTensor + Normalize, sobre imágenes (w,h)\n# - val  : solo ToTensor + Normalize (sin augmentations)\ntrain_ds = maskDataset(\n    dataframe_with_dataaugmentation, root_dir='data_final',\n    transform=train_transforms, output_size=(w,h)\n)  # ,output_size=(255,255)\n\nval_ds = maskDataset(\n    val_df, root_dir=train_root_dir,\n    transform=eval_transforms, output_size=(w,h)\n)  # ,output_size=(255,255)\n\n# DataLoaders: batching y paralelismo\n# - shuffle solo en train\n# - num_workers = cpu_count() para acelerar lectura/transform\ntrain_data = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=cpu_count())\nval_data   = DataLoader(val_ds,   batch_size=batch_size,               num_workers=cpu_count())\n\n# Model: instancia tu arquitectura (por defecto: input (3,640,640) y 2 clases si ajustaste el Model)\n# - .to(device): mueve a GPU/CPU\nmodel = Model().to(device)\nsummary(model, model.input_shape)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:11:13.185995Z","iopub.execute_input":"2025-09-26T02:11:13.186245Z","iopub.status.idle":"2025-09-26T02:11:13.354494Z","shell.execute_reply.started":"2025-09-26T02:11:13.186222Z","shell.execute_reply":"2025-09-26T02:11:13.353735Z"},"id":"02c3651a","outputId":"8bcde8d7-82c1-4631-922d-60e9caa3ef38","papermill":{"duration":0.449081,"end_time":"2025-09-04T04:16:22.186632","exception":false,"start_time":"2025-09-04T04:16:21.737551","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 128, 128]           9,408\n       BatchNorm2d-2         [-1, 64, 128, 128]             128\n              ReLU-3         [-1, 64, 128, 128]               0\n         MaxPool2d-4           [-1, 64, 64, 64]               0\n            Conv2d-5          [-1, 128, 64, 64]           8,192\n       BatchNorm2d-6          [-1, 128, 64, 64]             256\n              ReLU-7          [-1, 128, 64, 64]               0\n            Conv2d-8          [-1, 128, 64, 64]         147,456\n       BatchNorm2d-9          [-1, 128, 64, 64]             256\n             ReLU-10          [-1, 128, 64, 64]               0\n           Conv2d-11          [-1, 256, 64, 64]          32,768\n      BatchNorm2d-12          [-1, 256, 64, 64]             512\n           Conv2d-13          [-1, 256, 64, 64]          16,384\n      BatchNorm2d-14          [-1, 256, 64, 64]             512\n             ReLU-15          [-1, 256, 64, 64]               0\n       Bottleneck-16          [-1, 256, 64, 64]               0\n           Conv2d-17          [-1, 128, 64, 64]          32,768\n      BatchNorm2d-18          [-1, 128, 64, 64]             256\n             ReLU-19          [-1, 128, 64, 64]               0\n           Conv2d-20          [-1, 128, 64, 64]         147,456\n      BatchNorm2d-21          [-1, 128, 64, 64]             256\n             ReLU-22          [-1, 128, 64, 64]               0\n           Conv2d-23          [-1, 256, 64, 64]          32,768\n      BatchNorm2d-24          [-1, 256, 64, 64]             512\n             ReLU-25          [-1, 256, 64, 64]               0\n       Bottleneck-26          [-1, 256, 64, 64]               0\n           Conv2d-27          [-1, 128, 64, 64]          32,768\n      BatchNorm2d-28          [-1, 128, 64, 64]             256\n             ReLU-29          [-1, 128, 64, 64]               0\n           Conv2d-30          [-1, 128, 64, 64]         147,456\n      BatchNorm2d-31          [-1, 128, 64, 64]             256\n             ReLU-32          [-1, 128, 64, 64]               0\n           Conv2d-33          [-1, 256, 64, 64]          32,768\n      BatchNorm2d-34          [-1, 256, 64, 64]             512\n             ReLU-35          [-1, 256, 64, 64]               0\n       Bottleneck-36          [-1, 256, 64, 64]               0\n           Conv2d-37          [-1, 256, 64, 64]          65,536\n      BatchNorm2d-38          [-1, 256, 64, 64]             512\n             ReLU-39          [-1, 256, 64, 64]               0\n           Conv2d-40          [-1, 256, 32, 32]         589,824\n      BatchNorm2d-41          [-1, 256, 32, 32]             512\n             ReLU-42          [-1, 256, 32, 32]               0\n           Conv2d-43          [-1, 512, 32, 32]         131,072\n      BatchNorm2d-44          [-1, 512, 32, 32]           1,024\n           Conv2d-45          [-1, 512, 32, 32]         131,072\n      BatchNorm2d-46          [-1, 512, 32, 32]           1,024\n             ReLU-47          [-1, 512, 32, 32]               0\n       Bottleneck-48          [-1, 512, 32, 32]               0\n           Conv2d-49          [-1, 256, 32, 32]         131,072\n      BatchNorm2d-50          [-1, 256, 32, 32]             512\n             ReLU-51          [-1, 256, 32, 32]               0\n           Conv2d-52          [-1, 256, 32, 32]         589,824\n      BatchNorm2d-53          [-1, 256, 32, 32]             512\n             ReLU-54          [-1, 256, 32, 32]               0\n           Conv2d-55          [-1, 512, 32, 32]         131,072\n      BatchNorm2d-56          [-1, 512, 32, 32]           1,024\n             ReLU-57          [-1, 512, 32, 32]               0\n       Bottleneck-58          [-1, 512, 32, 32]               0\n           Conv2d-59          [-1, 256, 32, 32]         131,072\n      BatchNorm2d-60          [-1, 256, 32, 32]             512\n             ReLU-61          [-1, 256, 32, 32]               0\n           Conv2d-62          [-1, 256, 32, 32]         589,824\n      BatchNorm2d-63          [-1, 256, 32, 32]             512\n             ReLU-64          [-1, 256, 32, 32]               0\n           Conv2d-65          [-1, 512, 32, 32]         131,072\n      BatchNorm2d-66          [-1, 512, 32, 32]           1,024\n             ReLU-67          [-1, 512, 32, 32]               0\n       Bottleneck-68          [-1, 512, 32, 32]               0\n           Conv2d-69          [-1, 256, 32, 32]         131,072\n      BatchNorm2d-70          [-1, 256, 32, 32]             512\n             ReLU-71          [-1, 256, 32, 32]               0\n           Conv2d-72          [-1, 256, 32, 32]         589,824\n      BatchNorm2d-73          [-1, 256, 32, 32]             512\n             ReLU-74          [-1, 256, 32, 32]               0\n           Conv2d-75          [-1, 512, 32, 32]         131,072\n      BatchNorm2d-76          [-1, 512, 32, 32]           1,024\n             ReLU-77          [-1, 512, 32, 32]               0\n       Bottleneck-78          [-1, 512, 32, 32]               0\n           Conv2d-79          [-1, 512, 32, 32]         262,144\n      BatchNorm2d-80          [-1, 512, 32, 32]           1,024\n             ReLU-81          [-1, 512, 32, 32]               0\n           Conv2d-82          [-1, 512, 16, 16]       2,359,296\n      BatchNorm2d-83          [-1, 512, 16, 16]           1,024\n             ReLU-84          [-1, 512, 16, 16]               0\n           Conv2d-85         [-1, 1024, 16, 16]         524,288\n      BatchNorm2d-86         [-1, 1024, 16, 16]           2,048\n           Conv2d-87         [-1, 1024, 16, 16]         524,288\n      BatchNorm2d-88         [-1, 1024, 16, 16]           2,048\n             ReLU-89         [-1, 1024, 16, 16]               0\n       Bottleneck-90         [-1, 1024, 16, 16]               0\n           Conv2d-91          [-1, 512, 16, 16]         524,288\n      BatchNorm2d-92          [-1, 512, 16, 16]           1,024\n             ReLU-93          [-1, 512, 16, 16]               0\n           Conv2d-94          [-1, 512, 16, 16]       2,359,296\n      BatchNorm2d-95          [-1, 512, 16, 16]           1,024\n             ReLU-96          [-1, 512, 16, 16]               0\n           Conv2d-97         [-1, 1024, 16, 16]         524,288\n      BatchNorm2d-98         [-1, 1024, 16, 16]           2,048\n             ReLU-99         [-1, 1024, 16, 16]               0\n      Bottleneck-100         [-1, 1024, 16, 16]               0\n          Conv2d-101          [-1, 512, 16, 16]         524,288\n     BatchNorm2d-102          [-1, 512, 16, 16]           1,024\n            ReLU-103          [-1, 512, 16, 16]               0\n          Conv2d-104          [-1, 512, 16, 16]       2,359,296\n     BatchNorm2d-105          [-1, 512, 16, 16]           1,024\n            ReLU-106          [-1, 512, 16, 16]               0\n          Conv2d-107         [-1, 1024, 16, 16]         524,288\n     BatchNorm2d-108         [-1, 1024, 16, 16]           2,048\n            ReLU-109         [-1, 1024, 16, 16]               0\n      Bottleneck-110         [-1, 1024, 16, 16]               0\n          Conv2d-111          [-1, 512, 16, 16]         524,288\n     BatchNorm2d-112          [-1, 512, 16, 16]           1,024\n            ReLU-113          [-1, 512, 16, 16]               0\n          Conv2d-114          [-1, 512, 16, 16]       2,359,296\n     BatchNorm2d-115          [-1, 512, 16, 16]           1,024\n            ReLU-116          [-1, 512, 16, 16]               0\n          Conv2d-117         [-1, 1024, 16, 16]         524,288\n     BatchNorm2d-118         [-1, 1024, 16, 16]           2,048\n            ReLU-119         [-1, 1024, 16, 16]               0\n      Bottleneck-120         [-1, 1024, 16, 16]               0\n          Conv2d-121          [-1, 512, 16, 16]         524,288\n     BatchNorm2d-122          [-1, 512, 16, 16]           1,024\n            ReLU-123          [-1, 512, 16, 16]               0\n          Conv2d-124          [-1, 512, 16, 16]       2,359,296\n     BatchNorm2d-125          [-1, 512, 16, 16]           1,024\n            ReLU-126          [-1, 512, 16, 16]               0\n          Conv2d-127         [-1, 1024, 16, 16]         524,288\n     BatchNorm2d-128         [-1, 1024, 16, 16]           2,048\n            ReLU-129         [-1, 1024, 16, 16]               0\n      Bottleneck-130         [-1, 1024, 16, 16]               0\n          Conv2d-131          [-1, 512, 16, 16]         524,288\n     BatchNorm2d-132          [-1, 512, 16, 16]           1,024\n            ReLU-133          [-1, 512, 16, 16]               0\n          Conv2d-134          [-1, 512, 16, 16]       2,359,296\n     BatchNorm2d-135          [-1, 512, 16, 16]           1,024\n            ReLU-136          [-1, 512, 16, 16]               0\n          Conv2d-137         [-1, 1024, 16, 16]         524,288\n     BatchNorm2d-138         [-1, 1024, 16, 16]           2,048\n            ReLU-139         [-1, 1024, 16, 16]               0\n      Bottleneck-140         [-1, 1024, 16, 16]               0\n          Conv2d-141         [-1, 1024, 16, 16]       1,048,576\n     BatchNorm2d-142         [-1, 1024, 16, 16]           2,048\n            ReLU-143         [-1, 1024, 16, 16]               0\n          Conv2d-144           [-1, 1024, 8, 8]       9,437,184\n     BatchNorm2d-145           [-1, 1024, 8, 8]           2,048\n            ReLU-146           [-1, 1024, 8, 8]               0\n          Conv2d-147           [-1, 2048, 8, 8]       2,097,152\n     BatchNorm2d-148           [-1, 2048, 8, 8]           4,096\n          Conv2d-149           [-1, 2048, 8, 8]       2,097,152\n     BatchNorm2d-150           [-1, 2048, 8, 8]           4,096\n            ReLU-151           [-1, 2048, 8, 8]               0\n      Bottleneck-152           [-1, 2048, 8, 8]               0\n          Conv2d-153           [-1, 1024, 8, 8]       2,097,152\n     BatchNorm2d-154           [-1, 1024, 8, 8]           2,048\n            ReLU-155           [-1, 1024, 8, 8]               0\n          Conv2d-156           [-1, 1024, 8, 8]       9,437,184\n     BatchNorm2d-157           [-1, 1024, 8, 8]           2,048\n            ReLU-158           [-1, 1024, 8, 8]               0\n          Conv2d-159           [-1, 2048, 8, 8]       2,097,152\n     BatchNorm2d-160           [-1, 2048, 8, 8]           4,096\n            ReLU-161           [-1, 2048, 8, 8]               0\n      Bottleneck-162           [-1, 2048, 8, 8]               0\n          Conv2d-163           [-1, 1024, 8, 8]       2,097,152\n     BatchNorm2d-164           [-1, 1024, 8, 8]           2,048\n            ReLU-165           [-1, 1024, 8, 8]               0\n          Conv2d-166           [-1, 1024, 8, 8]       9,437,184\n     BatchNorm2d-167           [-1, 1024, 8, 8]           2,048\n            ReLU-168           [-1, 1024, 8, 8]               0\n          Conv2d-169           [-1, 2048, 8, 8]       2,097,152\n     BatchNorm2d-170           [-1, 2048, 8, 8]           4,096\n            ReLU-171           [-1, 2048, 8, 8]               0\n      Bottleneck-172           [-1, 2048, 8, 8]               0\nAdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n         Flatten-174                 [-1, 2048]               0\n         Dropout-175                 [-1, 2048]               0\nResNetFeatureExtractor-176                 [-1, 2048]               0\n          Linear-177                  [-1, 768]       1,573,632\n     BatchNorm1d-178                  [-1, 768]           1,536\n            SiLU-179                  [-1, 768]               0\n         Dropout-180                  [-1, 768]               0\n          Linear-181                  [-1, 256]         196,864\n     BatchNorm1d-182                  [-1, 256]             512\n            SiLU-183                  [-1, 256]               0\n         Dropout-184                  [-1, 256]               0\n          Linear-185                    [-1, 2]             514\n          Linear-186                 [-1, 1024]       2,098,176\n     BatchNorm1d-187                 [-1, 1024]           2,048\n            ReLU-188                 [-1, 1024]               0\n         Dropout-189                 [-1, 1024]               0\n          Linear-190                  [-1, 512]         524,800\n     BatchNorm1d-191                  [-1, 512]           1,024\n            ReLU-192                  [-1, 512]               0\n         Dropout-193                  [-1, 512]               0\n          Linear-194                  [-1, 256]         131,328\n     BatchNorm1d-195                  [-1, 256]             512\n            ReLU-196                  [-1, 256]               0\n          Linear-197                    [-1, 4]           1,028\n================================================================\nTotal params: 71,366,214\nTrainable params: 71,366,214\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.75\nForward/backward pass size (MB): 472.65\nParams size (MB): 272.24\nEstimated Total Size (MB): 745.64\n----------------------------------------------------------------\n","output_type":"stream"}],"execution_count":35},{"id":"96f88a5e","cell_type":"markdown","source":"- Se separaron los learning rates de backbone, clasificación y regresión. Según la literatura y experimentos, para modelos preentrenados es recomendable usar un learning rate bajo en el backbone para evitar perder el conocimiento aprendido. El learning rate de la cabeza de clasificación se mantuvo más bajo que el de regresión, permitiendo equilibrar el ritmo de aprendizaje entre ambas tareas y evitar que la clasificación domine el entrenamiento antes de que la regresión converja, se obtuvieron mejores resultados en clasificacion con LR de 1e-4 que con 1e-5.","metadata":{}},{"id":"71081c75","cell_type":"code","source":"# ===========================\n# OPTIMIZER + LANZAR ENTRENAMIENTO\n# ===========================\n\n# Optimizer: AdamW con learning rate 'lr' sobre TODOS los parámetros del modelo.\noptimizer = torch.optim.AdamW([\n    {\"params\": model.backbone.parameters(), \"lr\": 1e-5},\n    {\"params\": model.cls_head.parameters(), \"lr\": 1e-4},\n    {\"params\": model.reg_head.parameters(), \"lr\": 1e-2}\n], weight_decay=1e-4)\n\n# Loop de entrenamiento:\n# - 'train_data' como DataLoader de entrenamiento\n# - 'eval_datasets': lista de pares (nombre_split, DataLoader) para evaluación periódica\n# - 'loss_fn': pérdida multi-tarea (cls + bbox)\n# - 'metrics':\n#     • bbox  → IoU\n#     • class → accuracy (binaria con 2 logits en este proyecto)\n# - 'callbacks': funciones de logging/monitoreo (p. ej., printer)\n# - 'train_steps' y 'eval_steps': frecuencia de entrenamiento y evaluación\nmodel = train(\n    model,\n    optimizer,\n    train_data,\n    eval_datasets=[('val', val_data)],\n    loss_fn=loss_fn,\n    metrics={\n        'bbox': [('iou', iou)],\n        'class_id': [('accuracy', accuracy)]\n    },\n    callbacks=[printer],\n    device=device,\n    train_steps=100,\n    eval_steps=10\n)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:11:13.355322Z","iopub.execute_input":"2025-09-26T02:11:13.355600Z","iopub.status.idle":"2025-09-26T02:12:03.338710Z","shell.execute_reply.started":"2025-09-26T02:11:13.355581Z","shell.execute_reply":"2025-09-26T02:12:03.337895Z"},"id":"71081c75","outputId":"7bb9ef81-6557-4902-ccf2-f7c928125782","papermill":{"duration":25.228894,"end_time":"2025-09-04T04:16:47.443811","exception":false,"start_time":"2025-09-04T04:16:22.214917","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Iteration #:  0\n\ttrain_loss = 0.412200003862381\n\ttrain_cls_loss = 0.671500027179718\n\ttrain_reg_loss = 0.3010999858379364\n\ttrain_iou = 0.0066\n\ttrain_accuracy = 0.5625\n\tval_loss = 0.3698999881744385\n\tval_cls_loss = 0.6841999888420105\n\tval_reg_loss = 0.2353000044822693\n\tval_iou = 0.0049\n\tval_accuracy = 0.7825999855995178\n\nIteration #:  10\n\ttrain_loss = 0.26330000162124634\n\ttrain_cls_loss = 0.5896999835968018\n\ttrain_reg_loss = 0.12330000102519989\n\ttrain_iou = 0.0212\n\ttrain_accuracy = 0.625\n\tval_loss = 1.2186000347137451\n\tval_cls_loss = 0.6376000046730042\n\tval_reg_loss = 1.4677000045776367\n\tval_iou = 0.0243\n\tval_accuracy = 0.9129999876022339\n\nIteration #:  20\n\ttrain_loss = 0.1941000074148178\n\ttrain_cls_loss = 0.6007999777793884\n\ttrain_reg_loss = 0.019899999722838402\n\ttrain_iou = 0.2356\n\ttrain_accuracy = 0.7390999794006348\n\tval_loss = 0.26969999074935913\n\tval_cls_loss = 0.5321999788284302\n\tval_reg_loss = 0.15719999372959137\n\tval_iou = 0.159\n\tval_accuracy = 0.9129999876022339\n\nIteration #:  30\n\ttrain_loss = 0.1445000022649765\n\ttrain_cls_loss = 0.4449000060558319\n\ttrain_reg_loss = 0.015799999237060547\n\ttrain_iou = 0.2673\n\ttrain_accuracy = 0.8695999979972839\n\tval_loss = 0.14749999344348907\n\tval_cls_loss = 0.3885999917984009\n\tval_reg_loss = 0.04410000145435333\n\tval_iou = 0.2151\n\tval_accuracy = 0.9564999938011169\n\nIteration #:  40\n\ttrain_loss = 0.11079999804496765\n\ttrain_cls_loss = 0.3133000135421753\n\ttrain_reg_loss = 0.02410000003874302\n\ttrain_iou = 0.2922\n\ttrain_accuracy = 0.9129999876022339\n\tval_loss = 0.09200000017881393\n\tval_cls_loss = 0.28049999475479126\n\tval_reg_loss = 0.011300000362098217\n\tval_iou = 0.3656\n\tval_accuracy = 0.9564999938011169\n\nIteration #:  50\n\ttrain_loss = 0.08540000021457672\n\ttrain_cls_loss = 0.2565999925136566\n\ttrain_reg_loss = 0.012000000104308128\n\ttrain_iou = 0.3363\n\ttrain_accuracy = 0.8260999917984009\n\tval_loss = 0.06800000369548798\n\tval_cls_loss = 0.2134000062942505\n\tval_reg_loss = 0.00559999980032444\n\tval_iou = 0.4702\n\tval_accuracy = 0.9564999938011169\n\nIteration #:  60\n\ttrain_loss = 0.05979999899864197\n\ttrain_cls_loss = 0.17579999566078186\n\ttrain_reg_loss = 0.010099999606609344\n\ttrain_iou = 0.3826\n\ttrain_accuracy = 0.9564999938011169\n\tval_loss = 0.05220000073313713\n\tval_cls_loss = 0.16249999403953552\n\tval_reg_loss = 0.004999999888241291\n\tval_iou = 0.445\n\tval_accuracy = 0.9564999938011169\n\nIteration #:  70\n\ttrain_loss = 0.04839999973773956\n\ttrain_cls_loss = 0.13699999451637268\n\ttrain_reg_loss = 0.010400000028312206\n\ttrain_iou = 0.3786\n\ttrain_accuracy = 0.9564999938011169\n\tval_loss = 0.041200000792741776\n\tval_cls_loss = 0.12890000641345978\n\tval_reg_loss = 0.003599999938160181\n\tval_iou = 0.4693\n\tval_accuracy = 1.0\n\nIteration #:  80\n\ttrain_loss = 0.040300000458955765\n\ttrain_cls_loss = 0.11270000040531158\n\ttrain_reg_loss = 0.00930000003427267\n\ttrain_iou = 0.3487\n\ttrain_accuracy = 1.0\n\tval_loss = 0.03220000118017197\n\tval_cls_loss = 0.09849999845027924\n\tval_reg_loss = 0.003800000064074993\n\tval_iou = 0.4734\n\tval_accuracy = 1.0\n\nIteration #:  90\n\ttrain_loss = 0.02979999966919422\n\ttrain_cls_loss = 0.08190000057220459\n\ttrain_reg_loss = 0.007400000002235174\n\ttrain_iou = 0.3839\n\ttrain_accuracy = 1.0\n\tval_loss = 0.025699999183416367\n\tval_cls_loss = 0.0794999971985817\n\tval_reg_loss = 0.0027000000700354576\n\tval_iou = 0.5187\n\tval_accuracy = 1.0\n\nIteration #:  100\n\ttrain_loss = 0.022099999710917473\n\ttrain_cls_loss = 0.05779999867081642\n\ttrain_reg_loss = 0.006800000090152025\n\ttrain_iou = 0.419\n\ttrain_accuracy = 1.0\n\tval_loss = 0.022600000724196434\n\tval_cls_loss = 0.06909999996423721\n\tval_reg_loss = 0.0027000000700354576\n\tval_iou = 0.5169\n\tval_accuracy = 1.0\n\n","output_type":"stream"}],"execution_count":36},{"id":"1c807730","cell_type":"markdown","source":"# Análisis de algunos resultados (muestra).\n\n","metadata":{"id":"1c807730","papermill":{"duration":0.030721,"end_time":"2025-09-04T04:16:47.504185","exception":false,"start_time":"2025-09-04T04:16:47.473464","status":"completed"},"tags":[]}},{"id":"633c87a7","cell_type":"code","source":"num_imgs = 8\nncols = 8\nnrows = math.ceil(num_imgs / ncols)  # nº de filas para la grilla de visualización\n\nstart_idx = 0\n\n# ===========================\n# 1) CONSTRUIR LOTE DE INFERENCIA (SIN TRANSFORMS)\n# ===========================\n# Tomamos 'num_imgs' ejemplos del split de validación (val_df) para inferencia/visualización.\n# - root_dir: carpeta de imágenes originales.\n# - output_size=(w,h): aseguramos tamaño uniforme (p.ej., 640x640).\n# \ninference_ds = maskDataset(val_df.iloc[start_idx:start_idx+num_imgs], root_dir=train_root_dir,output_size=(w,h))\n\n# DataLoader con batch = num_imgs para procesar todo el subconjunto de una vez (sin barajar).\ninference_data = DataLoader(inference_ds, batch_size=num_imgs, num_workers=1, shuffle=False)\n\n# Extraemos un batch (diccionario con 'image', 'bbox', 'class_id')\ninference_batch = next(iter(inference_data))\n\n# Preasignamos un arreglo donde guardaremos las imágenes YA transformadas a tensor (N, C, H, W)\ninference_imgs = np.empty((num_imgs, 3, h, w))\n\n# Usaremos las transformaciones de evaluación (ToTensor + Normalizer) definidas antes.\n# Estas esperan un 'sample' con clave 'image' y devuelven 'image' como tensor CxHxW normalizado.\ntransform = eval_transforms\n\n# ===========================\n# 2) APLICAR TRANSFORMACIONES DE EVAL A CADA IMAGEN DEL BATCH\n# ===========================\n# El DataLoader devuelve 'inference_batch[\"image\"]' como tensor (N, H, W, C) o arreglo convertible.\n# Recorremos por imagen, aplicamos eval_transforms y guardamos en 'inference_imgs' con forma (C,H,W).\nfor i, img in enumerate(inference_batch['image']):\n    # Convertimos a numpy (HxWxC) si viniera como tensor y aplicamos el wrapper de transforms\n    inference_imgs[i] = transform(dict(image=img.numpy()))['image'].numpy()\n\n# ===========================\n# 3) INFERENCIA CON EL MODELO\n# ===========================\n# Convertimos 'inference_imgs' a tensor float en el device (cuda/cpu) y pasamos por el modelo.\npreds = model(torch.tensor(inference_imgs).float().to(device))\n\n# ===========================\n# 4) PREPARAR ELEMENTOS PARA VISUALIZACIÓN (GT vs PRED)\n# ===========================\n# Tomamos las mismas muestras del Dataset (sin transforms) para dibujar imágenes originales.\nsamples = [inference_ds[i] for i in range(start_idx, num_imgs)]\n\n# Imágenes originales (numpy HxWxC)\nimgs = [s['image'] for s in samples]\n\n# BBoxes ground-truth en píxeles para dibujar:\n#  - s['bbox'] se asume normalizada [0,1]; la convertimos a píxeles con normalize_bbox(h,w).\nbboxes = [normalize_bbox(s['bbox'].squeeze(), h, w) for s in samples]\n\n# Clases ground-truth (enteros), tal como están en el sample.\nclasses = [s['class_id'] for s in samples]\n\n# ===========================\n# 5) POSTPROCESO DE PREDICCIONES\n# ===========================\n# Cajas predichas:\n#  - preds['bbox'] es un tensor [N,4] en la MISMA escala que las etiquetas (normalizada si entrenaste así).\n#  - Convertimos a numpy y a píxeles para dibujar.\npred_bboxes = preds['bbox'].detach().cpu().numpy()\npred_bboxes = [normalize_bbox(bbox, h, w) for bbox in pred_bboxes]\n\n# Clases predichas (logits → argmax). Resultado: ids de clase por imagen.\npred_classes = preds['class_id'].argmax(-1).detach().cpu().numpy()\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:12:03.340116Z","iopub.execute_input":"2025-09-26T02:12:03.340803Z","iopub.status.idle":"2025-09-26T02:12:03.523701Z","shell.execute_reply.started":"2025-09-26T02:12:03.340773Z","shell.execute_reply":"2025-09-26T02:12:03.522947Z"},"id":"633c87a7","papermill":{"duration":0.200413,"end_time":"2025-09-04T04:16:47.867425","exception":false,"start_time":"2025-09-04T04:16:47.667012","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":37},{"id":"d8930038","cell_type":"code","source":"# ===========================\n# VISUALIZACIÓN: GT (verde) vs PRED (rojo) — versión robusta\n# ===========================\n\n# Determinar cuántos ejemplos hay realmente en cada lista/salida\nn = min(len(imgs), len(bboxes), len(pred_bboxes), len(pred_classes))\n\n# --- GT en VERDE ---\nimgs = draw_predictions(\n    imgs[:n], classes[:n], bboxes[:n],\n    [(0, 150, 0)], (int(w*0.1), int(h*0.1)),\n    thickness=1, fontScale=1\n)\n\n# --- PRED en ROJO ---\n# Adaptar clases predichas al formato esperado por draw_predictions\npred_classes_ = [np.array([c]) for c in pred_classes[:n]]\n\nimgs = draw_predictions(\n    imgs[:n], pred_classes_, pred_bboxes[:n],\n    [(200, 0, 0)], (int(w*0.8), int(h*0.8)),\n    thickness=1, fontScale=1\n)\n\n# --- GRID de visualización ---\n# Recalcular filas/columnas en función de n\nncols_eff = min(ncols, n)             # ncols original si cabe; si no, recorta\nnrows_eff = math.ceil(n / ncols_eff)\n\nfig, axes = plt.subplots(nrows=nrows_eff, ncols=ncols_eff, figsize=(30, 30))\n\n# Asegurar un iterable 1D de ejes\naxes_flat = np.array(axes).reshape(-1) if isinstance(axes, np.ndarray) else np.array([axes])\n\nfor i in range(n):\n    axes_flat[i].imshow(imgs[i])\n    axes_flat[i].axis('off')\n\n# Ocultar ejes sobrantes si la grilla es más grande que n\nfor j in range(n, len(axes_flat)):\n    axes_flat[j].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":200},"execution":{"iopub.status.busy":"2025-09-26T02:12:03.524669Z","iopub.execute_input":"2025-09-26T02:12:03.524914Z"},"id":"d8930038","outputId":"2757c070-c649-4f83-a2b4-2811fcd84d5d","papermill":{"duration":1.019688,"end_time":"2025-09-04T04:16:48.917116","exception":false,"start_time":"2025-09-04T04:16:47.897428","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"b51cd65b","cell_type":"code","source":"# Guarda el **modelo completo** (arquitectura + pesos) en disco.\ntorch.save(model, 'pretrained_model.pth')\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:12:04.284387Z","iopub.execute_input":"2025-09-26T02:12:04.284750Z","iopub.status.idle":"2025-09-26T02:12:04.772768Z","shell.execute_reply.started":"2025-09-26T02:12:04.284723Z","shell.execute_reply":"2025-09-26T02:12:04.772165Z"},"id":"b51cd65b","papermill":{"duration":0.474033,"end_time":"2025-09-04T04:16:49.450130","exception":false,"start_time":"2025-09-04T04:16:48.976097","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":39},{"id":"eb6d98e7","cell_type":"markdown","source":"# Submission","metadata":{"id":"eb6d98e7","papermill":{"duration":0.054331,"end_time":"2025-09-04T04:16:49.558863","exception":false,"start_time":"2025-09-04T04:16:49.504532","status":"completed"},"tags":[]}},{"id":"81097f0e","cell_type":"code","source":"# Detectar dispositivo\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Usando: {device}')\nmodel = model.to(device)\nmodel.eval()  # modo inferencia\n\n# Rutas y datos de test\ntest_root_dir = osp.join(DATA_DIR, \"images\")\ntest_df = pd.read_csv(osp.join(DATA_DIR, \"test.csv\"))\n\n# Dataset de test (usa tu clase correcta: maskDataset)\ntest_ds = maskDataset(\n    test_df,\n    root_dir=test_root_dir,\n    labeled=False,\n    transform=eval_transforms,\n    output_size=(w, h)\n)\n\n# DataLoader de test\ntest_data = DataLoader(test_ds, batch_size=1, num_workers=cpu_count(), shuffle=False)\n\n# Listas de salida\nclass_preds, bbox_preds = [], []\n\n# Bucle de inferencia\nwith torch.no_grad():\n    for batch in test_data:\n        imgs = batch['image'].float().to(device)\n        out = model(imgs)\n\n        # Predicciones\n        class_pred = out['class_id'].argmax(dim=-1).detach().cpu().numpy()\n        bbox_pred = out['bbox'].detach().cpu().numpy()\n\n        # Guardar\n        class_preds.append(class_pred.squeeze())\n        bbox_preds.append(bbox_pred.squeeze())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-26T02:12:04.773569Z","iopub.execute_input":"2025-09-26T02:12:04.773856Z","iopub.status.idle":"2025-09-26T02:12:05.609264Z","shell.execute_reply.started":"2025-09-26T02:12:04.773836Z","shell.execute_reply":"2025-09-26T02:12:05.608409Z"},"id":"81097f0e","outputId":"7927afdb-7a2a-431a-8ea2-d22d06bae664","papermill":{"duration":0.674667,"end_time":"2025-09-04T04:16:50.286648","exception":false,"start_time":"2025-09-04T04:16:49.611981","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Usando: cuda\n","output_type":"stream"}],"execution_count":40},{"id":"3a0a472b","cell_type":"code","source":"# Convertir las listas de predicciones en arreglos de NumPy\n# Esto facilita operaciones vectorizadas y el posterior guardado en archivo de submission\nclass_preds = np.array(class_preds)   # Arreglo con las clases predichas (una por imagen)\nbbox_preds = np.array(bbox_preds)     # Arreglo con las cajas predichas (coordenadas por imagen)","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:12:05.610293Z","iopub.execute_input":"2025-09-26T02:12:05.610604Z","iopub.status.idle":"2025-09-26T02:12:05.615529Z","shell.execute_reply.started":"2025-09-26T02:12:05.610576Z","shell.execute_reply":"2025-09-26T02:12:05.614987Z"},"id":"3a0a472b","papermill":{"duration":0.060401,"end_time":"2025-09-04T04:16:50.400348","exception":false,"start_time":"2025-09-04T04:16:50.339947","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":41},{"id":"7beda406","cell_type":"code","source":"submission = pd.DataFrame(\n    index=test_df.filename,   # Usar los nombres de archivo del conjunto de test como índice\n    data={\n        'class_id': class_preds,  # Columna con las clases predichas para cada imagen\n        }\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.status.busy":"2025-09-26T02:12:05.616372Z","iopub.execute_input":"2025-09-26T02:12:05.616641Z","iopub.status.idle":"2025-09-26T02:12:05.632220Z","shell.execute_reply.started":"2025-09-26T02:12:05.616617Z","shell.execute_reply":"2025-09-26T02:12:05.631544Z"},"id":"7beda406","outputId":"c2a3f162-1302-44e8-e037-d459d11daa90","papermill":{"duration":0.066548,"end_time":"2025-09-04T04:16:50.519591","exception":false,"start_time":"2025-09-04T04:16:50.453043","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":42},{"id":"d1223766","cell_type":"code","source":"submission[\"xmin\"] = bbox_preds[:, 0]*w_real  # Coordenada X mínima de la caja, escalada al ancho real de la imagen\nsubmission[\"ymin\"] = bbox_preds[:, 1]*h_real  # Coordenada Y mínima de la caja, escalada a la altura real de la imagen\nsubmission[\"xmax\"] = bbox_preds[:, 2]*w_real  # Coordenada X máxima de la caja, escalada al ancho real de la imagen\nsubmission[\"ymax\"] = bbox_preds[:, 3]*h_real  # Coordenada Y máxima de la caja, escalada a la altura real de la imagen\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:12:05.633093Z","iopub.execute_input":"2025-09-26T02:12:05.633361Z","iopub.status.idle":"2025-09-26T02:12:05.650410Z","shell.execute_reply.started":"2025-09-26T02:12:05.633339Z","shell.execute_reply":"2025-09-26T02:12:05.649810Z"},"id":"d1223766","papermill":{"duration":0.062504,"end_time":"2025-09-04T04:16:50.635373","exception":false,"start_time":"2025-09-04T04:16:50.572869","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":43},{"id":"c7aae0ef","cell_type":"code","source":"submission['class'] = submission['class_id'].replace(id2obj)  # Reemplaza los IDs de clase numéricos por sus nombres/etiquetas reales usando el diccionario id2obj\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:12:05.651028Z","iopub.execute_input":"2025-09-26T02:12:05.651215Z","iopub.status.idle":"2025-09-26T02:12:05.670677Z","shell.execute_reply.started":"2025-09-26T02:12:05.651201Z","shell.execute_reply":"2025-09-26T02:12:05.670168Z"},"id":"c7aae0ef","papermill":{"duration":0.060306,"end_time":"2025-09-04T04:16:50.750013","exception":false,"start_time":"2025-09-04T04:16:50.689707","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":44},{"id":"19ff5272","cell_type":"code","source":"submission['class'].value_counts()  # Muestra la cantidad de predicciones por cada clase (frecuencia de cada etiqueta en el submission)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"execution":{"iopub.status.busy":"2025-09-26T02:12:05.671368Z","iopub.execute_input":"2025-09-26T02:12:05.671610Z","iopub.status.idle":"2025-09-26T02:12:05.691575Z","shell.execute_reply.started":"2025-09-26T02:12:05.671589Z","shell.execute_reply":"2025-09-26T02:12:05.690970Z"},"id":"19ff5272","outputId":"a685379d-698c-4d94-d9a2-affa467312fd","papermill":{"duration":0.063294,"end_time":"2025-09-04T04:16:50.866389","exception":false,"start_time":"2025-09-04T04:16:50.803095","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"class\nno-mask    28\nmask       27\nName: count, dtype: int64"},"metadata":{}}],"execution_count":45},{"id":"5ecc0f8b","cell_type":"code","source":"submission.to_csv('submission_resnet.csv')  # Exporta el DataFrame de submission a un archivo CSV con el nombre 'submission_vgg16.csv'\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T02:12:05.692395Z","iopub.execute_input":"2025-09-26T02:12:05.692679Z","iopub.status.idle":"2025-09-26T02:12:05.713568Z","shell.execute_reply.started":"2025-09-26T02:12:05.692652Z","shell.execute_reply":"2025-09-26T02:12:05.712660Z"},"id":"5ecc0f8b","papermill":{"duration":0.068938,"end_time":"2025-09-04T04:16:51.122393","exception":false,"start_time":"2025-09-04T04:16:51.053455","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":46}]}
{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":112034,"databundleVersionId":13352295,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":128.343241,"end_time":"2025-08-27T02:46:31.262577","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-08-27T02:44:22.919336","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"13255ab062564078aeff3f6147b9763e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e9440b247e44843b9ac43bd94cd1d44":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5765ce57cee440fc908ccfde8b4e1627":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fb15a7b87a984f6dac67eb50eca2cbe2","placeholder":"​","style":"IPY_MODEL_cbdcd78834554d998b64f9051b814b0e","tabbable":null,"tooltip":null,"value":" 219/219 [00:01&lt;00:00, 124.46it/s]"}},"5e1e8fe4e8ee404aaa952d16edcda6ea":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"738acb2540654cba9d2b1ed5cb1534fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_4e9440b247e44843b9ac43bd94cd1d44","placeholder":"​","style":"IPY_MODEL_ce6e8970cc1242079895ab5e3b2cda97","tabbable":null,"tooltip":null,"value":"100%"}},"79b628cae1584373a0ad3be9adbeb0c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_738acb2540654cba9d2b1ed5cb1534fa","IPY_MODEL_c48a5b2986834c0fa106facef82cae90","IPY_MODEL_5765ce57cee440fc908ccfde8b4e1627"],"layout":"IPY_MODEL_13255ab062564078aeff3f6147b9763e","tabbable":null,"tooltip":null}},"c48a5b2986834c0fa106facef82cae90":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_5e1e8fe4e8ee404aaa952d16edcda6ea","max":219,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca07ff10a3304f06ac928a1d75af70d5","tabbable":null,"tooltip":null,"value":219}},"ca07ff10a3304f06ac928a1d75af70d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cbdcd78834554d998b64f9051b814b0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"ce6e8970cc1242079895ab5e3b2cda97":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"fb15a7b87a984f6dac67eb50eca2cbe2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importación de librerias requeridas","metadata":{"papermill":{"duration":0.012791,"end_time":"2025-08-27T02:44:27.09387","exception":false,"start_time":"2025-08-27T02:44:27.081079","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:08.648947Z","iopub.execute_input":"2025-09-26T01:51:08.649258Z","iopub.status.idle":"2025-09-26T01:51:08.653893Z","shell.execute_reply.started":"2025-09-26T01:51:08.649230Z","shell.execute_reply":"2025-09-26T01:51:08.653099Z"},"papermill":{"duration":0.016812,"end_time":"2025-08-27T02:44:27.122968","exception":false,"start_time":"2025-08-27T02:44:27.106156","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nfrom __future__ import print_function, division\n\n# ===========================\n# CONFIGURACIÓN INICIAL\n# ===========================\n\n# tqdm es una librería para mostrar barras de progreso en ciclos (loops).\n# \"tqdm.auto\" detecta si estamos en un notebook (como Kaggle o Jupyter)\n# o en una terminal, y se adapta automáticamente sin mostrar advertencias.\nfrom tqdm.auto import tqdm\ntqdm.pandas()   # Integra tqdm con pandas → se ven barras de progreso en operaciones de pandas.\n\n# ===========================\n# MANEJO DE WEIGHTS & BIASES (wandb)\n# ===========================\n# wandb es una herramienta para registrar experimentos de machine learning.\n# En Kaggle a veces genera errores o no queremos usarlo.\n# Con esta configuración lo desactivamos por defecto y creamos un \"plan B\"\n# para que el código siga funcionando aunque wandb falle o no esté instalado.\n\nimport os\nos.environ.setdefault(\"WANDB_DISABLED\", \"true\")  # Kaggle lo desactiva automáticamente\n\ntry:\n    import wandb  # Intentamos importar wandb\nexcept Exception as e:\n    # Si falla la importación, creamos una clase de \"simulación\"\n    # que actúa como reemplazo básico (stub).\n    # Así, cuando en el código se llame a wandb.init() o wandb.log(),\n    # no se producirá un error.\n    class _WandbStub:\n        def init(self, *args, **kwargs):\n            class _Ctx:\n                def __enter__(self): return self\n                def __exit__(self, exc_type, exc, tb): pass\n            return _Ctx()\n        def log(self, *args, **kwargs): pass\n        def watch(self, *args, **kwargs): pass\n        def finish(self, *args, **kwargs): pass\n\n    wandb = _WandbStub()  # Reemplazamos wandb por el \"stub\"\n\n# ===========================\n# IMPORTACIÓN DE LIBRERÍAS\n# ===========================\n# Estas librerías cubren diferentes tareas:\n# - Numpy/Pandas: análisis de datos\n# - PyTorch/Torchvision: redes neuronales\n# - Albumentations: aumentación de imágenes\n# - Scikit-learn: partición de datos\n# - OpenCV/Skimage: procesamiento de imágenes\n# - Matplotlib: visualización\n\nimport numpy as np\nimport pandas as pd\nfrom numpy.typing import NDArray\nfrom functools import reduce\nfrom itertools import islice, chain\nimport math, copy\n\nfrom PIL import Image\n\nimport torch\nfrom torch import nn, Tensor\nfrom torch.optim import Optimizer\nimport torch.nn.functional as F\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom torchsummary import summary  # Muestra la arquitectura de la red\n\n# Albumentations: librería de aumentación de datos para imágenes\nimport albumentations as A\n\nfrom sklearn.model_selection import train_test_split\nfrom multiprocessing import cpu_count  # Para paralelizar procesos\n\nimport os.path as osp\nfrom skimage import io, transform\nimport matplotlib.pyplot as plt\nimport typing as ty\nimport cv2\n\nplt.ion()  # Activa el \"modo interactivo\" → las gráficas se actualizan automáticamente.\n\n# ===========================\n# EXPLORACIÓN DE DATOS EN KAGGLE\n# ===========================\n# Kaggle guarda los datasets en la carpeta \"/kaggle/input\".\n# El siguiente bloque recorre esa carpeta y muestra los primeros 10 archivos encontrados.\n# Esto ayuda a verificar qué datos tenemos disponibles sin abrir manualmente el explorador.\nfor root, dirs, filenames in os.walk('/kaggle/input'):\n    for i, filepath in enumerate(filenames):\n        if i >= 10:  # Solo mostramos hasta 10 archivos para no saturar la salida\n            print()\n            break\n        print(osp.join(root, filepath))\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-09-26T01:51:08.654748Z","iopub.execute_input":"2025-09-26T01:51:08.655078Z","iopub.status.idle":"2025-09-26T01:51:54.309125Z","shell.execute_reply.started":"2025-09-26T01:51:08.655056Z","shell.execute_reply":"2025-09-26T01:51:54.308330Z"},"papermill":{"duration":47.013501,"end_time":"2025-08-27T02:45:14.147641","exception":false,"start_time":"2025-08-27T02:44:27.13414","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/check_version.py:147: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n  data = fetch_version_info()\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/aa-iv-2025-ii-object-localization/sample_submission.csv\n/kaggle/input/aa-iv-2025-ii-object-localization/train.csv\n/kaggle/input/aa-iv-2025-ii-object-localization/test.csv\n/kaggle/input/aa-iv-2025-ii-object-localization/images/videoplayback-1-_mp4-6_jpg.rf.e2195c50e4aa68ffc18f41c80fd7d235.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_4861_mp4-43_jpg.rf.4271f075e21c21ee8dc01731c6a7ea89.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_4861_mp4-56_jpg.rf.2c4bd7a2dd787d0344f2b49af88f21f1.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_3100_mp4-24_jpg.rf.894c40eafa77cd73ff50a69982e3f924.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_4860_mp4-45_jpg.rf.29bb394fd84df979b2a6096746751f42.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_4861_mp4-9_jpg.rf.bcc352b97426c7378bcd8004247f4433.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/video_CDC-YOUTUBE_mp4-41_jpg.rf.4f56be4b40c9775509474d515489f5a5.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_4921-2_mp4-124_jpg.rf.60e2e62f7f6c331d5960bf81261d0d8c.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_0871_MOV-30_jpg.rf.4bca57028e6bc86bf4ed9604468dc336.jpg\n/kaggle/input/aa-iv-2025-ii-object-localization/images/IMG_4860_mp4-4_jpg.rf.4ebe7129b4b9ebff2910ba3953e16a6b.jpg\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Ahora, vamos a crear la estructura del dataset","metadata":{"papermill":{"duration":0.01112,"end_time":"2025-08-27T02:45:14.170372","exception":false,"start_time":"2025-08-27T02:45:14.159252","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ===========================\n# Configuración básica en PyTorch\n# ===========================\n\ntorch.manual_seed(32)  \n# Fija la semilla para que los resultados sean reproducibles.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Usando {device}')  \n# Selecciona GPU si está disponible, de lo contrario usa CPU.\n\ntest = torch.ones((100, 100)).to(device)  \n# Crea un tensor de prueba en el dispositivo seleccionado (CPU o GPU).\n\ndel test  \n# Elimina el tensor de la memoria.\n\ntorch.cuda.empty_cache()  \n# Limpia la memoria de la GPU, dejándola lista para entrenar modelos.\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:54.310897Z","iopub.execute_input":"2025-09-26T01:51:54.311293Z","iopub.status.idle":"2025-09-26T01:51:54.566982Z","shell.execute_reply.started":"2025-09-26T01:51:54.311274Z","shell.execute_reply":"2025-09-26T01:51:54.566354Z"},"papermill":{"duration":0.226272,"end_time":"2025-08-27T02:45:14.4085","exception":false,"start_time":"2025-08-27T02:45:14.182228","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Usando cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ===========================\n# CONFIGURACIÓN DE DIRECTORIOS Y PARÁMETROS\n# ===========================\n\nDATA_DIR = '/kaggle/input/aa-iv-2025-ii-object-localization'  # Carpeta donde Kaggle guarda el dataset (solo lectura)\nWORK_DIR = '/kaggle/working'                                  # Carpeta de trabajo (aquí se guardan outputs y resultados)\nBATCH_SIZE = 32                                               # Tamaño de lote (batch) para el entrenamiento del modelo\n\n# Ruta donde están guardadas las imágenes\nimg_dir = osp.join(DATA_DIR, \"images\")\n\n# ===========================\n# CARGA DEL DATASET\n# ===========================\n\n# Leemos el archivo CSV de entrenamiento que contiene:\n# - nombre de la imagen\n# - coordenadas de la caja delimitadora (bounding box: xmin, ymin, xmax, ymax)\n# - clase del objeto\ndf = pd.read_csv(osp.join(DATA_DIR, \"train.csv\"))\n\n# ===========================\n# MAPEO DE CLASES A IDs\n# ===========================\n\n# Diccionario para convertir las clases (texto) en identificadores numéricos\n# Diccionario que asigna un número a cada clase:\n# - \"no-mask\" → 0\n# - \"mask\"    → 1\nobj2id  = {'no-mask':0,'mask':1}\n\n# Diccionario inverso: convertir IDs numéricos en nombres de clases\nid2obj  = {0:'no-mask',1:'mask'}\n\n# Crear nueva columna \"class_id\" en el DataFrame con el valor numérico de la clase\ndf[\"class_id\"] = df[\"class\"].map(obj2id)\n\n# ===========================\n# SELECCIÓN DE COLUMNAS ÚTILES\n# ===========================\n\n# Definimos qué columnas necesitamos realmente del dataset\ncolumns_f=['filename','xmin','ymin','xmax','ymax','class','class_id']\n\n# Nos quedamos únicamente con esas columnas\ndf = df[columns_f].copy()\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:54.567688Z","iopub.execute_input":"2025-09-26T01:51:54.567880Z","iopub.status.idle":"2025-09-26T01:51:54.593543Z","shell.execute_reply.started":"2025-09-26T01:51:54.567864Z","shell.execute_reply":"2025-09-26T01:51:54.592908Z"},"papermill":{"duration":0.039952,"end_time":"2025-08-27T02:45:14.460024","exception":false,"start_time":"2025-08-27T02:45:14.420072","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Exploremos un poco los datos","metadata":{"papermill":{"duration":0.011344,"end_time":"2025-08-27T02:45:14.482993","exception":false,"start_time":"2025-08-27T02:45:14.471649","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:54.594304Z","iopub.execute_input":"2025-09-26T01:51:54.594508Z","iopub.status.idle":"2025-09-26T01:51:54.615715Z","shell.execute_reply.started":"2025-09-26T01:51:54.594492Z","shell.execute_reply":"2025-09-26T01:51:54.614879Z"},"papermill":{"duration":0.036382,"end_time":"2025-08-27T02:45:14.580777","exception":false,"start_time":"2025-08-27T02:45:14.544395","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                              filename  xmin  ymin  xmax  \\\n0    video_CDC-YOUTUBE_mp4-63_jpg.rf.2f4f64f6ef712f...   315   249   468   \n1    IMG_4860_mp4-36_jpg.rf.01a053cabddff2cdd19f04e...   257   237   299   \n2    IMG_1491_mp4-12_jpg.rf.9df64033aebef44b8bb9a6a...   291   245   582   \n3    IMG_4861_mp4-64_jpg.rf.74ab6d1da8a1fa9b8fbb576...   231   229   577   \n4    IMG_9950-1-_mp4-83_jpg.rf.74dca33810c23ba144d8...   107   168   515   \n..                                                 ...   ...   ...   ...   \n214  videoplayback-1-_mp4-58_jpg.rf.bfdf3258d74f87d...   408   168   465   \n215  video_CDC-YOUTUBE_mp4-36_jpg.rf.5d17748e659665...   181   232   350   \n216  IMG_4861_mp4-38_jpg.rf.880a11c3ebf59b3d0cf988f...   112   179   413   \n217  How-to-Properly-Wear-a-Face-Mask-_-UC-San-Dieg...   268   134   382   \n218  videoplayback-1-_mp4-28_jpg.rf.ffdc6b183c1a5f9...   396    85   479   \n\n     ymax    class  class_id  \n0     374  no-mask         0  \n1     264  no-mask         0  \n2     449     mask         1  \n3     420  no-mask         0  \n4     469  no-mask         0  \n..    ...      ...       ...  \n214   212  no-mask         0  \n215   356     mask         1  \n216   438  no-mask         0  \n217   422  no-mask         0  \n218   153  no-mask         0  \n\n[219 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>xmax</th>\n      <th>ymax</th>\n      <th>class</th>\n      <th>class_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>video_CDC-YOUTUBE_mp4-63_jpg.rf.2f4f64f6ef712f...</td>\n      <td>315</td>\n      <td>249</td>\n      <td>468</td>\n      <td>374</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>IMG_4860_mp4-36_jpg.rf.01a053cabddff2cdd19f04e...</td>\n      <td>257</td>\n      <td>237</td>\n      <td>299</td>\n      <td>264</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>IMG_1491_mp4-12_jpg.rf.9df64033aebef44b8bb9a6a...</td>\n      <td>291</td>\n      <td>245</td>\n      <td>582</td>\n      <td>449</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>IMG_4861_mp4-64_jpg.rf.74ab6d1da8a1fa9b8fbb576...</td>\n      <td>231</td>\n      <td>229</td>\n      <td>577</td>\n      <td>420</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>IMG_9950-1-_mp4-83_jpg.rf.74dca33810c23ba144d8...</td>\n      <td>107</td>\n      <td>168</td>\n      <td>515</td>\n      <td>469</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>214</th>\n      <td>videoplayback-1-_mp4-58_jpg.rf.bfdf3258d74f87d...</td>\n      <td>408</td>\n      <td>168</td>\n      <td>465</td>\n      <td>212</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>215</th>\n      <td>video_CDC-YOUTUBE_mp4-36_jpg.rf.5d17748e659665...</td>\n      <td>181</td>\n      <td>232</td>\n      <td>350</td>\n      <td>356</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>216</th>\n      <td>IMG_4861_mp4-38_jpg.rf.880a11c3ebf59b3d0cf988f...</td>\n      <td>112</td>\n      <td>179</td>\n      <td>413</td>\n      <td>438</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>217</th>\n      <td>How-to-Properly-Wear-a-Face-Mask-_-UC-San-Dieg...</td>\n      <td>268</td>\n      <td>134</td>\n      <td>382</td>\n      <td>422</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>218</th>\n      <td>videoplayback-1-_mp4-28_jpg.rf.ffdc6b183c1a5f9...</td>\n      <td>396</td>\n      <td>85</td>\n      <td>479</td>\n      <td>153</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>219 rows × 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# ===========================\n# CARGA DE UNA IMAGEN DE EJEMPLO\n# ===========================\n\n# Construimos la ruta completa a un archivo de imagen específico.\nimg_filename = osp.join(DATA_DIR, \"images\", 'IMG_1493_mp4-21_jpg.rf.c5a3e237451e64e0674d5b0a6d556c25.jpg')\n\n# 1) Lectura de la imagen con OpenCV (cv2)\n# ----------------------------------------\n# OpenCV lee las imágenes en formato BGR (Blue, Green, Red) por defecto.\nimg1 = cv2.imread(img_filename)\n\n# Convertimos de BGR a RGB para que los colores sean correctos al visualizar con matplotlib.\nimg1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n\n# 2) Lectura de la misma imagen con skimage (io.imread)\n# -----------------------------------------------------\n# La función `io.imread` de scikit-image lee las imágenes directamente en formato RGB,\n# por lo tanto no es necesario hacer la conversión de colores.\nimg2 = io.imread(img_filename)\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:54.616551Z","iopub.execute_input":"2025-09-26T01:51:54.617197Z","iopub.status.idle":"2025-09-26T01:51:54.693503Z","shell.execute_reply.started":"2025-09-26T01:51:54.617171Z","shell.execute_reply":"2025-09-26T01:51:54.692884Z"},"papermill":{"duration":0.096843,"end_time":"2025-08-27T02:45:14.690536","exception":false,"start_time":"2025-08-27T02:45:14.593693","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Mostramos la forma original de la imagen: (alto, ancho, canales) → (H, W, C)\nprint(img1.shape)\n\n# Transponemos para pasar a formato (canales, alto, ancho) → (C, H, W),\n# que es el requerido por PyTorch. -> como se vió en clase, pytorch trabaja\n# con Channel first, no Channel last.\nprint(img1.transpose((2,0,1)).shape)\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:54.694209Z","iopub.execute_input":"2025-09-26T01:51:54.694427Z","iopub.status.idle":"2025-09-26T01:51:54.698601Z","shell.execute_reply.started":"2025-09-26T01:51:54.694411Z","shell.execute_reply":"2025-09-26T01:51:54.697974Z"},"papermill":{"duration":0.017137,"end_time":"2025-08-27T02:45:14.719636","exception":false,"start_time":"2025-08-27T02:45:14.702499","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"(640, 640, 3)\n(3, 640, 640)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ===========================\n# ANÁLISIS EXPLORATORIO DE IMÁGENES\n# ===========================\n\n# Obtenemos la lista de nombres de archivos de las imágenes desde el DataFrame\nlist_image = list(df.filename)\n\n# Inicializamos listas vacías donde guardaremos información de cada imagen\ndata_shape = []   # Guardará la forma completa de la imagen (alto, ancho, canales)\ndata_dim = []     # Guardará el número de dimensiones (ej: 2 para escala de grises, 3 para RGB)\ndata_w = []       # Guardará el ancho de la imagen\ndata_h = []       # Guardará la altura de la imagen\n\n# Recorremos todas las imágenes con una barra de progreso (tqdm)\nfor i in tqdm(list_image):  \n    # Construimos la ruta completa de la imagen\n    ruta_imagen = osp.join(img_dir, i)\n    \n    # Leemos la imagen con skimage → obtenemos forma y número de dimensiones\n    imagen = io.imread(ruta_imagen)\n    shapes = imagen.shape      # Ejemplo: (300, 400, 3)\n    dimen = imagen.ndim        # Ejemplo: 3 si es RGB, 2 si es escala de grises\n    \n    # Leemos la imagen con PIL → obtenemos ancho y alto\n    imagen = Image.open(ruta_imagen)\n    w, h = imagen.size         # size devuelve (ancho, alto)\n    \n    # Guardamos toda la información en las listas\n    data_w.append(w)\n    data_h.append(h)\n    data_shape.append(shapes)\n    data_dim.append(dimen)\n\n# Construimos un DataFrame con toda la información recopilada\ndata_w_h = pd.DataFrame(\n    [list_image, data_shape, data_dim, data_w, data_h]\n).T.rename(columns={0:'filename', 1:'shapes', 2:'ndim', 3:'w', 4:'h'})\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:54.699401Z","iopub.execute_input":"2025-09-26T01:51:54.699656Z","iopub.status.idle":"2025-09-26T01:51:56.252723Z","shell.execute_reply.started":"2025-09-26T01:51:54.699638Z","shell.execute_reply":"2025-09-26T01:51:56.251813Z"},"papermill":{"duration":1.90919,"end_time":"2025-08-27T02:45:16.641776","exception":false,"start_time":"2025-08-27T02:45:14.732586","status":"completed"},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/219 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5d9441a05ab469e8bfd917d11398e35"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Contamos cuántas veces aparece cada forma (alto, ancho, canales) en el dataset.\ndata_w_h['shapes'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:56.255185Z","iopub.execute_input":"2025-09-26T01:51:56.255413Z","iopub.status.idle":"2025-09-26T01:51:56.263832Z","shell.execute_reply.started":"2025-09-26T01:51:56.255395Z","shell.execute_reply":"2025-09-26T01:51:56.263109Z"},"papermill":{"duration":0.02173,"end_time":"2025-08-27T02:45:16.742457","exception":false,"start_time":"2025-08-27T02:45:16.720727","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"shapes\n(640, 640, 3)    219\nName: count, dtype: int64"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Contamos cuántas veces aparece cada clase en el dataset (en formato de texto).\n# Esto muestra la distribución de imágenes entre las clases \"mask\" y \"no-mask\".\ndf['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:56.264730Z","iopub.execute_input":"2025-09-26T01:51:56.264984Z","iopub.status.idle":"2025-09-26T01:51:56.280572Z","shell.execute_reply.started":"2025-09-26T01:51:56.264962Z","shell.execute_reply":"2025-09-26T01:51:56.280021Z"},"papermill":{"duration":0.02019,"end_time":"2025-08-27T02:45:16.808855","exception":false,"start_time":"2025-08-27T02:45:16.788665","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"class\nno-mask    135\nmask        84\nName: count, dtype: int64"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Verificamos si existen errores en las coordenadas de las cajas delimitadoras (bounding boxes).\n\n# Caso 1: xmin >= xmax\n# Esto indicaría que el lado izquierdo de la caja está a la derecha del lado derecho → caja inválida.\n\n# Caso 2: ymin >= ymax\n# Esto indicaría que la parte superior de la caja está por debajo de la parte inferior → caja inválida.\ndf[df['xmin']>=df['xmax']].shape, df[df['ymin']>=df['ymax']].shape","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:56.281345Z","iopub.execute_input":"2025-09-26T01:51:56.281624Z","iopub.status.idle":"2025-09-26T01:51:56.298437Z","shell.execute_reply.started":"2025-09-26T01:51:56.281596Z","shell.execute_reply":"2025-09-26T01:51:56.297729Z"},"papermill":{"duration":0.020251,"end_time":"2025-08-27T02:45:16.841426","exception":false,"start_time":"2025-08-27T02:45:16.821175","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"((0, 7), (0, 7))"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# Normalizamos los bounding box","metadata":{"papermill":{"duration":0.012065,"end_time":"2025-08-27T02:45:16.895734","exception":false,"start_time":"2025-08-27T02:45:16.883669","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Mostramos estadísticas básicas (min, max, promedio, etc.) de las coordenadas de las bounding boxes: ymin, ymax, xmin, xmax.\n# Sirve para verificar que las cajas estén dentro de los rangos esperados\n# y detectar valores anómalos en las anotaciones.\nprint(df[[\"ymin\", \"ymax\", \"xmin\", \"xmax\"]].describe())","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:56.299227Z","iopub.execute_input":"2025-09-26T01:51:56.299509Z","iopub.status.idle":"2025-09-26T01:51:56.331046Z","shell.execute_reply.started":"2025-09-26T01:51:56.299487Z","shell.execute_reply":"2025-09-26T01:51:56.330491Z"},"papermill":{"duration":0.030967,"end_time":"2025-08-27T02:45:16.939163","exception":false,"start_time":"2025-08-27T02:45:16.908196","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"             ymin        ymax        xmin        xmax\ncount  219.000000  219.000000  219.000000  219.000000\nmean   175.296804  371.278539  217.068493  446.849315\nstd     67.509690   97.632620  108.656136  104.015128\nmin      3.000000  145.000000    0.000000  146.000000\n25%    134.000000  333.000000  154.000000  382.500000\n50%    168.000000  394.000000  204.000000  466.000000\n75%    224.000000  431.500000  274.500000  513.000000\nmax    420.000000  640.000000  557.000000  640.000000\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ===========================\n# NORMALIZACIÓN DE COORDENADAS DE LAS BOUNDING BOXES\n# ===========================\n\n# Definimos la altura y el ancho reales de las imágenes del dataset.\n# En este caso todas son de 640 x 640 píxeles.\nh_real = 640\nw_real = 640\n\n# Normalizamos las coordenadas de las cajas delimitadoras dividiéndolas\n# entre la altura o el ancho correspondiente.\n# De esta forma los valores quedan entre 0 y 1, lo que facilita el entrenamiento.\ndf[[\"ymin\", \"ymax\"]] = df[[\"ymin\", \"ymax\"]].div(h_real, axis=0)\ndf[[\"xmin\", \"xmax\"]] = df[[\"xmin\", \"xmax\"]].div(w_real, axis=0)","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:56.331697Z","iopub.execute_input":"2025-09-26T01:51:56.331901Z","iopub.status.idle":"2025-09-26T01:51:56.338939Z","shell.execute_reply.started":"2025-09-26T01:51:56.331886Z","shell.execute_reply":"2025-09-26T01:51:56.338214Z"},"papermill":{"duration":0.021616,"end_time":"2025-08-27T02:45:16.97327","exception":false,"start_time":"2025-08-27T02:45:16.951654","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Estadisticos normalizados\nprint(df[[\"ymin\", \"ymax\", \"xmin\", \"xmax\"]].describe())","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:56.339653Z","iopub.execute_input":"2025-09-26T01:51:56.339905Z","iopub.status.idle":"2025-09-26T01:51:56.365099Z","shell.execute_reply.started":"2025-09-26T01:51:56.339881Z","shell.execute_reply":"2025-09-26T01:51:56.364255Z"},"papermill":{"duration":0.028876,"end_time":"2025-08-27T02:45:17.014581","exception":false,"start_time":"2025-08-27T02:45:16.985705","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"             ymin        ymax        xmin        xmax\ncount  219.000000  219.000000  219.000000  219.000000\nmean     0.273901    0.580123    0.339170    0.698202\nstd      0.105484    0.152551    0.169775    0.162524\nmin      0.004687    0.226562    0.000000    0.228125\n25%      0.209375    0.520312    0.240625    0.597656\n50%      0.262500    0.615625    0.318750    0.728125\n75%      0.350000    0.674219    0.428906    0.801562\nmax      0.656250    1.000000    0.870313    1.000000\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Ahora visualizamos el df con los bbox normalizados\ndf","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:56.365818Z","iopub.execute_input":"2025-09-26T01:51:56.366095Z","iopub.status.idle":"2025-09-26T01:51:56.376979Z","shell.execute_reply.started":"2025-09-26T01:51:56.366075Z","shell.execute_reply":"2025-09-26T01:51:56.376366Z"},"papermill":{"duration":0.025314,"end_time":"2025-08-27T02:45:17.052518","exception":false,"start_time":"2025-08-27T02:45:17.027204","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                              filename      xmin      ymin  \\\n0    video_CDC-YOUTUBE_mp4-63_jpg.rf.2f4f64f6ef712f...  0.492188  0.389062   \n1    IMG_4860_mp4-36_jpg.rf.01a053cabddff2cdd19f04e...  0.401562  0.370312   \n2    IMG_1491_mp4-12_jpg.rf.9df64033aebef44b8bb9a6a...  0.454688  0.382812   \n3    IMG_4861_mp4-64_jpg.rf.74ab6d1da8a1fa9b8fbb576...  0.360938  0.357812   \n4    IMG_9950-1-_mp4-83_jpg.rf.74dca33810c23ba144d8...  0.167187  0.262500   \n..                                                 ...       ...       ...   \n214  videoplayback-1-_mp4-58_jpg.rf.bfdf3258d74f87d...  0.637500  0.262500   \n215  video_CDC-YOUTUBE_mp4-36_jpg.rf.5d17748e659665...  0.282813  0.362500   \n216  IMG_4861_mp4-38_jpg.rf.880a11c3ebf59b3d0cf988f...  0.175000  0.279687   \n217  How-to-Properly-Wear-a-Face-Mask-_-UC-San-Dieg...  0.418750  0.209375   \n218  videoplayback-1-_mp4-28_jpg.rf.ffdc6b183c1a5f9...  0.618750  0.132812   \n\n         xmax      ymax    class  class_id  \n0    0.731250  0.584375  no-mask         0  \n1    0.467187  0.412500  no-mask         0  \n2    0.909375  0.701562     mask         1  \n3    0.901563  0.656250  no-mask         0  \n4    0.804688  0.732812  no-mask         0  \n..        ...       ...      ...       ...  \n214  0.726562  0.331250  no-mask         0  \n215  0.546875  0.556250     mask         1  \n216  0.645312  0.684375  no-mask         0  \n217  0.596875  0.659375  no-mask         0  \n218  0.748437  0.239063  no-mask         0  \n\n[219 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>xmax</th>\n      <th>ymax</th>\n      <th>class</th>\n      <th>class_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>video_CDC-YOUTUBE_mp4-63_jpg.rf.2f4f64f6ef712f...</td>\n      <td>0.492188</td>\n      <td>0.389062</td>\n      <td>0.731250</td>\n      <td>0.584375</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>IMG_4860_mp4-36_jpg.rf.01a053cabddff2cdd19f04e...</td>\n      <td>0.401562</td>\n      <td>0.370312</td>\n      <td>0.467187</td>\n      <td>0.412500</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>IMG_1491_mp4-12_jpg.rf.9df64033aebef44b8bb9a6a...</td>\n      <td>0.454688</td>\n      <td>0.382812</td>\n      <td>0.909375</td>\n      <td>0.701562</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>IMG_4861_mp4-64_jpg.rf.74ab6d1da8a1fa9b8fbb576...</td>\n      <td>0.360938</td>\n      <td>0.357812</td>\n      <td>0.901563</td>\n      <td>0.656250</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>IMG_9950-1-_mp4-83_jpg.rf.74dca33810c23ba144d8...</td>\n      <td>0.167187</td>\n      <td>0.262500</td>\n      <td>0.804688</td>\n      <td>0.732812</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>214</th>\n      <td>videoplayback-1-_mp4-58_jpg.rf.bfdf3258d74f87d...</td>\n      <td>0.637500</td>\n      <td>0.262500</td>\n      <td>0.726562</td>\n      <td>0.331250</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>215</th>\n      <td>video_CDC-YOUTUBE_mp4-36_jpg.rf.5d17748e659665...</td>\n      <td>0.282813</td>\n      <td>0.362500</td>\n      <td>0.546875</td>\n      <td>0.556250</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>216</th>\n      <td>IMG_4861_mp4-38_jpg.rf.880a11c3ebf59b3d0cf988f...</td>\n      <td>0.175000</td>\n      <td>0.279687</td>\n      <td>0.645312</td>\n      <td>0.684375</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>217</th>\n      <td>How-to-Properly-Wear-a-Face-Mask-_-UC-San-Dieg...</td>\n      <td>0.418750</td>\n      <td>0.209375</td>\n      <td>0.596875</td>\n      <td>0.659375</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>218</th>\n      <td>videoplayback-1-_mp4-28_jpg.rf.ffdc6b183c1a5f9...</td>\n      <td>0.618750</td>\n      <td>0.132812</td>\n      <td>0.748437</td>\n      <td>0.239063</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>219 rows × 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# ===========================\n# PARTICIÓN ENTRENAMIENTO / VALIDACIÓN (estratificada)\n# ===========================\n# Dividimos el DataFrame 'df' en dos subconjuntos:\n#  - train_df: datos para entrenar el modelo (75%)\n#  - val_df:   datos para validar el modelo (25%)\n#\n# Parámetros clave:\n#  - stratify=df['class_id']  → mantiene la misma proporción de clases en\n#    train y val (muy importante si el dataset está desbalanceado).\n#  - test_size=0.25           → 25% de los datos va a validación.\n#  - random_state=42           → semilla para reproducibilidad del split.\ntrain_df, val_df = train_test_split(\n    df, stratify=df['class_id'], test_size=0.25, random_state=42\n)\n\n# Tamaños resultantes de cada partición\nprint(train_df.shape)\nprint(val_df.shape)","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:56.377716Z","iopub.execute_input":"2025-09-26T01:51:56.377965Z","iopub.status.idle":"2025-09-26T01:51:56.398350Z","shell.execute_reply.started":"2025-09-26T01:51:56.377947Z","shell.execute_reply":"2025-09-26T01:51:56.397639Z"},"papermill":{"duration":0.020804,"end_time":"2025-08-27T02:45:17.086199","exception":false,"start_time":"2025-08-27T02:45:17.065395","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"(164, 7)\n(55, 7)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"**Importante**: El set de entrenamiento debe tener información acerca de la clase y las coordenadas correspondientes a los bbox","metadata":{"papermill":{"duration":0.011901,"end_time":"2025-08-27T02:45:17.110277","exception":false,"start_time":"2025-08-27T02:45:17.098376","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ===========================\n# DISTRIBUCIÓN DE CLASES EN TRAIN (en %)\n# ===========================\n# ahora verificamos que la distribución de las clases se mantengan en el train\n# Útil para verificar que el split estratificado mantuvo el balance de clases.\ntrain_df['class'].value_counts(normalize=True) * 100","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:56.398941Z","iopub.execute_input":"2025-09-26T01:51:56.399257Z","iopub.status.idle":"2025-09-26T01:51:56.419857Z","shell.execute_reply.started":"2025-09-26T01:51:56.399239Z","shell.execute_reply":"2025-09-26T01:51:56.419233Z"},"papermill":{"duration":0.019589,"end_time":"2025-08-27T02:45:17.141931","exception":false,"start_time":"2025-08-27T02:45:17.122342","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"class\nno-mask    61.585366\nmask       38.414634\nName: proportion, dtype: float64"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"Hay que tener presente que el conjunto de prueba solo contiene el nombre de archivo de cada imagen (se puede verificar en la data), por lo que tenemos que generar predicciones y enviarlas a la competencia de Kaggle.","metadata":{"papermill":{"duration":0.011973,"end_time":"2025-08-27T02:45:17.197885","exception":false,"start_time":"2025-08-27T02:45:17.185912","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ---------------------------------------------------------------------------\n# FIRMAS DE TRANSFORMACIONES (TIPADO)\n# ---------------------------------------------------------------------------\n# Cada transformación debe recibir y devolver un diccionario de numpy arrays\n# con claves como 'image', 'bbox' y/o 'class_id'. Esto ayuda a documentar\n# la interfaz esperada por el pipeline de datos.\ntransform_func_inp_signature = ty.Dict[str, NDArray[np.float_]]\ntransform_func_signature = ty.Callable[\n    [transform_func_inp_signature],  # entrada: sample dict\n    transform_func_inp_signature     # salida: sample dict (misma estructura)\n]\n\nclass maskDataset(Dataset):\n    \"\"\"\n    Location image dataset\n    \"\"\"\n    def __init__(\n        self, \n        df: pd.DataFrame, \n        root_dir: str, \n        labeled: bool = True,\n        transform: ty.Optional[ty.List[transform_func_signature]] = None,\n        output_size: ty.Optional[tuple] = None  \n    ) -> None:\n        # df: DataFrame con las anotaciones. Se asume un orden de columnas donde:\n        #  [0] = 'filename', [1:5] = ['xmin','ymin','xmax','ymax'], y además 'class_id'.\n        # root_dir: carpeta donde viven las imágenes.\n        # labeled: si True, el __getitem__ añade 'bbox' y 'class_id' al sample.\n        # transform: lista/callable de transformaciones que operan sobre el dict sample.\n        # output_size: tamaño de redimensionado de imagen (w, h). Si las bboxes están\n        #              normalizadas en [0,1], no requieren ajuste al hacer resize.\n        self.df = df\n        self.root_dir = root_dir\n        self.transform = transform\n        self.labeled = labeled\n        self.output_size = output_size  # Almacenar el tamaño de salida\n        \n    def __len__(self):\n        # Tamaño del dataset = número de filas del DataFrame.\n        return self.df.shape[0]\n    \n    def __getitem__(self, idx: int) -> transform_func_signature: \n        # Soporte para indexación con tensores (DataLoader puede pasar un tensor).\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        # Read image\n        # Construye la ruta absoluta a la imagen y la carga con skimage (RGB por defecto).\n        img_name = os.path.join(self.root_dir, self.df.filename.iloc[idx])\n        #img_name = os.path.join(self.root_dir, self.df.iloc[idx]['filename'])\n        image = io.imread(img_name)\n        #image = cv2.imread(img_name)\n        \n        \n        if image is None:\n            # Falla temprano si la imagen no existe o no pudo cargarse.\n            raise FileNotFoundError(f\"Image not found: {img_name}\")\n            \n        # Normalización de canales:\n        # - Si viene en escala de grises (ndim == 2), conviértela a 3 canales.\n        # - Si viene con alfa (RGBA), descarta el canal alfa y deja RGB.\n        if image.ndim == 2:  # Si la imagen está en escala de grises\n            image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)  # Convertir a RGB\n            # Nota: típicamente sería cv2.COLOR_GRAY2RGB si la fuente es escala de grises.\n            # Aquí se mantiene tal cual (no modificar código) y se deja la advertencia.\n        elif image.shape[2] == 4:  # Si la imagen es RGBA\n            image = image[:, :, :3] \n            \n        # Redimensionar la imagen si se especifica un tamaño de salida\n        if self.output_size:\n            # cv2.resize espera (width, height). Asegúrate de pasar en ese orden.\n            # Si las bboxes estuvieran en píxeles, habría que reescalarlas también.\n            # Si están normalizadas en [0,1], no requieren ajuste.\n            image = cv2.resize(image, self.output_size)  # Redimensionar la imagen si es necesario\n        \n        # Armar el sample básico (siempre incluye la imagen en formato np.ndarray HxWxC).\n        sample = {'image': image}\n        \n        if self.labeled:\n            # Read labels\n            # class_id: entero con la clase. Se empaqueta como np.array de forma (1,).\n            img_class = self.df.class_id.iloc[idx]\n            # bbox: se toma por posición de columnas [1:5] → ['xmin','ymin','xmax','ymax'].\n            img_bbox = self.df.iloc[idx, 1:5]\n\n            # Convertir a numpy y fijar tipos. bbox → float; class_id → int.\n            # bbox queda con forma (1, 4) y class_id con forma (1,).\n            img_bbox = np.array([img_bbox]).astype('float')\n            img_class = np.array([img_class]).astype('int')\n            sample.update({'bbox': img_bbox, 'class_id': img_class})\n        \n        if self.transform:\n            sample = self.transform(sample)\n        \n        # Devuelve el sample listo para el DataLoader / modelo.\n        return sample\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:56.420702Z","iopub.execute_input":"2025-09-26T01:51:56.420928Z","iopub.status.idle":"2025-09-26T01:51:56.438891Z","shell.execute_reply.started":"2025-09-26T01:51:56.420912Z","shell.execute_reply":"2025-09-26T01:51:56.438223Z"},"papermill":{"duration":0.022815,"end_time":"2025-08-27T02:45:17.233024","exception":false,"start_time":"2025-08-27T02:45:17.210209","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def draw_bbox(img, bbox, color,thickness: int = 3):\n    # Dibuja un único cuadro delimitador (bounding box) sobre una imagen.\n    # Parámetros:\n    #   img      : np.ndarray (imagen en formato BGR si se usa OpenCV)\n    #   bbox     : iterable con 4 enteros/píxeles en el orden (xmin, ymin, xmax, ymax)\n    #              IMPORTANTE: estas coordenadas deben estar en píxeles, no normalizadas.\n    #   color    : tupla BGR (por ejemplo, (255,0,0) para azul en OpenCV)\n    #   thickness: grosor de la línea del rectángulo\n    # Retorna:\n    #   img con el rectángulo dibujado (la operación modifica la imagen in-place)\n    xmin, ymin, xmax, ymax = bbox\n    img = cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color, thickness)\n    return img\n\ndef normalize_bbox(bbox, h: int, w: int):\n    \"\"\"Escala las coordenadas normalizadas al tamaño real de la imagen.\"\"\"\n    # Convierte una caja en formato normalizado [0,1] a píxeles enteros según (w, h).\n    # Parámetros:\n    #   bbox: iterable [xmin_norm, ymin_norm, xmax_norm, ymax_norm] en [0,1]\n    #   h   : altura de la imagen en píxeles\n    #   w   : ancho de la imagen en píxeles\n    # Retorna:\n    #   lista [xmin_px, ymin_px, xmax_px, ymax_px] como enteros\n    return [\n        int(bbox[0] * w),  # xmin\n        int(bbox[1] * h),  # ymin\n        int(bbox[2] * w),  # xmax\n        int(bbox[3] * h),  # ymax\n    ]\n\ndef draw_bboxes(imgs, bboxes, colors,thickness):\n    \"\"\"Dibuja múltiples cuadros delimitadores en imágenes, escalando según h y w.\"\"\"\n    # Dibuja una lista de bounding boxes sobre una lista de imágenes.\n    # Parámetros:\n    #   imgs    : lista de imágenes (np.ndarray) de igual longitud que bboxes y colors\n    #   bboxes  : lista de cajas en píxeles [(xmin,ymin,xmax,ymax), ...]\n    #             NOTA: esta función asume que las cajas YA están en píxeles.\n    #   colors  : lista de colores BGR por cada imagen/caja (o reutilizadas externamente)\n    #   thickness: grosor del rectángulo\n    # Retorna:\n    #   lista de imágenes con las cajas dibujadas\n    for i, (img, bbox, color) in enumerate(zip(imgs, bboxes, colors)):\n        imgs[i] = draw_bbox(img, bbox, color,thickness)\n    return imgs\n\ndef draw_classes(imgs, classes, colors, origin, prefix: str ='',fontScale : int = 2):\n    \"\"\"Dibuja las clases en las imágenes.\"\"\"\n    # Escribe el nombre de la clase sobre cada imagen.\n    # Parámetros:\n    #   imgs     : lista de imágenes (np.ndarray)\n    #   classes  : iterable de ids de clase (p. ej., [[1], [0], ...] o [1,0,...])\n    #   colors   : lista de colores BGR para el texto\n    #   origin   : punto (x, y) donde iniciar el texto en cada imagen\n    #   prefix   : texto opcional para anteponer (por ejemplo, \"pred: \")\n    #   fontScale: tamaño de fuente en OpenCV\n    # Dependencias externas esperadas:\n    #   - Un diccionario global 'id2obj' para mapear id → nombre de clase.\n    for i, (img, class_id, color) in enumerate(zip(imgs, classes, colors)):\n        if type(c)==list:\n            name_class_=id2obj[classes[i]]\n        else:\n            name_class_=id2obj[classes[i][0]]\n        imgs[i] = cv2.putText(\n            img, f'{prefix}{name_class_}', \n            origin, cv2.FONT_HERSHEY_SIMPLEX,\n            fontScale , color, 2, cv2.LINE_AA\n        )\n    return imgs\n\ndef draw_predictions(imgs, classes, bboxes, colors, origin,thickness,fontScale):\n    \"\"\"\n    Combina las funciones anteriores para dibujar cuadros delimitadores\n    y clases en las imágenes.\n    \"\"\"\n    # Flujo:\n    #   1) Verifica que todas las listas tengan longitud > 0.\n    #   2) Si hay un solo color, lo replica para todas las imágenes.\n    #   3) Dibuja las cajas.\n    #   4) Dibuja las etiquetas de clase.\n    # Parámetros:\n    #   imgs, classes, bboxes: listas alineadas por índice\n    #   colors               : lista de colores BGR (o uno solo para todos)\n    #   origin               : punto (x,y) para el texto\n    #   thickness            : grosor del rectángulo\n    #   fontScale            : tamaño del texto\n    # Retorna:\n    #   lista de imágenes con predicciones (cajas + etiquetas) dibujadas\n    assert all(len(x) > 0 for x in [imgs, classes, bboxes, colors])\n    if len(colors) == 1:\n        colors = [colors[0] for _ in imgs]\n    imgs = draw_bboxes(imgs, bboxes, colors,thickness)\n    imgs = draw_classes(imgs, classes, colors, origin,\"\",fontScale)\n    return imgs\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:56.439553Z","iopub.execute_input":"2025-09-26T01:51:56.439744Z","iopub.status.idle":"2025-09-26T01:51:56.454799Z","shell.execute_reply.started":"2025-09-26T01:51:56.439730Z","shell.execute_reply":"2025-09-26T01:51:56.453931Z"},"papermill":{"duration":0.021979,"end_time":"2025-08-27T02:45:17.267367","exception":false,"start_time":"2025-08-27T02:45:17.245388","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"h, w, c = 256, 256, 3","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:56.455516Z","iopub.execute_input":"2025-09-26T01:51:56.455771Z","iopub.status.idle":"2025-09-26T01:51:56.474964Z","shell.execute_reply.started":"2025-09-26T01:51:56.455749Z","shell.execute_reply":"2025-09-26T01:51:56.474400Z"},"papermill":{"duration":0.016676,"end_time":"2025-08-27T02:45:17.296197","exception":false,"start_time":"2025-08-27T02:45:17.279521","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# ===========================\n# VISUALIZACIÓN DE MUESTRAS CON CAJAS Y CLASES\n# ===========================\n\n# Carpeta raíz donde están las imágenes del split de entrenamiento\ntrain_root_dir = osp.join(DATA_DIR, \"images\")#, \"train\"\n\n# Instanciamos el Dataset con el DataFrame de train y forzamos tamaño de salida (w, h)\ntrain_ds = maskDataset(train_df, root_dir=train_root_dir,output_size=(w,h))\n\n# Número de imágenes a mostrar y desde qué índice empezar\nnum_imgs = 6\nstart_idx = 0\n\n# Tomamos 'num_imgs' muestras consecutivas a partir de 'start_idx'\nsamples = [train_ds[i] for i in range(start_idx, num_imgs)]\n\n# Extraemos por separado las imágenes, bboxes y clases de cada sample\nimgs = [s['image'] for s in samples]\n# Convertimos las cajas normalizadas [0,1] a píxeles con (w,h) de salida\nbboxes = [normalize_bbox(s['bbox'].squeeze(),h,w) for s in samples]\nclasses = [s['class_id'] for s in samples]\n\n# Dibujamos predicciones: cajas + etiquetas\n# - colors: lista con un color (BGR) que se reutiliza para todas las imágenes\n# - origin: punto (x,y) para el texto (10% del ancho y alto)\n# - thickness y fontScale: grosor de línea y tamaño de fuente\nimgs = draw_predictions(imgs, classes, bboxes, [(0, 150, 0)], (int(w*0.1), int(h*0.1)),thickness = 1,fontScale=1)#(150, 10)\n\n# Creamos una figura grande y colocamos cada imagen en una subgráfica\nfig = plt.figure(figsize=(30, num_imgs))\n\nfor i, img in enumerate(imgs):\n    fig.add_subplot(1, num_imgs, i+1)\n    plt.imshow(img)\n\n# Mostramos el collage de imágenes con sus cajas y clases\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:56.475590Z","iopub.execute_input":"2025-09-26T01:51:56.475781Z"},"papermill":{"duration":0.911805,"end_time":"2025-08-27T02:45:18.22027","exception":false,"start_time":"2025-08-27T02:45:17.308465","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DEFINICION DE ARQUITECTURA CNN\n\nCada capa de la arquitectura cumple un rol específico en el proceso de abstracción de la información visual:\n\n-   **Layer 1**: recibe la imagen en sus tres canales de color (RGB) y produce 32 mapas de características. Su función principal es detectar patrones simples como bordes, colores y texturas básicas.\n    \n-   **Layer 2**: incrementa la representación a 64 canales. En este nivel, el modelo comienza a identificar combinaciones de los patrones previos, lo que permite reconocer formas más definidas y simples.\n    \n-   **Layer 3**: amplía la profundidad a 128 canales. Aquí se extraen representaciones más abstractas, como regiones u objetos parciales como regiones del rostro.\n    \n-   **Layer 4**: alcanza 256 canales. Esta última capa está orientada a la detección de conceptos de alto nivel, integrando la información de las capas anteriores para construir descripciones más completas de la imagen.\n    \n\nEn síntesis, a medida que se avanza en las capas, el modelo **incrementa la cantidad de canales** para capturar más información, mientras que **reduce la resolución espacial**, lo cual permite concentrar los detalles relevantes en un espacio más compacto y manejable.\n\nAdaptiveAvgPool2d: estandariza la salida de la CNN a un tamaño fijo, lo que simplifica la conexión con capas densas y permite flexibilidad con distintos tamaños de imagen.\n\n-init_weights: Permite usar HE-NORMAL. El obejtivo es inicializar los pesos en funcion de la red neuronal, para no hacer una inializacion aleatoria ","metadata":{"papermill":{"duration":0.028475,"end_time":"2025-08-27T02:45:18.276076","exception":false,"start_time":"2025-08-27T02:45:18.247601","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\nclass MyBackboneFromCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        def conv_block(in_channels, out_channels):\n            return nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 3, padding=1),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(out_channels, out_channels, 3, stride=2, padding=1),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(2)  # reduce resolución\n            )\n\n        self.layer1 = conv_block(3, 32)\n        self.layer2 = conv_block(32, 64)\n        self.layer3 = conv_block(64, 128)\n        self.layer4 = conv_block(128, 256)\n\n        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n        self.flatten = nn.Flatten()\n        self.dropout = nn.Dropout(0.5)\n\n        # aplicar inicialización He\n        self.apply(self.init_weights)\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Conv2d):\n            # He normal\n            init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            if m.bias is not None:\n                init.constant_(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            if m.bias is not None:\n                init.constant_(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            init.constant_(m.weight, 1)\n            init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.global_pool(x)  # [B, 256, 1, 1]\n        x = self.flatten(x)\n        x = self.dropout(x)\n        return x   # [B, 256]\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nown_model = MyBackboneFromCNN().to(device)\nown_model.eval()","metadata":{"execution":{"iopub.execute_input":"2025-09-26T01:51:57.518958Z","iopub.status.idle":"2025-09-26T01:51:57.564577Z"},"trusted":true},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"MyBackboneFromCNN(\n  (layer1): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (layer2): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (layer3): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (layer4): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (dropout): Dropout(p=0.5, inplace=False)\n)"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"summary(own_model, (3, 640, 640))","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:57.565337Z","iopub.execute_input":"2025-09-26T01:51:57.565553Z","iopub.status.idle":"2025-09-26T01:51:58.212578Z","shell.execute_reply.started":"2025-09-26T01:51:57.565538Z","shell.execute_reply":"2025-09-26T01:51:58.211843Z"},"papermill":{"duration":0.545073,"end_time":"2025-08-27T02:45:54.29023","exception":false,"start_time":"2025-08-27T02:45:53.745157","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 32, 640, 640]             896\n       BatchNorm2d-2         [-1, 32, 640, 640]              64\n              ReLU-3         [-1, 32, 640, 640]               0\n            Conv2d-4         [-1, 32, 320, 320]           9,248\n       BatchNorm2d-5         [-1, 32, 320, 320]              64\n              ReLU-6         [-1, 32, 320, 320]               0\n         MaxPool2d-7         [-1, 32, 160, 160]               0\n            Conv2d-8         [-1, 64, 160, 160]          18,496\n       BatchNorm2d-9         [-1, 64, 160, 160]             128\n             ReLU-10         [-1, 64, 160, 160]               0\n           Conv2d-11           [-1, 64, 80, 80]          36,928\n      BatchNorm2d-12           [-1, 64, 80, 80]             128\n             ReLU-13           [-1, 64, 80, 80]               0\n        MaxPool2d-14           [-1, 64, 40, 40]               0\n           Conv2d-15          [-1, 128, 40, 40]          73,856\n      BatchNorm2d-16          [-1, 128, 40, 40]             256\n             ReLU-17          [-1, 128, 40, 40]               0\n           Conv2d-18          [-1, 128, 20, 20]         147,584\n      BatchNorm2d-19          [-1, 128, 20, 20]             256\n             ReLU-20          [-1, 128, 20, 20]               0\n        MaxPool2d-21          [-1, 128, 10, 10]               0\n           Conv2d-22          [-1, 256, 10, 10]         295,168\n      BatchNorm2d-23          [-1, 256, 10, 10]             512\n             ReLU-24          [-1, 256, 10, 10]               0\n           Conv2d-25            [-1, 256, 5, 5]         590,080\n      BatchNorm2d-26            [-1, 256, 5, 5]             512\n             ReLU-27            [-1, 256, 5, 5]               0\n        MaxPool2d-28            [-1, 256, 2, 2]               0\nAdaptiveAvgPool2d-29            [-1, 256, 1, 1]               0\n          Flatten-30                  [-1, 256]               0\n          Dropout-31                  [-1, 256]               0\n================================================================\nTotal params: 1,174,176\nTrainable params: 1,174,176\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 4.69\nForward/backward pass size (MB): 435.61\nParams size (MB): 4.48\nEstimated Total Size (MB): 444.78\n----------------------------------------------------------------\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# Normalización de imagen","metadata":{"papermill":{"duration":0.025104,"end_time":"2025-08-27T02:45:54.498685","exception":false,"start_time":"2025-08-27T02:45:54.473581","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ===========================\n# CÁLCULO DE MEDIA Y DESVIACIÓN ESTÁNDAR (por canal) DEL DATASET\n# ===========================\n# Objetivo: estimar las estadísticas de color (mean y std de R, G, B) para\n# usarlas luego en una normalización tipo torchvision.transforms.Normalize(mean, std).\n\ntrain_ds = maskDataset(train_df, root_dir=train_root_dir,output_size=(w,h))#,output_size=(255,255)\n\n# Acumuladores para medias/STD por canal (R,G,B)\nmeans = np.zeros(3)\nstds = np.zeros(3)\nn_images = 0\n\n# Recorremos todas las imágenes del split de entrenamiento\nfor x in train_ds:\n    img = x['image']  # Imagen en formato HxWxC (RGB). \n    n_images += 1\n\n    # Para cada canal (0=R, 1=G, 2=B), calculamos la media y la STD de la imagen actual\n    for channel in range(3):\n        channel_pixels = img[..., channel]  # Todos los píxeles del canal\n        # Se acumula la media y la desviación estándar por imagen (promedio de medias, no ponderado por píxeles)\n        means[channel] += np.mean(channel_pixels)\n        stds[channel] += np.std(channel_pixels)\n\n# Promediamos sobre el número de imágenes para obtener la estimación final por canal\nmeans /= n_images\nstds /= n_images\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:58.213384Z","iopub.execute_input":"2025-09-26T01:51:58.213717Z","iopub.status.idle":"2025-09-26T01:51:59.084186Z","shell.execute_reply.started":"2025-09-26T01:51:58.213695Z","shell.execute_reply":"2025-09-26T01:51:59.083506Z"},"papermill":{"duration":1.009511,"end_time":"2025-08-27T02:45:55.533483","exception":false,"start_time":"2025-08-27T02:45:54.523972","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# INSPECCIÓN DE ESTADÍSTICAS POR CANAL\n# ===========================\n# 'means': medias por canal [R, G, B] calculadas en el bloque anterior.\n# 'stds' : desviaciones estándar por canal [R, G, B].\n# Útil para configurar transforms.Normalize(mean, std).\nprint(means)\nprint(stds)","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:59.084917Z","iopub.execute_input":"2025-09-26T01:51:59.085151Z","iopub.status.idle":"2025-09-26T01:51:59.089814Z","shell.execute_reply.started":"2025-09-26T01:51:59.085133Z","shell.execute_reply":"2025-09-26T01:51:59.089194Z"},"papermill":{"duration":0.031515,"end_time":"2025-08-27T02:45:55.591166","exception":false,"start_time":"2025-08-27T02:45:55.559651","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"[150.87213377 140.8888561  133.6496836 ]\n[62.79959127 61.64436314 59.85598115]\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Transformación de imagenes\n\nSe hace uso de la librería de aumentación de imagenes en https://albumentations.ai/docs/examples/pytorch_classification/\n\n### **HorizontalFlip (p=1)**\n\n-   gira la imagen de izquierda a derecha y ajusta las bounding boxes\n\n\n### 2. **RandomBrightnessContrast (p=0.3)**\n\n-   cambia de manera aleatoria el brillo y el contraste \n\n### 3. **MotionBlur (p=0.4)**\n\n-   aplica un desenfoque que simula movimiento.\n    \n### 4. **HueSaturationValue (p=0.3)**\n\n-   cambia aleatoriamente el tono (color), la saturación (intensidad) y el valor (brillo).\n    \n### 5. **Affine (p=0.2)**\n\n-   aplica escalado (más grande o más pequeño) y pequeños desplazamientos.\n    \n\nSe experimentó con distintas transformaciones de datos con el objetivo de representar situaciones propias del mundo real. Por ejemplo, se aplicó MotionBlur para simular imágenes borrosas, RandomBrightnessContrast para reflejar escenarios con mucha o poca iluminación y transformaciones de escalado para reconocer objetos en diferentes tamaños. Inicialmente se intentó emplear rotaciones, pero al analizar los resultados se identificaron inconsistencias en las bounding boxes, por lo que se descartó esta opción. De esta experiencia se concluyó que no todas las transformaciones contribuyen de manera positiva al rendimiento del modelo; algunas, por el contrario, generan imágenes con demasiado ruido que dificultan el aprendizaje. Por esta razón, se optó por asignar probabilidades bajas (p) a la mayoría de las transformaciones, de manera que el dataset mantuviera un balance adecuado entre diversidad y calidad.","metadata":{"papermill":{"duration":0.024782,"end_time":"2025-08-27T02:45:55.640916","exception":false,"start_time":"2025-08-27T02:45:55.616134","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ===========================\n# TRANSFORMACIONES DE IMAGEN (pipeline estilo \"sample\" dict)\n# Cada transformación recibe un diccionario 'sample' y lo devuelve modificado.\n# Convención de 'sample':\n#   sample = {\n#       'image': np.ndarray o torch.Tensor,\n#       'bbox' : np.ndarray(1,4) opcional (xmin, ymin, xmax, ymax) normalizado o en píxeles según tu flujo,\n#       'class_id': np.ndarray(1,) opcional\n#   }\n# ===========================\n\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        image = sample['image']\n\n        # Cambiamos el orden de ejes:\n        # numpy:  H x W x C  (canal al final)\n        # torch:  C x H x W  (es el que necesitamos Channel first, NCHW)\n        image = image.transpose((2, 0, 1))\n\n        # Convertimos a Tensor de tipo float32.\n        image = torch.from_numpy(image).float()\n\n        # Actualizamos el diccionario 'sample' in-place\n        sample.update({'image': image})\n        return sample\n\n\nclass Normalizer(object):\n    \n    def __init__(self, stds, means):\n        \"\"\"\n        Arguments:\n            stds: array de longitud 3 con la desviación estándar por canal (RGB).\n            means: array de longitud 3 con la media por canal (RGB).\n        \"\"\"\n        self.stds = stds\n        self.means = means\n    \n    def __call__(self, sample):\n        \"\"\"\n        Sample: diccionario que contiene:\n            image: imagen en formato (C, H, W) y tipo float (misma escala que means/stds).\n        Returns:\n            'image' con normalización por canal: (x - mean) / std\n        \"\"\"\n        image = sample['image']\n        \n        for channel in range(3):\n            image[channel] = (image[channel] - means[channel]) / stds[channel]\n\n        sample['image'] = image\n        return sample\n\n\nclass TVTransformWrapper(object):\n    \"\"\"Torch Vision Transform Wrapper\n    \"\"\"\n    def __init__(self, transform: torch.nn.Module):\n        # Recibe un transform de torchvision (que espera torch.Tensor CxHxW)\n        self.transform = transform\n        \n    def __call__(self, sample):\n        # Aplica el transform SOLO sobre 'image'.\n        # Útil para cosas como transforms.Normalize, Resize (si operan en tensor), etc.\n        sample['image'] = self.transform(sample['image'])\n        return sample\n\n\nclass AlbumentationsWrapper(object):\n    \n    def __init__(self, transform):\n        # Recibe un 'Compose' (u otro) de Albumentations, que trabaja con numpy HxWxC.\n        self.transform = transform\n    \n    def __call__(self, sample):\n        # Aplica Albumentations sobre imagen y bboxes.\n        # Requiere que 'sample[\"image\"]' esté en formato numpy HxWxC (no tensor).\n        # Formato y normalización de 'bboxes' dependen de cómo se configuró la\n        # transformación (p. ej., 'bbox_params' en A.Compose: formato 'pascal_voc' o 'yolo').\n        transformed = self.transform(\n            image=sample['image'], \n            bboxes=sample['bbox']\n        )\n\n        # Albumentations devuelve dict; actualizamos 'sample' con los resultados.\n        sample['image'] = transformed['image']\n        sample['bbox'] = np.array(transformed['bboxes'])\n        return sample\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:59.090486Z","iopub.execute_input":"2025-09-26T01:51:59.090685Z","iopub.status.idle":"2025-09-26T01:51:59.110145Z","shell.execute_reply.started":"2025-09-26T01:51:59.090669Z","shell.execute_reply":"2025-09-26T01:51:59.109549Z"},"papermill":{"duration":0.035259,"end_time":"2025-08-27T02:45:55.701355","exception":false,"start_time":"2025-08-27T02:45:55.666096","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# ===========================\n# DATA AUGMENTATION (Albumentations) + Wrapper para el pipeline\n# ===========================\n\ntrain_data_augmentations = A.Compose([\n    A.HorizontalFlip(p=1),    \n    A.RandomBrightnessContrast(\n        brightness_limit=0.2,\n        contrast_limit=0.2,\n        p=0.3\n    ),\n    A.MotionBlur(\n        blur_limit=3,\n        p=0.4\n    ),\n    A.HueSaturationValue(\n        hue_shift_limit=10,\n        sat_shift_limit=15,\n        val_shift_limit=10,\n        p=0.3\n    ),\n    # Transformación de recorte (puede mejorar la robustez del modelo)\n    A.Affine(\n        scale=(0.95, 1.05),     # Simula scale_limit=0.05\n        translate_percent=(0.05, 0.05), # Simula shift_limit=0.05\n        p=0.2\n    )\n    ],\n    bbox_params=A.BboxParams(\n        format='albumentations',   # Formato de bboxes esperado por Albumentations:\n                                   # [x_min, y_min, x_max, y_max] NORMALIZADO en [0,1].\n                                   # (Coherente con nuestro flujo si ya normalizaste las bboxes).\n        label_fields=[],           # No se pasan etiquetas (category_ids) a las transforms.\n    )\n)\n\n# En nuestro pipeline, las transforms operan sobre 'sample' (dict).\n# Usamos un wrapper que aplica Albumentations sobre sample['image'] y sample['bbox'].\ndataaug_transforms = torchvision.transforms.Compose(\n    [\n        AlbumentationsWrapper(train_data_augmentations)  # Aplica flip y actualiza bboxes.\n    ]\n)\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:59.114421Z","iopub.execute_input":"2025-09-26T01:51:59.114612Z","iopub.status.idle":"2025-09-26T01:51:59.137231Z","shell.execute_reply.started":"2025-09-26T01:51:59.114597Z","shell.execute_reply":"2025-09-26T01:51:59.136673Z"},"papermill":{"duration":0.033363,"end_time":"2025-08-27T02:45:55.761623","exception":false,"start_time":"2025-08-27T02:45:55.72826","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import shutil\nimport re  # Usaremos expresiones regulares para extraer números de cualquier nombre de archivo\n\n# ============================================================\n# 1) PREPARAR CARPETA DE SALIDA PARA IMÁGENES FINALES\n#    (BORRAR SI EXISTE Y CREAR DE NUEVO)\n# ============================================================\nif os.path.exists('data_final'):\n    shutil.rmtree('data_final')   # Eliminamos la carpeta anterior para empezar “limpio”\n\nos.mkdir('data_final')            # Carpeta donde guardaremos: imágenes aumentadas + originales\n\n# Dataset base SIN resize (conserva tamaño original).\ntrain_ds_da = maskDataset(train_df, root_dir=train_root_dir) \n\n# ============================================================\n# 2) CÁLCULO ROBUSTO DEL ÚLTIMO ÍNDICE A PARTIR DEL NOMBRE\n#    Funciona con .jpg o .jpeg y con nombres arbitrarios.\n#    Ej.: 'IMG_4921-2...jpg' → extrae '4921' y usa el ÚLTIMO grupo de dígitos.\n# ============================================================\ndef extract_any_int(name: str) -> int:\n    base, _ = os.path.splitext(name)  # separa nombre y extensión\n    nums = re.findall(r'\\d+', base)   # encuentra todos los grupos de dígitos\n    return int(nums[-1]) if nums else -1  # toma el último grupo si existe; si no, -1\n\nlast_index = train_ds_da.df.filename.apply(extract_any_int).max()\nif last_index < 0:    # si ningún archivo tenía dígitos, empezamos desde 0\n    last_index = 0\nindex = int(last_index) + 1   # primer índice nuevo para imágenes sintéticas\n\n# ============================================================\n# 3) GENERAR IMÁGENES AUMENTADAS Y SUS ANOTACIONES\n#    - Recorremos el dataset de train\n#    - Aplicamos data augmentation (p. ej., flip horizontal)\n#    - Guardamos imagen aumentada\n#    - Registramos fila con filename, class_id y bbox\n# ============================================================\nrows = []\nfor j in range(0,1):  # Cantidad de imágenes sintéticas por imagen original (aquí: 1)\n    iterador = iter(train_ds_da)\n    for i in range(len(train_ds_da)):\n        x = next(iterador)                 # sample original: {'image', 'bbox', 'class_id', ...}\n        x_transformed = copy.deepcopy(x)   # copiamos para no modificar el original\n        x_transformed = dataaug_transforms(x_transformed)  # aplicamos augmentations\n\n        # Construimos nombre único para la imagen aumentada\n        filename = f\"image_id_{index}_t{j}.jpeg\"\n\n        # Recuperamos la imagen aumentada (numpy HxWxC, RGB)\n        image = x_transformed['image']  \n\n        # Guardamos en disco (cv2 usa BGR, por eso convertimos de RGB→BGR)\n        cv2.imwrite(\"data_final/\"+filename, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n\n        # Registramos anotación:\n        #   - filename de la nueva imagen\n        #   - class_id (tal como viene en el sample)\n        #   - bbox (xmin, ymin, xmax, ymax)\n        row = [filename, *x_transformed[\"class_id\"], *x_transformed['bbox'].squeeze()]\n        rows.append(row)\n        index += 1\n\n# Construimos DataFrame con las anotaciones de las imágenes aumentadas\naug_df = pd.DataFrame(rows, columns=['filename', 'class_id', 'xmin', 'ymin', 'xmax', 'ymax',])\n\n# ============================================================\n# 4) COPIAR TAMBIÉN LAS IMÁGENES ORIGINALES A 'data_final'\n#    (tendremos en una misma carpeta originales + aumentadas)\n# ============================================================\nsource = train_root_dir\ndestination = 'data_final'\nallfiles = os.listdir(source)\n\nfor f in allfiles:\n    if f in train_df['filename'].values:   # solo las del split de entrenamiento\n        src_path = os.path.join(source, f)\n        dst_path = os.path.join(destination, f)\n        shutil.copy(src_path, dst_path)\n\n# ============================================================\n# 5) UNIR ANOTACIONES: ORIGINALES + AUMENTADAS\n#    Y AGREGAR LA COLUMNA 'class' A PARTIR DE 'class_id'\n# ============================================================\ndataframe_with_dataaugmentation = pd.concat([train_df, aug_df], ignore_index=True)\ndataframe_with_dataaugmentation['class'] = dataframe_with_dataaugmentation['class_id'].replace(id2obj)\n\n# Mostramos el DataFrame final con todas las anotaciones\ndataframe_with_dataaugmentation\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:51:59.138003Z","iopub.execute_input":"2025-09-26T01:51:59.138329Z","iopub.status.idle":"2025-09-26T01:52:01.053577Z","shell.execute_reply.started":"2025-09-26T01:51:59.138306Z","shell.execute_reply":"2025-09-26T01:52:01.052821Z"},"papermill":{"duration":1.387097,"end_time":"2025-08-27T02:45:57.174087","exception":false,"start_time":"2025-08-27T02:45:55.78699","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"                                              filename      xmin      ymin  \\\n0    IMG_4921-2_mp4-124_jpg.rf.60e2e62f7f6c331d5960...  0.000000  0.326562   \n1    IMG_3099_mp4-26_jpg.rf.44828067865615f50965e95...  0.189062  0.250000   \n2    videoplayback-1-_mp4-0_jpg.rf.2b8492685ce5a86f...  0.667188  0.298438   \n3    video_CDC-YOUTUBE_mp4-31_jpg.rf.9dcb8f35940393...  0.428125  0.389062   \n4    Apple-Tests-Face-ID-Feature-While-Wearing-a-Ma...  0.245312  0.278125   \n..                                                 ...       ...       ...   \n323                     image_id_5295680176929_t0.jpeg  0.365625  0.462500   \n324                     image_id_5295680176930_t0.jpeg  0.001563  0.264062   \n325                     image_id_5295680176931_t0.jpeg  0.145312  0.296875   \n326                     image_id_5295680176932_t0.jpeg  0.268750  0.259375   \n327                     image_id_5295680176933_t0.jpeg  0.385938  0.390625   \n\n         xmax      ymax    class  class_id  \n0    0.501563  0.781250  no-mask         0  \n1    0.718750  0.648438  no-mask         0  \n2    0.731250  0.351562  no-mask         0  \n3    0.598437  0.528125     mask         1  \n4    0.662500  0.553125  no-mask         0  \n..        ...       ...      ...       ...  \n323  0.404687  0.485938     mask         1  \n324  0.515625  0.710938  no-mask         0  \n325  0.707812  0.818750     mask         1  \n326  0.500000  0.562500     mask         1  \n327  0.873437  0.690625     mask         1  \n\n[328 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>xmax</th>\n      <th>ymax</th>\n      <th>class</th>\n      <th>class_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>IMG_4921-2_mp4-124_jpg.rf.60e2e62f7f6c331d5960...</td>\n      <td>0.000000</td>\n      <td>0.326562</td>\n      <td>0.501563</td>\n      <td>0.781250</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>IMG_3099_mp4-26_jpg.rf.44828067865615f50965e95...</td>\n      <td>0.189062</td>\n      <td>0.250000</td>\n      <td>0.718750</td>\n      <td>0.648438</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>videoplayback-1-_mp4-0_jpg.rf.2b8492685ce5a86f...</td>\n      <td>0.667188</td>\n      <td>0.298438</td>\n      <td>0.731250</td>\n      <td>0.351562</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>video_CDC-YOUTUBE_mp4-31_jpg.rf.9dcb8f35940393...</td>\n      <td>0.428125</td>\n      <td>0.389062</td>\n      <td>0.598437</td>\n      <td>0.528125</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Apple-Tests-Face-ID-Feature-While-Wearing-a-Ma...</td>\n      <td>0.245312</td>\n      <td>0.278125</td>\n      <td>0.662500</td>\n      <td>0.553125</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>323</th>\n      <td>image_id_5295680176929_t0.jpeg</td>\n      <td>0.365625</td>\n      <td>0.462500</td>\n      <td>0.404687</td>\n      <td>0.485938</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>324</th>\n      <td>image_id_5295680176930_t0.jpeg</td>\n      <td>0.001563</td>\n      <td>0.264062</td>\n      <td>0.515625</td>\n      <td>0.710938</td>\n      <td>no-mask</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>325</th>\n      <td>image_id_5295680176931_t0.jpeg</td>\n      <td>0.145312</td>\n      <td>0.296875</td>\n      <td>0.707812</td>\n      <td>0.818750</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>326</th>\n      <td>image_id_5295680176932_t0.jpeg</td>\n      <td>0.268750</td>\n      <td>0.259375</td>\n      <td>0.500000</td>\n      <td>0.562500</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>327</th>\n      <td>image_id_5295680176933_t0.jpeg</td>\n      <td>0.385938</td>\n      <td>0.390625</td>\n      <td>0.873437</td>\n      <td>0.690625</td>\n      <td>mask</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>328 rows × 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"# ===========================\n# VERIFICACIÓN RÁPIDA DE FORMAS (nº de filas y columnas)\n# ===========================\n# Muestra un par de tuplas:\n#  - Primero: shape de train_df  → (n_filas_train, n_columnas)\n#  - Segundo: shape de dataframe_with_dataaugmentation → (n_filas_total, n_columnas)\ntrain_df.shape, dataframe_with_dataaugmentation.shape\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:52:01.055055Z","iopub.execute_input":"2025-09-26T01:52:01.055279Z","iopub.status.idle":"2025-09-26T01:52:01.060315Z","shell.execute_reply.started":"2025-09-26T01:52:01.055262Z","shell.execute_reply":"2025-09-26T01:52:01.059515Z"},"papermill":{"duration":0.032614,"end_time":"2025-08-27T02:45:57.233265","exception":false,"start_time":"2025-08-27T02:45:57.200651","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"((164, 7), (328, 7))"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"# ===========================\n# PIPELINE DE TRANSFORMACIONES\n# - common_transforms: pasos comunes a train y valid/test\n# - train_data_augmentations: augmentations solo para entrenamiento\n# - train_transforms: augmentations + comunes (en ese orden)\n# - eval_transforms: solo comunes (sin augmentations)\n# ===========================\n\ncommon_transforms = [\n    ToTensor(),               # Convierte imagen de numpy (H,W,C) → torch.Tensor (C,H,W), float32\n    Normalizer(               # Normaliza por canal: (x - mean) / std\n        means=means,          \n        stds=stds,            \n    )\n]\n\ntrain_data_augmentations = A.Compose([\n    A.HorizontalFlip(p=0.5)   # Flip horizontal con prob. 0.5 (afecta imagen y ajusta bboxes)\n    \n    ],\n    bbox_params=A.BboxParams(\n        format='albumentations',  # Formato esperado: [xmin, ymin, xmax, ymax] NORMALIZADO en [0,1]\n        label_fields=[],          \n    )\n)\n\n# En entrenamiento: primero augmentations (operan sobre numpy HxWxC),\n# luego ToTensor() y Normalizer() (operan sobre tensor CxHxW).\ntrain_transforms = torchvision.transforms.Compose(\n    [\n        AlbumentationsWrapper(train_data_augmentations),  # Aplica A.Compose a 'image' y 'bbox'\n    ] + common_transforms\n)\n\n# En validación/evaluación: NO se aplican augmentations, solo los pasos comunes\n# (ToTensor + Normalizer) para mantener consistencia.\neval_transforms = torchvision.transforms.Compose(common_transforms)\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:52:01.061064Z","iopub.execute_input":"2025-09-26T01:52:01.061324Z","iopub.status.idle":"2025-09-26T01:52:01.078573Z","shell.execute_reply.started":"2025-09-26T01:52:01.061307Z","shell.execute_reply":"2025-09-26T01:52:01.078019Z"},"papermill":{"duration":0.033287,"end_time":"2025-08-27T02:45:57.348972","exception":false,"start_time":"2025-08-27T02:45:57.315685","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# ===========================\n# DATASET + DATALOADER (entrenamiento)\n# ===========================\n# Creamos el Dataset a partir del DataFrame que une originales + aumentadas.\n# root_dir='data_final'  → carpeta donde guardamos todas las imágenes (originales y sintéticas).\n# transform=train_transforms → aplica (1) augmentations (AlbumentationsWrapper) y (2) pasos comunes (ToTensor + Normalizer).\n# output_size=(w,h)      → fuerza que todas las imágenes salgan con el mismo tamaño (p. ej., 640x640).\ntrain_ds = maskDataset(dataframe_with_dataaugmentation, root_dir='data_final', transform=train_transforms,output_size=(w,h)) #train_root_dir\n\n# DataLoader: empaqueta el dataset en lotes (batches) para entrenamiento.\n# batch_size=16 → cada iteración entrega 16 muestras (imágenes + labels si labeled=True).\ntrain_data = torch.utils.data.DataLoader(train_ds, batch_size=16)\n\n# Iteramos una sola vez sobre el DataLoader para inspeccionar la forma del tensor de imágenes.\n# Esperado en PyTorch (channel-first): [batch, channels, height, width] → (16, 3, h, w)\nfor x in train_data:\n    print(x['image'].size())  # Deberías ver algo como: torch.Size([16, 3, 640, 640])\n    break                     # 'break' para no consumir todo el DataLoader en esta comprobación\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:52:01.079205Z","iopub.execute_input":"2025-09-26T01:52:01.079369Z","iopub.status.idle":"2025-09-26T01:52:01.191625Z","shell.execute_reply.started":"2025-09-26T01:52:01.079356Z","shell.execute_reply":"2025-09-26T01:52:01.190787Z"},"papermill":{"duration":0.12799,"end_time":"2025-08-27T02:45:57.502654","exception":false,"start_time":"2025-08-27T02:45:57.374664","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([16, 3, 256, 256])\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"Nota: Se verifica que el tensor tenga forma [B,C,H,W]","metadata":{"papermill":{"duration":0.024956,"end_time":"2025-08-27T02:45:57.556089","exception":false,"start_time":"2025-08-27T02:45:57.531133","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Arquitectura CNN","metadata":{"papermill":{"duration":0.024802,"end_time":"2025-08-27T02:45:57.605816","exception":false,"start_time":"2025-08-27T02:45:57.581014","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### **Cabeza de Clasificación**\n\n1.  **Estructura progresiva (1024 → 512 → 256 → 4):**\n    \n    -   Permite ir reduciendo gradualmente la dimensionalidad del vector de características.\n        \n    -   Esto ayuda a extraer representaciones más compactas y discriminativas antes de llegar a los _logits_.\n        \n2.  **Uso de `BatchNorm1d`:**\n    \n    -   Normaliza las activaciones, acelera el entrenamiento y mejora la estabilidad numérica.\n        \n    -   Reduce la sensibilidad a la inicialización de pesos.\n        \n3.  **Activación `SiLU`:**\n    \n    -   Elegida en lugar de ReLU porque es más suave.\n        \n    -   Estudios recientes muestran mejor rendimiento en clasificación gracias a su comportamiento no lineal más rico.\n        \n4.  **Dropout en capas intermedias:**\n    \n    -   Se emplea para mitigar el sobreajuste, especialmente importante en tareas de clasificación donde el modelo puede memorizar patrones.\n        \n    -   Los valores (0.5 y 0.3) se seleccionan para equilibrar regularización sin perder demasiada capacidad de representación.\n\n\n\nLas cabezas se diseñaron pensando en las necesidades específicas de cada tarea:\n\n-   La **clasificación** necesita un modelo que generalice bien (más regularización, activaciones suaves, estructura tipo embudo).\n    \n-   La **regresión** requiere precisión numérica y estabilidad (más capas, activaciones simples, sin ruido extra).","metadata":{}},{"cell_type":"code","source":"def get_output_shape(model: nn.Module, image_dim: ty.Tuple[int, int, int]):\n    # ===========================================================\n    # UTILIDAD: INFERIR LA FORMA DE SALIDA DEL BACKBONE\n    # -----------------------------------------------------------\n    #     # - Crea un tensor aleatorio con esa forma, lo pasa por el modelo\n    #     y devuelve la 'shape' resultante (solo para inspección).\n    # ===========================================================\n    return model(torch.rand(*(image_dim)).to(device)).data.shape\n\n\nclass Model(nn.Module):\n    def __init__(self, input_shape: ty.Tuple[int, int, int] = (3, 256, 256), n_classes: int = 2):\n        \n        super().__init__()\n        \n        self.input_shape = input_shape\n        \n        self.backbone = own_model\n\n        # Inferimos cuántas características (F) salen del backbone para este input.\n        # Se usa un batch sintético de 1 imagen: [1, C, H, W].\n        backbone_output_shape = get_output_shape(self.backbone, [1, *input_shape])\n        # Aplastamos todas las dimensiones de salida para obtener F (nº de features).\n        backbone_output_features = reduce(lambda x, y: x*y, backbone_output_shape)\n        \n        # ---------------------------\n        # CABEZA DE CLASIFICACIÓN \n        # ---------------------------\n        # Toma el vector de features (F) y produce 'logits' de tamaño n_classes.\n        self.cls_head = nn.Sequential(\n            nn.Linear(backbone_output_features, 1024),\n            nn.BatchNorm1d(1024),\n            nn.SiLU(),\n            nn.Dropout(0.5),\n\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.SiLU(),\n            nn.Dropout(0.3),\n\n            nn.Linear(512, 256),\n            nn.SiLU(),\n\n            nn.Linear(256, n_classes)  # logits\n        )\n\n        # ---------------------------\n        # CABEZA DE REGRESIÓN (BBOX) \n        # ---------------------------\n        # Predice 4 valores: [xmin, ymin, xmax, ymax] en la MISMA escala que tus etiquetas.\n        self.reg_head = nn.Sequential(\n            nn.Linear(backbone_output_features, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n\n            nn.Linear(512, 256),\n            nn.ReLU(),\n\n            nn.Linear(256, 128),\n            nn.ReLU(),\n\n            nn.Linear(128, 4)\n        )\n\n    def forward(self, x: Tensor) -> ty.Dict[str, Tensor]:\n        # ===========================================================\n        # FLUJO HACIA ADELANTE\n        # x: tensor de imágenes [B, 3, 640, 640]\n        # 1) Extraemos features con el backbone → [B, F]\n        # 2) cls_head(features) → logits de clase [B, n_classes]\n        # 3) reg_head(features) → bbox [B, 4]\n        # 4) Devolvemos un diccionario con ambas salidas\n        # ===========================================================\n        features = self.backbone(x)\n        cls_logits = self.cls_head(features)\n        pred_bbox = self.reg_head(features)\n        predictions = {'bbox': pred_bbox, 'class_id': cls_logits}\n        return predictions\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:54:56.963122Z","iopub.execute_input":"2025-09-26T01:54:56.963457Z","iopub.status.idle":"2025-09-26T01:54:56.973870Z","shell.execute_reply.started":"2025-09-26T01:54:56.963433Z","shell.execute_reply":"2025-09-26T01:54:56.972896Z"},"papermill":{"duration":0.037347,"end_time":"2025-08-27T02:45:57.78238","exception":false,"start_time":"2025-08-27T02:45:57.745033","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# Libera la memoria **en caché** de CUDA que PyTorch reservó pero no está usando.\n# No borra tensores activos ni reduce memoria de objetos vivos.\n# Útil tras `del` de tensores grandes para evitar OOM, pero abusar puede bajar rendimiento.\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:54:56.975585Z","iopub.execute_input":"2025-09-26T01:54:56.976327Z","iopub.status.idle":"2025-09-26T01:54:57.025349Z","shell.execute_reply.started":"2025-09-26T01:54:56.976296Z","shell.execute_reply":"2025-09-26T01:54:57.024815Z"},"papermill":{"duration":0.066715,"end_time":"2025-08-27T02:45:57.874839","exception":false,"start_time":"2025-08-27T02:45:57.808124","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# Imprime el tamaño del tensor de imagen que viene en x['image'].\n# Si proviene de un DataLoader, lo esperado es [B, C, H, W] (p. ej., torch.Size([16, 3, 640, 640])).\n# Si proviene de un __getitem__ directo del Dataset, típicamente será [C, H, W] (sin batch),\n# en cuyo caso habría que añadir una dimensión de batch antes de pasar al modelo (no se hace aquí).\nprint('image', x['image'].size())\n\n# Instancia el modelo con el tamaño de entrada (3, h, w) y 2 clases.\nmodel = Model(input_shape=(3, h, w), n_classes=2).to(device)\n\n# Mueve el tensor de imágenes al mismo device que el modelo (cuda o cpu).\nx['image'] = x['image'].to(device)\n\n# Forward: pasa el batch de imágenes por el modelo.\n# Salida esperada (diccionario):\n#   - preds['bbox']: tensor [B, 4] con las coordenadas predichas (en la misma escala que las etiquetas).\n#   - preds['class_id']: tensor [B, n_classes] con logits de clasificación.\npreds = model(x['image'])\n\n# Muestra el diccionario de predicciones.\n# Para interpretar:\n#   • Clase predicha: preds['class_id'].argmax(dim=1)\n#   • Probabilidades: torch.softmax(preds['class_id'], dim=1)\npreds\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:54:57.025974Z","iopub.execute_input":"2025-09-26T01:54:57.026241Z","iopub.status.idle":"2025-09-26T01:54:57.079477Z","shell.execute_reply.started":"2025-09-26T01:54:57.026219Z","shell.execute_reply":"2025-09-26T01:54:57.078922Z"},"papermill":{"duration":0.698092,"end_time":"2025-08-27T02:45:58.658758","exception":false,"start_time":"2025-08-27T02:45:57.960666","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"image torch.Size([16, 3, 256, 256])\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"{'bbox': tensor([[ 0.1803, -0.0689,  0.0568,  0.0269],\n         [ 0.1890, -0.0161,  0.0102,  0.0398],\n         [ 0.1847, -0.0916,  0.0192,  0.0998],\n         [ 0.0658, -0.0121, -0.0340, -0.0298],\n         [ 0.1788, -0.0694,  0.0030, -0.0580],\n         [ 0.2096, -0.1078, -0.0127, -0.0446],\n         [ 0.1478, -0.0907, -0.0168,  0.0044],\n         [ 0.1766, -0.1459,  0.0222, -0.0016],\n         [ 0.1353, -0.1051,  0.0247, -0.0068],\n         [ 0.1732, -0.0222,  0.0204,  0.0279],\n         [ 0.1061, -0.0538,  0.0708,  0.0736],\n         [ 0.1947, -0.0532,  0.0709,  0.0652],\n         [ 0.2303, -0.0328, -0.0392,  0.0315],\n         [ 0.1480, -0.0785, -0.0051,  0.0283],\n         [ 0.1631, -0.0236, -0.0244,  0.0241],\n         [ 0.1017, -0.0013, -0.0078,  0.0578]], device='cuda:0',\n        grad_fn=<AddmmBackward0>),\n 'class_id': tensor([[ 0.0441, -0.1146],\n         [-0.0224, -0.1785],\n         [ 0.0377, -0.1591],\n         [-0.0986, -0.0219],\n         [-0.1553, -0.3952],\n         [-0.1072,  0.0446],\n         [ 0.1743, -0.0297],\n         [-0.1220, -0.1620],\n         [ 0.0906, -0.0727],\n         [-0.0503,  0.0575],\n         [-0.0561, -0.2103],\n         [ 0.0272, -0.1626],\n         [ 0.0082,  0.0149],\n         [-0.0052, -0.1019],\n         [-0.0152, -0.0353],\n         [ 0.1330, -0.0615]], device='cuda:0', grad_fn=<AddmmBackward0>)}"},"metadata":{}}],"execution_count":58},{"cell_type":"markdown","source":"# Métricas","metadata":{"papermill":{"duration":0.02552,"end_time":"2025-08-27T02:45:58.712795","exception":false,"start_time":"2025-08-27T02:45:58.687275","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def iou(y_true: Tensor, y_pred: Tensor):\n    \"\"\"\n    Calcula el IoU (Intersection over Union) promedio entre cajas verdaderas y predichas.\n\n    Supuestos importantes (para este curso):\n    - Formato de cajas: [xmin, ymin, xmax, ymax] (xyxy), en la MISMA escala para y_true y y_pred\n      (puede ser normalizada [0,1] o en píxeles; lo clave es que coincidan).\n    \"\"\"\n    # Matriz de IoU por pares (GT vs Pred)\n    pairwise_iou = torchvision.ops.box_iou(y_true.squeeze(), y_pred.squeeze())\n    # Promedio de la diagonal (asumiendo correspondencia 1:1 y mismo orden)\n    result = torch.trace(pairwise_iou) / pairwise_iou.size()[0]\n    return result\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:54:57.080257Z","iopub.execute_input":"2025-09-26T01:54:57.080519Z","iopub.status.idle":"2025-09-26T01:54:57.084971Z","shell.execute_reply.started":"2025-09-26T01:54:57.080490Z","shell.execute_reply":"2025-09-26T01:54:57.084144Z"},"papermill":{"duration":0.044668,"end_time":"2025-08-27T02:45:58.789517","exception":false,"start_time":"2025-08-27T02:45:58.744849","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":59},{"cell_type":"code","source":"def accuracy(y_true: Tensor, y_pred: Tensor) -> Tensor:\n    \"\"\"\n    Accuracy para **clasificación binaria con 2 logits** (CrossEntropyLoss).\n\n    Supuestos:\n    - y_pred: logits de forma [B, 2] (clases: 0 = no-mask, 1 = mask).\n    - y_true: etiquetas enteras {0,1} con forma [B] o [B,1].\n\n    Devuelve:\n    - Tensor escalar en [0,1] con la proporción de aciertos del batch.\n    \"\"\"\n    # Alinear forma y tipo de y_true\n    if y_true.dim() > 1:\n        y_true = y_true.squeeze(-1)     # [B,1] -> [B]\n    y_true = y_true.long()              # CrossEntropy espera enteros\n\n    # Clase predicha: índice del logit mayor\n    pred = torch.argmax(y_pred, dim=-1) # [B,2] -> [B]\n\n    # Alinear shapes si hace falta\n    if pred.shape != y_true.shape:\n        y_true = y_true.view_as(pred)\n\n    # Accuracy = (# aciertos) / (# ejemplos)\n    return (pred == y_true).float().mean()\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:54:57.086962Z","iopub.execute_input":"2025-09-26T01:54:57.087216Z","iopub.status.idle":"2025-09-26T01:54:57.100681Z","shell.execute_reply.started":"2025-09-26T01:54:57.087201Z","shell.execute_reply":"2025-09-26T01:54:57.100042Z"},"papermill":{"duration":0.032557,"end_time":"2025-08-27T02:45:58.853592","exception":false,"start_time":"2025-08-27T02:45:58.821035","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":"# Loss function","metadata":{"papermill":{"duration":0.025186,"end_time":"2025-08-27T02:45:58.904593","exception":false,"start_time":"2025-08-27T02:45:58.879407","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Pérdida multi-tarea:\n#  - Clasificación: CrossEntropy sobre y_preds['class_id'] (logits [B,C]) vs y_true['class_id'] (enteros [B]).\n#  - Regresión: MSE sobre y_preds['bbox'] y y_true['bbox'] (ambos [B,4] y en la MISMA escala).\n#  - Total: (1-α)*cls_loss + α*reg_loss. α=0.5 equilibra ambas.\n#\n# Devuelve dict con total, reg_loss y cls_loss (útil para monitoreo).\ndef loss_fn(y_true, y_preds, alpha=0.7):\n    # --- CLASIFICACIÓN ---\n    logits = y_preds['class_id']            # [N, K] si K>1, o [N, 1] si binario\n    N, K = logits.shape[0], logits.shape[1]\n\n    if K > 1:\n        # MULTICLASE (una sola etiqueta por ejemplo)\n        # targets: long en [0..K-1], shape [N]\n        y = y_true['class_id']\n        if y.dim() == 2 and y.size(1) == 1:   # [N,1] -> [N]\n            y = y.squeeze(1)\n        if y.dim() == 2 and y.size(1) == K:   # one-hot -> índices\n            y = y.argmax(1)\n        y = y.long()\n\n        class_weights = torch.tensor([0.35, 0.65], dtype=torch.float32).to(logits.device)\n        cls_loss = F.cross_entropy(logits, y, weight=class_weights)\n\n    else:\n        # BINARIA\n        # Usa BCEWithLogitsLoss (no apliques sigmoid en la cabeza)\n        y = y_true['class_id']\n        # Asegura float y mismo shape que logits\n        if y.dim() == 1:                      # [N] -> [N,1]\n            y = y.unsqueeze(1)\n        y = y.float()\n        if y.shape != logits.shape:\n            y = y.view_as(logits)\n        bce = torch.nn.BCEWithLogitsLoss()\n        cls_loss = bce(logits, y)\n\n    # --- REGRESIÓN (bbox) ---\n    reg_pred = y_preds['bbox'].float()\n    reg_true = y_true['bbox'].float()\n    if reg_pred.shape != reg_true.shape:\n        reg_true = reg_true.view_as(reg_pred)\n    reg_loss = F.mse_loss(reg_pred, reg_true)\n\n    total = (1 - alpha) * cls_loss + alpha * reg_loss\n    return {'loss': total, 'cls_loss': cls_loss, 'reg_loss': reg_loss}\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:54:57.101542Z","iopub.execute_input":"2025-09-26T01:54:57.101892Z","iopub.status.idle":"2025-09-26T01:54:57.123826Z","shell.execute_reply.started":"2025-09-26T01:54:57.101871Z","shell.execute_reply":"2025-09-26T01:54:57.123234Z"},"papermill":{"duration":0.03462,"end_time":"2025-08-27T02:45:58.965075","exception":false,"start_time":"2025-08-27T02:45:58.930455","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"# Callbacks","metadata":{"papermill":{"duration":0.025922,"end_time":"2025-08-27T02:45:59.077779","exception":false,"start_time":"2025-08-27T02:45:59.051857","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def printer(logs: ty.Dict[str, ty.Any]):\n    \"\"\"\n    Callback de logging:\n    - Recibe un diccionario 'logs' (p. ej., {'iters': i, 'loss': ..., 'acc': ...}).\n    - Imprime cada 10 iteraciones (controlado por logs['iters']).\n    - Redondea valores numéricos a 4 decimales (incluye tensores).\n    Útil para monitorear entrenamiento sin saturar la salida.\n    \"\"\"\n    # imprimir cada 10 pasos\n    if logs['iters'] % 10 != 0:\n        return\n\n    print('Iteration #: ', logs['iters'])\n    for name, value in logs.items():\n        if name == 'iters':\n            continue\n\n        if type(value) in [float, int]:\n            value = round(value, 4)\n        elif type(value) is torch.Tensor:\n            value = torch.round(value, decimals=4)\n\n        print(f'\\t{name} = {value}')\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:54:57.124404Z","iopub.execute_input":"2025-09-26T01:54:57.124581Z","iopub.status.idle":"2025-09-26T01:54:57.143480Z","shell.execute_reply.started":"2025-09-26T01:54:57.124567Z","shell.execute_reply":"2025-09-26T01:54:57.142877Z"},"papermill":{"duration":0.032724,"end_time":"2025-08-27T02:45:59.136122","exception":false,"start_time":"2025-08-27T02:45:59.103398","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":"# Bucle de entrenamiento/ training loop","metadata":{"papermill":{"duration":0.025944,"end_time":"2025-08-27T02:45:59.188109","exception":false,"start_time":"2025-08-27T02:45:59.162165","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def evaluate(\n    logs: ty.Dict[str, ty.Any], \n    labels: ty.Dict[str, Tensor],\n    preds: ty.Dict[str, Tensor],\n    eval_set: str,\n    metrics: ty.Dict[str, ty.Callable[[Tensor, Tensor], Tensor]],\n    losses: ty.Optional[ty.Dict[str, Tensor]] = None,\n) -> ty.Dict[str, ty.Any]:\n    \"\"\"\n    Callback de evaluación (uso dentro del training loop).\n    - Agrega al diccionario 'logs' las pérdidas y métricas calculadas.\n    - Prefija cada clave con el nombre del split: p.ej., 'train_loss', 'val_acc', 'val_iou'.\n\n    Parámetros:\n      logs     : dict acumulador (se modifica in-place).\n      labels   : dict con etiquetas por tarea, p.ej. {'class_id': y_cls, 'bbox': y_box}.\n      preds    : dict con predicciones por tarea, p.ej. {'class_id': logits, 'bbox': boxes}.\n      eval_set : nombre del split ('train', 'val', 'test', ...).\n      metrics  : dict de listas de métricas por tarea. Formato esperado:\n                 {\n                   'class_id': [('accuracy', acc_fn), ...],\n                   'bbox'    : [('iou', iou_fn), ...]\n                 }\n      losses   : (opcional) dict salido de loss_fn con {'loss','cls_loss','reg_loss'}.\n\n    Retorna:\n      logs con claves añadidas: {f'{eval_set}_{loss_name}': valor, f'{eval_set}_{metric_name}': valor}\n    \"\"\"\n    if losses is not None:\n        for loss_name, loss_value in losses.items():\n            logs[f'{eval_set}_{loss_name}'] = loss_value\n    \n    for task_name, label in labels.items():\n        for metric_name, metric in metrics[task_name]:\n            value = metric(label, preds[task_name])\n            logs[f'{eval_set}_{metric_name}'] = value\n            \n    return logs\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:54:57.144305Z","iopub.execute_input":"2025-09-26T01:54:57.144528Z","iopub.status.idle":"2025-09-26T01:54:57.165234Z","shell.execute_reply.started":"2025-09-26T01:54:57.144513Z","shell.execute_reply":"2025-09-26T01:54:57.164412Z"},"papermill":{"duration":0.033683,"end_time":"2025-08-27T02:45:59.247369","exception":false,"start_time":"2025-08-27T02:45:59.213686","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":63},{"cell_type":"code","source":"def step(\n    model: Model, \n    optimizer: Optimizer, \n    batch: maskDataset,\n    loss_fn: ty.Callable[[ty.Dict[str, torch.Tensor]], torch.Tensor],\n    device: str,\n    train: bool = False,\n) -> ty.Tuple[ty.Dict[str, Tensor], ty.Dict[str, Tensor]]:\n    \"\"\"\n    Un paso (train o eval):\n      1) Mueve batch a 'device' (imagen y etiquetas).\n      2) Hace forward → preds.\n      3) Calcula pérdidas con loss_fn.\n      4) Si train=True: backward + optimizer.step().\n\n    Notas para el problema binario + regresión:\n      - Clasificación binaria: la cabeza debe devolver logits [B, 2] y la loss usar CrossEntropy.\n      - BBoxes: tensores [B,4] en la MISMA escala que labels (normalizada [0,1] en este curso).\n\n    Retorna:\n      losses: dict {'loss','cls_loss','reg_loss'}\n      preds : dict {'class_id': logits, 'bbox': boxes}\n    \"\"\"\n    if train:\n        optimizer.zero_grad()\n    \n    # Extrae imagen y la sube a device (batch.pop muta el dict; ya no contiene 'image')\n    img = batch.pop('image').to(device)\n    \n    # Sube etiquetas restantes a device\n    for k in list(batch.keys()):\n        batch[k] = batch[k].to(device)\n    \n    # Forward + pérdidas\n    preds = model(img.float())\n    losses = loss_fn(batch, preds)\n    final_loss = losses['loss']\n    \n    if train:\n        final_loss.backward()\n        optimizer.step()\n    \n    return losses, preds\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:54:57.165881Z","iopub.execute_input":"2025-09-26T01:54:57.166121Z","iopub.status.idle":"2025-09-26T01:54:57.185084Z","shell.execute_reply.started":"2025-09-26T01:54:57.166095Z","shell.execute_reply":"2025-09-26T01:54:57.184531Z"},"papermill":{"duration":0.033829,"end_time":"2025-08-27T02:45:59.306869","exception":false,"start_time":"2025-08-27T02:45:59.27304","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":64},{"cell_type":"code","source":"def train(\n    model: Model, \n    optimizer: Optimizer, \n    dataset: DataLoader,\n    eval_datasets: ty.List[ty.Tuple[str, DataLoader]],\n    loss_fn: ty.Callable[[ty.Dict[str, torch.Tensor]], torch.Tensor],\n    metrics: ty.Dict[str, ty.Callable[[Tensor, Tensor], Tensor]],\n    callbacks: ty.List[ty.Callable[[ty.Dict[ty.Any, ty.Any]], None]],\n    device: str,\n    train_steps: 100,\n    eval_steps: 10,\n) -> Model:\n    \"\"\"\n    Training loop genérico (clasificación binaria + regresión de bbox).\n\n    Flujo:\n      - Itera hasta 'train_steps'.\n      - Cada iteración:\n         * Toma un batch (recicla el iterador al agotarse).\n         * Llama a step(..., train=True): forward, loss, backward, step.\n         * Llama a evaluate(...) para registrar 'train_*' en logs (accuracy, IoU, pérdidas).\n      - Cada 'eval_steps' iteraciones:\n         * model.eval() y torch.no_grad() para evaluar en datasets de validación/prueba.\n         * Para cada (nombre, loader) en eval_datasets, evalúa y registra 'val_*'/'test_*'.\n      - Ejecuta 'callbacks(logs)' (p.ej., printer) en cada iteración.\n\n    Requisitos/Convenciones:\n      - loss_fn debe devolver dict con llaves {'loss','cls_loss','reg_loss'}.\n      - metrics debe tener la estructura por tarea:\n          {'class_id': [('accuracy', acc_fn)], 'bbox': [('iou', iou_fn)]}\n      - Para binario: logits [B,2] y etiquetas {0,1} (long). BBoxes [B,4] (normalizadas [0,1]).\n\n    Retorna:\n      - El modelo (con pesos actualizados).\n    \"\"\"\n    model = model.to(device)\n    iters = 0\n    iterator = iter(dataset)\n    assert train_steps > eval_steps, 'Train steps should be greater than the eval steps'\n    \n    while iters <= train_steps:\n        logs = dict()\n        logs['iters'] = iters\n        try:\n            batch = next(iterator)\n        except StopIteration:\n            iterator = iter(dataset)\n            batch = next(iterator)\n\n        # Paso de entrenamiento\n        losses, preds = step(model, optimizer, batch, loss_fn, device, train=True)\n        logs = evaluate(logs, batch, preds, 'train', metrics, losses)\n        \n        # Evaluación periódica\n        if iters % eval_steps == 0:        \n            model.eval()  # desactiva Dropout/BatchNorm para eval\n            \n            with torch.no_grad():\n                for name, dataset in eval_datasets:\n                    for batch in dataset:\n                        losses, preds = step(model, optimizer, batch, loss_fn, device, train=False)\n                        logs = evaluate(logs, batch, preds, name, metrics, losses)\n\n            model.train()\n\n        # Callbacks (p.ej., imprimir logs cada N iters)\n        for callback in callbacks:\n            callback(logs)\n        \n        iters += 1\n    \n    return model\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:54:57.185753Z","iopub.execute_input":"2025-09-26T01:54:57.185957Z","iopub.status.idle":"2025-09-26T01:54:57.206017Z","shell.execute_reply.started":"2025-09-26T01:54:57.185936Z","shell.execute_reply":"2025-09-26T01:54:57.205346Z"},"papermill":{"duration":0.036761,"end_time":"2025-08-27T02:45:59.370318","exception":false,"start_time":"2025-08-27T02:45:59.333557","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":"# Run","metadata":{"papermill":{"duration":0.026596,"end_time":"2025-08-27T02:45:59.424068","exception":false,"start_time":"2025-08-27T02:45:59.397472","status":"completed"},"tags":[]}},{"cell_type":"code","source":"os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:54:57.206793Z","iopub.execute_input":"2025-09-26T01:54:57.207070Z","iopub.status.idle":"2025-09-26T01:54:57.224549Z","shell.execute_reply.started":"2025-09-26T01:54:57.207043Z","shell.execute_reply":"2025-09-26T01:54:57.223972Z"},"papermill":{"duration":0.032055,"end_time":"2025-08-27T02:45:59.48279","exception":false,"start_time":"2025-08-27T02:45:59.450735","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":"- El batch size se definió como resultado de diferentes experimentos y comparación de resultados, con valores inferiores o superiores a 32 teniamos problemas de Memoria desbordada o las metricas se desmejoraban","metadata":{}},{"cell_type":"code","source":"# ===========================\n# RUN: configuración rápida para lanzar entrenamiento/validación\n# ===========================\n\n# Hparams: hiperparámetros básicos del run\nbatch_size = 32\n\n# Data: datasets y transforms\n# - train: augmentations + ToTensor + Normalize, sobre imágenes (w,h)\n# - val  : solo ToTensor + Normalize (sin augmentations)\ntrain_ds = maskDataset(\n    dataframe_with_dataaugmentation, root_dir='data_final',\n    transform=train_transforms, output_size=(w,h)\n)  \nval_ds = maskDataset(\n    val_df, root_dir=train_root_dir,\n    transform=eval_transforms, output_size=(w,h)\n)  \n\n# DataLoaders: batching y paralelismo\n# - shuffle solo en train\n# - num_workers = cpu_count() para acelerar lectura/transform\ntrain_data = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=cpu_count())\nval_data   = DataLoader(val_ds,   batch_size=batch_size,               num_workers=cpu_count())\n\nmodel = Model().to(device)\nsummary(model, model.input_shape)\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:54:57.225458Z","iopub.execute_input":"2025-09-26T01:54:57.225761Z","iopub.status.idle":"2025-09-26T01:54:57.273769Z","shell.execute_reply.started":"2025-09-26T01:54:57.225740Z","shell.execute_reply":"2025-09-26T01:54:57.273233Z"},"papermill":{"duration":0.442778,"end_time":"2025-08-27T02:45:59.95249","exception":false,"start_time":"2025-08-27T02:45:59.509712","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 32, 256, 256]             896\n       BatchNorm2d-2         [-1, 32, 256, 256]              64\n              ReLU-3         [-1, 32, 256, 256]               0\n            Conv2d-4         [-1, 32, 128, 128]           9,248\n       BatchNorm2d-5         [-1, 32, 128, 128]              64\n              ReLU-6         [-1, 32, 128, 128]               0\n         MaxPool2d-7           [-1, 32, 64, 64]               0\n            Conv2d-8           [-1, 64, 64, 64]          18,496\n       BatchNorm2d-9           [-1, 64, 64, 64]             128\n             ReLU-10           [-1, 64, 64, 64]               0\n           Conv2d-11           [-1, 64, 32, 32]          36,928\n      BatchNorm2d-12           [-1, 64, 32, 32]             128\n             ReLU-13           [-1, 64, 32, 32]               0\n        MaxPool2d-14           [-1, 64, 16, 16]               0\n           Conv2d-15          [-1, 128, 16, 16]          73,856\n      BatchNorm2d-16          [-1, 128, 16, 16]             256\n             ReLU-17          [-1, 128, 16, 16]               0\n           Conv2d-18            [-1, 128, 8, 8]         147,584\n      BatchNorm2d-19            [-1, 128, 8, 8]             256\n             ReLU-20            [-1, 128, 8, 8]               0\n        MaxPool2d-21            [-1, 128, 4, 4]               0\n           Conv2d-22            [-1, 256, 4, 4]         295,168\n      BatchNorm2d-23            [-1, 256, 4, 4]             512\n             ReLU-24            [-1, 256, 4, 4]               0\n           Conv2d-25            [-1, 256, 2, 2]         590,080\n      BatchNorm2d-26            [-1, 256, 2, 2]             512\n             ReLU-27            [-1, 256, 2, 2]               0\n        MaxPool2d-28            [-1, 256, 1, 1]               0\nAdaptiveAvgPool2d-29            [-1, 256, 1, 1]               0\n          Flatten-30                  [-1, 256]               0\n          Dropout-31                  [-1, 256]               0\nMyBackboneFromCNN-32                  [-1, 256]               0\n           Linear-33                 [-1, 1024]         263,168\n      BatchNorm1d-34                 [-1, 1024]           2,048\n             SiLU-35                 [-1, 1024]               0\n          Dropout-36                 [-1, 1024]               0\n           Linear-37                  [-1, 512]         524,800\n      BatchNorm1d-38                  [-1, 512]           1,024\n             SiLU-39                  [-1, 512]               0\n          Dropout-40                  [-1, 512]               0\n           Linear-41                  [-1, 256]         131,328\n             SiLU-42                  [-1, 256]               0\n           Linear-43                    [-1, 2]             514\n           Linear-44                 [-1, 1024]         263,168\n      BatchNorm1d-45                 [-1, 1024]           2,048\n             ReLU-46                 [-1, 1024]               0\n          Dropout-47                 [-1, 1024]               0\n           Linear-48                  [-1, 512]         524,800\n      BatchNorm1d-49                  [-1, 512]           1,024\n             ReLU-50                  [-1, 512]               0\n           Linear-51                  [-1, 256]         131,328\n             ReLU-52                  [-1, 256]               0\n           Linear-53                  [-1, 128]          32,896\n             ReLU-54                  [-1, 128]               0\n           Linear-55                    [-1, 4]             516\n================================================================\nTotal params: 3,052,838\nTrainable params: 3,052,838\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.75\nForward/backward pass size (MB): 69.80\nParams size (MB): 11.65\nEstimated Total Size (MB): 82.20\n----------------------------------------------------------------\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"- Se separaron los learning rates de backbone, clasificación y regresión. como ya no tenemos un modelo preentrenado usamos un learning rate similar para todas para que apredna al mismo ritmo. El learning rate de la cabeza de clasificación y backbone se mantuvo más bajo que el de regresión, permitiendo equilibrar el ritmo de aprendizaje entre ambas tareas y evitar que la clasificación domine el entrenamiento antes de que la regresión converja, se obtuvieron mejores resultados en clasificacion con LR de 1e-3 que con 1e-4.","metadata":{}},{"cell_type":"code","source":"# ===========================\n# OPTIMIZER + LANZAR ENTRENAMIENTO\n# ===========================\n\n# Optimizer: AdamW con learning rate 'lr' independiente.\noptimizer = torch.optim.AdamW([\n    {\"params\": model.backbone.parameters(), \"lr\": 1e-4},\n    {\"params\": model.cls_head.parameters(), \"lr\": 1e-4},\n    {\"params\": model.reg_head.parameters(), \"lr\": 1e-3}\n], weight_decay=1e-4)\n\n# Loop de entrenamiento:\n# - 'train_data' como DataLoader de entrenamiento\n# - 'eval_datasets': lista de pares (nombre_split, DataLoader) para evaluación periódica\n# - 'loss_fn': pérdida multi-tarea (cls + bbox)\n# - 'metrics':\n#     • bbox  → IoU\n#     • class → accuracy (binaria con 2 logits en este proyecto)\n# - 'callbacks': funciones de logging/monitoreo (p. ej., printer)\n# - 'train_steps' y 'eval_steps': frecuencia de entrenamiento y evaluación\nmodel = train(\n    model,\n    optimizer,\n    train_data,\n    eval_datasets=[('val', val_data)],\n    loss_fn=loss_fn,\n    metrics={\n        'bbox': [('iou', iou)],\n        'class_id': [('accuracy', accuracy)]\n    },\n    callbacks=[printer],\n    device=device,\n    train_steps=300,\n    eval_steps=10\n)\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:54:57.274479Z","iopub.execute_input":"2025-09-26T01:54:57.274685Z","iopub.status.idle":"2025-09-26T01:56:15.312806Z","shell.execute_reply.started":"2025-09-26T01:54:57.274669Z","shell.execute_reply":"2025-09-26T01:56:15.311985Z"},"papermill":{"duration":24.572911,"end_time":"2025-08-27T02:46:24.552217","exception":false,"start_time":"2025-08-27T02:45:59.979306","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Iteration #:  0\n\ttrain_loss = 0.43709999322891235\n\ttrain_cls_loss = 0.7218999862670898\n\ttrain_reg_loss = 0.3149999976158142\n\ttrain_iou = 0.0\n\ttrain_accuracy = 0.4375\n\tval_loss = 0.38999998569488525\n\tval_cls_loss = 0.6881999969482422\n\tval_reg_loss = 0.2623000144958496\n\tval_iou = 0.0\n\tval_accuracy = 0.34779998660087585\n\nIteration #:  10\n\ttrain_loss = 0.2223999947309494\n\ttrain_cls_loss = 0.6690999865531921\n\ttrain_reg_loss = 0.030899999663233757\n\ttrain_iou = 0.207\n\ttrain_accuracy = 0.5\n\tval_loss = 0.3206999897956848\n\tval_cls_loss = 0.605400025844574\n\tval_reg_loss = 0.19869999587535858\n\tval_iou = 0.0\n\tval_accuracy = 0.7390999794006348\n\nIteration #:  20\n\ttrain_loss = 0.14069999754428864\n\ttrain_cls_loss = 0.4246000051498413\n\ttrain_reg_loss = 0.01899999938905239\n\ttrain_iou = 0.2712\n\ttrain_accuracy = 0.9129999876022339\n\tval_loss = 0.186599999666214\n\tval_cls_loss = 0.43070000410079956\n\tval_reg_loss = 0.0820000022649765\n\tval_iou = 0.0114\n\tval_accuracy = 1.0\n\nIteration #:  30\n\ttrain_loss = 0.08060000091791153\n\ttrain_cls_loss = 0.24240000545978546\n\ttrain_reg_loss = 0.01119999960064888\n\ttrain_iou = 0.3219\n\ttrain_accuracy = 1.0\n\tval_loss = 0.08720000088214874\n\tval_cls_loss = 0.22419999539852142\n\tval_reg_loss = 0.02850000001490116\n\tval_iou = 0.1294\n\tval_accuracy = 1.0\n\nIteration #:  40\n\ttrain_loss = 0.053199999034404755\n\ttrain_cls_loss = 0.15469999611377716\n\ttrain_reg_loss = 0.009700000286102295\n\ttrain_iou = 0.353\n\ttrain_accuracy = 1.0\n\tval_loss = 0.03970000147819519\n\tval_cls_loss = 0.10750000178813934\n\tval_reg_loss = 0.010700000450015068\n\tval_iou = 0.2995\n\tval_accuracy = 1.0\n\nIteration #:  50\n\ttrain_loss = 0.029899999499320984\n\ttrain_cls_loss = 0.08429999649524689\n\ttrain_reg_loss = 0.0066999997943639755\n\ttrain_iou = 0.3888\n\ttrain_accuracy = 1.0\n\tval_loss = 0.025699999183416367\n\tval_cls_loss = 0.058800000697374344\n\tval_reg_loss = 0.011500000022351742\n\tval_iou = 0.3226\n\tval_accuracy = 1.0\n\nIteration #:  60\n\ttrain_loss = 0.023099999874830246\n\ttrain_cls_loss = 0.05249999836087227\n\ttrain_reg_loss = 0.010499999858438969\n\ttrain_iou = 0.385\n\ttrain_accuracy = 1.0\n\tval_loss = 0.016899999231100082\n\tval_cls_loss = 0.0357000008225441\n\tval_reg_loss = 0.008799999952316284\n\tval_iou = 0.335\n\tval_accuracy = 1.0\n\nIteration #:  70\n\ttrain_loss = 0.01489999983459711\n\ttrain_cls_loss = 0.03689999878406525\n\ttrain_reg_loss = 0.005499999970197678\n\ttrain_iou = 0.4198\n\ttrain_accuracy = 1.0\n\tval_loss = 0.012799999676644802\n\tval_cls_loss = 0.023900000378489494\n\tval_reg_loss = 0.00800000037997961\n\tval_iou = 0.3471\n\tval_accuracy = 1.0\n\nIteration #:  80\n\ttrain_loss = 0.013399999588727951\n\ttrain_cls_loss = 0.029999999329447746\n\ttrain_reg_loss = 0.006300000008195639\n\ttrain_iou = 0.4161\n\ttrain_accuracy = 1.0\n\tval_loss = 0.01119999960064888\n\tval_cls_loss = 0.01679999940097332\n\tval_reg_loss = 0.008799999952316284\n\tval_iou = 0.3584\n\tval_accuracy = 1.0\n\nIteration #:  90\n\ttrain_loss = 0.011099999770522118\n\ttrain_cls_loss = 0.02319999970495701\n\ttrain_reg_loss = 0.005900000222027302\n\ttrain_iou = 0.4429\n\ttrain_accuracy = 1.0\n\tval_loss = 0.007799999788403511\n\tval_cls_loss = 0.012199999764561653\n\tval_reg_loss = 0.005900000222027302\n\tval_iou = 0.423\n\tval_accuracy = 1.0\n\nIteration #:  100\n\ttrain_loss = 0.008999999612569809\n\ttrain_cls_loss = 0.01720000058412552\n\ttrain_reg_loss = 0.005499999970197678\n\ttrain_iou = 0.4539\n\ttrain_accuracy = 1.0\n\tval_loss = 0.007300000172108412\n\tval_cls_loss = 0.009600000455975533\n\tval_reg_loss = 0.006399999838322401\n\tval_iou = 0.3905\n\tval_accuracy = 1.0\n\nIteration #:  110\n\ttrain_loss = 0.007400000002235174\n\ttrain_cls_loss = 0.014499999582767487\n\ttrain_reg_loss = 0.004399999976158142\n\ttrain_iou = 0.4743\n\ttrain_accuracy = 1.0\n\tval_loss = 0.00559999980032444\n\tval_cls_loss = 0.007799999788403511\n\tval_reg_loss = 0.004600000102072954\n\tval_iou = 0.429\n\tval_accuracy = 1.0\n\nIteration #:  120\n\ttrain_loss = 0.0071000000461936\n\ttrain_cls_loss = 0.013000000268220901\n\ttrain_reg_loss = 0.004600000102072954\n\ttrain_iou = 0.4091\n\ttrain_accuracy = 1.0\n\tval_loss = 0.0044999998062849045\n\tval_cls_loss = 0.006300000008195639\n\tval_reg_loss = 0.003800000064074993\n\tval_iou = 0.4467\n\tval_accuracy = 1.0\n\nIteration #:  130\n\ttrain_loss = 0.006099999882280827\n\ttrain_cls_loss = 0.010200000368058681\n\ttrain_reg_loss = 0.00430000014603138\n\ttrain_iou = 0.4743\n\ttrain_accuracy = 1.0\n\tval_loss = 0.004100000020116568\n\tval_cls_loss = 0.0052999998442828655\n\tval_reg_loss = 0.0035000001080334187\n\tval_iou = 0.4738\n\tval_accuracy = 1.0\n\nIteration #:  140\n\ttrain_loss = 0.004600000102072954\n\ttrain_cls_loss = 0.008100000210106373\n\ttrain_reg_loss = 0.0031999999191612005\n\ttrain_iou = 0.4942\n\ttrain_accuracy = 1.0\n\tval_loss = 0.0034000000450760126\n\tval_cls_loss = 0.004600000102072954\n\tval_reg_loss = 0.002899999963119626\n\tval_iou = 0.4999\n\tval_accuracy = 1.0\n\nIteration #:  150\n\ttrain_loss = 0.005499999970197678\n\ttrain_cls_loss = 0.008200000040233135\n\ttrain_reg_loss = 0.00430000014603138\n\ttrain_iou = 0.4534\n\ttrain_accuracy = 1.0\n\tval_loss = 0.0034000000450760126\n\tval_cls_loss = 0.004100000020116568\n\tval_reg_loss = 0.003000000026077032\n\tval_iou = 0.4833\n\tval_accuracy = 1.0\n\nIteration #:  160\n\ttrain_loss = 0.0044999998062849045\n\ttrain_cls_loss = 0.006800000090152025\n\ttrain_reg_loss = 0.0035000001080334187\n\ttrain_iou = 0.4679\n\ttrain_accuracy = 1.0\n\tval_loss = 0.003100000089034438\n\tval_cls_loss = 0.0031999999191612005\n\tval_reg_loss = 0.003100000089034438\n\tval_iou = 0.4784\n\tval_accuracy = 1.0\n\nIteration #:  170\n\ttrain_loss = 0.00430000014603138\n\ttrain_cls_loss = 0.006899999920278788\n\ttrain_reg_loss = 0.003100000089034438\n\ttrain_iou = 0.473\n\ttrain_accuracy = 1.0\n\tval_loss = 0.00279999990016222\n\tval_cls_loss = 0.002899999963119626\n\tval_reg_loss = 0.0027000000700354576\n\tval_iou = 0.4836\n\tval_accuracy = 1.0\n\nIteration #:  180\n\ttrain_loss = 0.004999999888241291\n\ttrain_cls_loss = 0.005799999926239252\n\ttrain_reg_loss = 0.004699999932199717\n\ttrain_iou = 0.4581\n\ttrain_accuracy = 1.0\n\tval_loss = 0.00279999990016222\n\tval_cls_loss = 0.0026000000070780516\n\tval_reg_loss = 0.00279999990016222\n\tval_iou = 0.4925\n\tval_accuracy = 1.0\n\nIteration #:  190\n\ttrain_loss = 0.004900000058114529\n\ttrain_cls_loss = 0.004399999976158142\n\ttrain_reg_loss = 0.005100000184029341\n\ttrain_iou = 0.4619\n\ttrain_accuracy = 1.0\n\tval_loss = 0.0024999999441206455\n\tval_cls_loss = 0.002199999988079071\n\tval_reg_loss = 0.0027000000700354576\n\tval_iou = 0.5061\n\tval_accuracy = 1.0\n\nIteration #:  200\n\ttrain_loss = 0.004399999976158142\n\ttrain_cls_loss = 0.005400000140070915\n\ttrain_reg_loss = 0.004000000189989805\n\ttrain_iou = 0.5041\n\ttrain_accuracy = 1.0\n\tval_loss = 0.002099999925121665\n\tval_cls_loss = 0.0020000000949949026\n\tval_reg_loss = 0.002099999925121665\n\tval_iou = 0.5282\n\tval_accuracy = 1.0\n\nIteration #:  210\n\ttrain_loss = 0.004600000102072954\n\ttrain_cls_loss = 0.004600000102072954\n\ttrain_reg_loss = 0.004699999932199717\n\ttrain_iou = 0.4732\n\ttrain_accuracy = 1.0\n\tval_loss = 0.0015999999595806003\n\tval_cls_loss = 0.0017000000225380063\n\tval_reg_loss = 0.001500000013038516\n\tval_iou = 0.5858\n\tval_accuracy = 1.0\n\nIteration #:  220\n\ttrain_loss = 0.0032999999821186066\n\ttrain_cls_loss = 0.0038999998942017555\n\ttrain_reg_loss = 0.003000000026077032\n\ttrain_iou = 0.5011\n\ttrain_accuracy = 1.0\n\tval_loss = 0.0017999999690800905\n\tval_cls_loss = 0.001500000013038516\n\tval_reg_loss = 0.0020000000949949026\n\tval_iou = 0.5106\n\tval_accuracy = 1.0\n\nIteration #:  230\n\ttrain_loss = 0.0027000000700354576\n\ttrain_cls_loss = 0.0032999999821186066\n\ttrain_reg_loss = 0.002400000113993883\n\ttrain_iou = 0.5094\n\ttrain_accuracy = 1.0\n\tval_loss = 0.00139999995008111\n\tval_cls_loss = 0.00139999995008111\n\tval_reg_loss = 0.00139999995008111\n\tval_iou = 0.5716\n\tval_accuracy = 1.0\n\nIteration #:  240\n\ttrain_loss = 0.0027000000700354576\n\ttrain_cls_loss = 0.003100000089034438\n\ttrain_reg_loss = 0.0024999999441206455\n\ttrain_iou = 0.5381\n\ttrain_accuracy = 1.0\n\tval_loss = 0.0017999999690800905\n\tval_cls_loss = 0.0013000000035390258\n\tval_reg_loss = 0.0020000000949949026\n\tval_iou = 0.5309\n\tval_accuracy = 1.0\n\nIteration #:  250\n\ttrain_loss = 0.002899999963119626\n\ttrain_cls_loss = 0.003000000026077032\n\ttrain_reg_loss = 0.002899999963119626\n\ttrain_iou = 0.5035\n\ttrain_accuracy = 1.0\n\tval_loss = 0.00139999995008111\n\tval_cls_loss = 0.0012000000569969416\n\tval_reg_loss = 0.001500000013038516\n\tval_iou = 0.5858\n\tval_accuracy = 1.0\n\nIteration #:  260\n\ttrain_loss = 0.002199999988079071\n\ttrain_cls_loss = 0.003100000089034438\n\ttrain_reg_loss = 0.0017999999690800905\n\ttrain_iou = 0.5666\n\ttrain_accuracy = 1.0\n\tval_loss = 0.0012000000569969416\n\tval_cls_loss = 0.0010999999940395355\n\tval_reg_loss = 0.0013000000035390258\n\tval_iou = 0.5821\n\tval_accuracy = 1.0\n\nIteration #:  270\n\ttrain_loss = 0.003000000026077032\n\ttrain_cls_loss = 0.00279999990016222\n\ttrain_reg_loss = 0.003100000089034438\n\ttrain_iou = 0.4794\n\ttrain_accuracy = 1.0\n\tval_loss = 0.002099999925121665\n\tval_cls_loss = 0.0008999999845400453\n\tval_reg_loss = 0.0024999999441206455\n\tval_iou = 0.5004\n\tval_accuracy = 1.0\n\nIteration #:  280\n\ttrain_loss = 0.0017000000225380063\n\ttrain_cls_loss = 0.002199999988079071\n\ttrain_reg_loss = 0.001500000013038516\n\ttrain_iou = 0.5749\n\ttrain_accuracy = 1.0\n\tval_loss = 0.0008999999845400453\n\tval_cls_loss = 0.0008999999845400453\n\tval_reg_loss = 0.0010000000474974513\n\tval_iou = 0.595\n\tval_accuracy = 1.0\n\nIteration #:  290\n\ttrain_loss = 0.002199999988079071\n\ttrain_cls_loss = 0.002300000051036477\n\ttrain_reg_loss = 0.002199999988079071\n\ttrain_iou = 0.511\n\ttrain_accuracy = 1.0\n\tval_loss = 0.0015999999595806003\n\tval_cls_loss = 0.0007999999797903001\n\tval_reg_loss = 0.0020000000949949026\n\tval_iou = 0.533\n\tval_accuracy = 1.0\n\nIteration #:  300\n\ttrain_loss = 0.002099999925121665\n\ttrain_cls_loss = 0.0019000000320374966\n\ttrain_reg_loss = 0.002099999925121665\n\ttrain_iou = 0.5505\n\ttrain_accuracy = 1.0\n\tval_loss = 0.0013000000035390258\n\tval_cls_loss = 0.000699999975040555\n\tval_reg_loss = 0.001500000013038516\n\tval_iou = 0.5445\n\tval_accuracy = 1.0\n\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"# Análisis de resultados .\n\nEl entrenamiento muestra estabilidad, con pérdidas de entrenamiento y validación bajas y exactitud de clasificación constante en 1.0, lo que indica que la cabeza de clasificación ha aprendido correctamente. Sin embargo, la métrica de IoU para la localización de cajas se mantiene en valores moderados (entre 0.50 y 0.56 en validación), lo que evidencia que la cabeza de regresión todavía necesita mejorar la precisión de las predicciones. En conjunto, el modelo se desempeña bien en clasificación, pero la localización requiere más refinamiento; no se observan signos de sobreajuste, y el aprendizaje es consistente, aunque aún incompleto en la tarea de predicción de coordenadas.","metadata":{"papermill":{"duration":0.026273,"end_time":"2025-08-27T02:46:24.605932","exception":false,"start_time":"2025-08-27T02:46:24.579659","status":"completed"},"tags":[]}},{"cell_type":"code","source":"num_imgs = 8\nncols = 8\nnrows = math.ceil(num_imgs / ncols)  # nº de filas para la grilla de visualización\n\nstart_idx = 0\n\n# ===========================\n# 1) CONSTRUIR LOTE DE INFERENCIA (SIN TRANSFORMS)\n# ===========================\n# Tomamos 'num_imgs' ejemplos del split de validación (val_df) para inferencia/visualización.\n# - root_dir: carpeta de imágenes originales.\n# - output_size=(w,h): aseguramos tamaño uniforme (p.ej., 640x640).\ninference_ds = maskDataset(val_df.iloc[start_idx:start_idx+num_imgs], root_dir=train_root_dir,output_size=(w,h))\n\n# DataLoader con batch = num_imgs para procesar todo el subconjunto de una vez (sin barajar).\ninference_data = DataLoader(inference_ds, batch_size=num_imgs, num_workers=1, shuffle=False)\n\n# Extraemos un batch (diccionario con 'image', 'bbox', 'class_id')\ninference_batch = next(iter(inference_data))\n\n# Preasignamos un arreglo donde guardaremos las imágenes YA transformadas a tensor (N, C, H, W)\ninference_imgs = np.empty((num_imgs, 3, h, w))\n\n# Usaremos las transformaciones de evaluación (ToTensor + Normalizer) definidas antes.\n# Estas esperan un 'sample' con clave 'image' y devuelven 'image' como tensor CxHxW normalizado.\ntransform = eval_transforms\n\n# ===========================\n# 2) APLICAR TRANSFORMACIONES DE EVAL A CADA IMAGEN DEL BATCH\n# ===========================\n# El DataLoader devuelve 'inference_batch[\"image\"]' como tensor (N, H, W, C) o arreglo convertible.\n# Recorremos por imagen, aplicamos eval_transforms y guardamos en 'inference_imgs' con forma (C,H,W).\nfor i, img in enumerate(inference_batch['image']):\n    # Convertimos a numpy (HxWxC) si viniera como tensor y aplicamos el wrapper de transforms\n    inference_imgs[i] = transform(dict(image=img.numpy()))['image'].numpy()\n\n# ===========================\n# 3) INFERENCIA CON EL MODELO\n# ===========================\n# Convertimos 'inference_imgs' a tensor float en el device (cuda/cpu) y pasamos por el modelo.\npreds = model(torch.tensor(inference_imgs).float().to(device))\n\n# ===========================\n# 4) PREPARAR ELEMENTOS PARA VISUALIZACIÓN (GT vs PRED)\n# ===========================\n# Tomamos las mismas muestras del Dataset (sin transforms) para dibujar imágenes originales.\nsamples = [inference_ds[i] for i in range(start_idx, num_imgs)]\n\n# Imágenes originales (numpy HxWxC)\nimgs = [s['image'] for s in samples]\n\n# BBoxes ground-truth en píxeles para dibujar:\n#  - s['bbox'] se asume normalizada [0,1]; la convertimos a píxeles con normalize_bbox(h,w).\nbboxes = [normalize_bbox(s['bbox'].squeeze(), h, w) for s in samples]\n\n# Clases ground-truth (enteros), tal como están en el sample.\nclasses = [s['class_id'] for s in samples]\n\n# ===========================\n# 5) POSTPROCESO DE PREDICCIONES\n# ===========================\n# Cajas predichas:\n#  - preds['bbox'] es un tensor [N,4] en la MISMA escala que las etiquetas (normalizada si entrenaste así).\n#  - Convertimos a numpy y a píxeles para dibujar.\npred_bboxes = preds['bbox'].detach().cpu().numpy()\npred_bboxes = [normalize_bbox(bbox, h, w) for bbox in pred_bboxes]\n\n# Clases predichas (logits → argmax). Resultado: ids de clase por imagen.\npred_classes = preds['class_id'].argmax(-1).detach().cpu().numpy()\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:56:15.315776Z","iopub.execute_input":"2025-09-26T01:56:15.316134Z","iopub.status.idle":"2025-09-26T01:56:15.497123Z","shell.execute_reply.started":"2025-09-26T01:56:15.316110Z","shell.execute_reply":"2025-09-26T01:56:15.496257Z"},"papermill":{"duration":0.191693,"end_time":"2025-08-27T02:46:24.937208","exception":false,"start_time":"2025-08-27T02:46:24.745515","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":69},{"cell_type":"code","source":"# ===========================\n# VISUALIZACIÓN: GT (verde) vs PRED (rojo) — versión robusta\n# ===========================\n\n# Determinar cuántos ejemplos hay realmente en cada lista/salida\nn = min(len(imgs), len(bboxes), len(pred_bboxes), len(pred_classes))\n\n# --- GT en VERDE ---\nimgs = draw_predictions(\n    imgs[:n], classes[:n], bboxes[:n],\n    [(0, 150, 0)], (int(w*0.1), int(h*0.1)),\n    thickness=1, fontScale=1\n)\n\n# --- PRED en ROJO ---\n# Adaptar clases predichas al formato esperado por draw_predictions\npred_classes_ = [np.array([c]) for c in pred_classes[:n]]\n\nimgs = draw_predictions(\n    imgs[:n], pred_classes_, pred_bboxes[:n],\n    [(200, 0, 0)], (int(w*0.8), int(h*0.8)),\n    thickness=1, fontScale=1\n)\n\n# --- GRID de visualización ---\n# Recalcular filas/columnas en función de n\nncols_eff = min(ncols, n)             # ncols original si cabe; si no, recorta\nnrows_eff = math.ceil(n / ncols_eff)\n\nfig, axes = plt.subplots(nrows=nrows_eff, ncols=ncols_eff, figsize=(30, 30))\n\n# Asegurar un iterable 1D de ejes\naxes_flat = np.array(axes).reshape(-1) if isinstance(axes, np.ndarray) else np.array([axes])\n\nfor i in range(n):\n    axes_flat[i].imshow(imgs[i])\n    axes_flat[i].axis('off')\n\n# Ocultar ejes sobrantes si la grilla es más grande que n\nfor j in range(n, len(axes_flat)):\n    axes_flat[j].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:56:15.498290Z","iopub.execute_input":"2025-09-26T01:56:15.498584Z"},"papermill":{"duration":1.011951,"end_time":"2025-08-27T02:46:25.976904","exception":false,"start_time":"2025-08-27T02:46:24.964953","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Guarda el **modelo completo** (arquitectura + pesos) en disco.\ntorch.save(model, 'own_model.pth')\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:56:16.269025Z","iopub.execute_input":"2025-09-26T01:56:16.269276Z","iopub.status.idle":"2025-09-26T01:56:16.325944Z"},"papermill":{"duration":0.475496,"end_time":"2025-08-27T02:46:26.510099","exception":false,"start_time":"2025-08-27T02:46:26.034603","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{"papermill":{"duration":0.052224,"end_time":"2025-08-27T02:46:26.618229","exception":false,"start_time":"2025-08-27T02:46:26.566005","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Detectar dispositivo\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Usando: {device}')\nmodel = model.to(device)\nmodel.eval()  # modo inferencia\n\n# Rutas y datos de test\ntest_root_dir = osp.join(DATA_DIR, \"images\")\ntest_df = pd.read_csv(osp.join(DATA_DIR, \"test.csv\"))\n\n# Dataset de test (usa tu clase correcta: maskDataset)\ntest_ds = maskDataset(\n    test_df,\n    root_dir=test_root_dir,\n    labeled=False,\n    transform=eval_transforms,\n    output_size=(w, h)\n)\n\n# DataLoader de test\ntest_data = DataLoader(test_ds, batch_size=1, num_workers=cpu_count(), shuffle=False)\n\n# Listas de salida\nclass_preds, bbox_preds = [], []\n\n# Bucle de inferencia\nwith torch.no_grad():\n    for batch in test_data:\n        imgs = batch['image'].float().to(device)\n        out = model(imgs)\n\n        # Predicciones\n        class_pred = out['class_id'].argmax(dim=-1).detach().cpu().numpy()\n        bbox_pred = out['bbox'].detach().cpu().numpy()\n\n        # Guardar\n        class_preds.append(class_pred.squeeze())\n        bbox_preds.append(bbox_pred.squeeze())","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:56:16.326815Z","iopub.execute_input":"2025-09-26T01:56:16.327097Z","iopub.status.idle":"2025-09-26T01:56:16.803299Z","shell.execute_reply.started":"2025-09-26T01:56:16.327079Z","shell.execute_reply":"2025-09-26T01:56:16.802083Z"},"papermill":{"duration":0.633161,"end_time":"2025-08-27T02:46:27.303654","exception":false,"start_time":"2025-08-27T02:46:26.670493","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Usando: cuda\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"# Convertir las listas de predicciones en arreglos de NumPy\n# Esto facilita operaciones vectorizadas y el posterior guardado en archivo de submission\nclass_preds = np.array(class_preds)   # Arreglo con las clases predichas (una por imagen)\nbbox_preds = np.array(bbox_preds)     # Arreglo con las cajas predichas (coordenadas por imagen)","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:56:16.804687Z","iopub.execute_input":"2025-09-26T01:56:16.805061Z","iopub.status.idle":"2025-09-26T01:56:16.813030Z","shell.execute_reply.started":"2025-09-26T01:56:16.805021Z","shell.execute_reply":"2025-09-26T01:56:16.812101Z"},"papermill":{"duration":0.056966,"end_time":"2025-08-27T02:46:27.411859","exception":false,"start_time":"2025-08-27T02:46:27.354893","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":73},{"cell_type":"code","source":"submission = pd.DataFrame(  \n    index=test_df.filename,   # Usar los nombres de archivo del conjunto de test como índice  \n    data={  \n        'class': class_preds,  # Columna con las clases predichas para cada imagen  \n        }  \n)  \nsubmission   # Mostrar el DataFrame de submission (con índice y columna de predicciones)  \n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:56:16.813972Z","iopub.execute_input":"2025-09-26T01:56:16.814275Z","iopub.status.idle":"2025-09-26T01:56:16.835772Z","shell.execute_reply.started":"2025-09-26T01:56:16.814250Z","shell.execute_reply":"2025-09-26T01:56:16.835028Z"},"papermill":{"duration":0.062155,"end_time":"2025-08-27T02:46:27.524226","exception":false,"start_time":"2025-08-27T02:46:27.462071","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"                                                    class\nfilename                                                 \nIMG_4861_mp4-50_jpg.rf.7173e37ed9f62f8939af8232...      0\nvideo_CDC-YOUTUBE_mp4-58_jpg.rf.370d5f316397477...      1\nvideo_CDC-YOUTUBE_mp4-57_jpg.rf.de4856b9a314980...      1\nIMG_3102_mp4-0_jpg.rf.6a18575fb4bf7f69cc9006b9a...      0\nIMG_3094_mp4-34_jpg.rf.11eecb9601680286dc8338d5...      0\nIMG_3100_mp4-22_jpg.rf.5cd5ee55e81838ff0a10d41a...      1\nIMG_4861_mp4-22_jpg.rf.287fa919e56ae28def307356...      0\n012106_jpg_1140x855_jpg.rf.b784fe385fa3967de70f...      0\nIMG_0873_mp4-5_jpg.rf.221b34cf6b46d41a81d674bae...      0\nIMG_3094_mp4-16_jpg.rf.dd59fa65d58061b3fd22e8b7...      1\nIMG_4849_mp4-46_jpg.rf.c842cbb15d9565f19550e733...      0\nApple-Tests-Face-ID-Feature-While-Wearing-a-Mas...      1\nvideo_CDC-YOUTUBE_mp4-39_jpg.rf.1c0863a2cdead89...      0\nIMG_4860_mp4-16_jpg.rf.411ff5da0837aff460014ee8...      0\nIMG_4860_mp4-12_jpg.rf.1781ffcadd06a00c8f5b114e...      0\nIMG_3093_mp4-6_jpg.rf.e6968ac27aad22a56f05d8756...      0\nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...      0\nTHE-HAVE-IT-ALL-TOUR-STARTS-TOMORROW-shorts-fun...      0\n1303078448-China-Coronavirus-Death-Toll-Hits-30...      1\nApple-Tests-Face-ID-Feature-While-Wearing-a-Mas...      1\nIMG_0871_mp4-19_jpg.rf.1aa53957d0f368529eb99586...      1\nIMG_4861_mp4-1_jpg.rf.7fe334aa62780ca9303dcde17...      0\nIMG_0873_MOV-1_jpg.rf.5ca65a952f300fb65b415a7ac...      0\nIMG_4861_mp4-6_jpg.rf.e0d60552d526ccd84863173fb...      0\nIMG_4860_mp4-19_jpg.rf.368a23fd16e38cce656dac90...      0\nIMG_3102_mp4-5_jpg.rf.41636f1d89a85858252bda24a...      0\nvideoplayback-1-_mp4-39_jpg.rf.c6b9de7158c88bef...      0\nIMG_6622_mp4-13_jpg.rf.209806c6f85e84169f1fd7fb...      1\nIMG_1492_mp4-22_jpg.rf.a8691873b03c8255550cd10b...      0\nIMG_3093_mp4-17_jpg.rf.b545e49d12fd70bba4cc71be...      1\nIMG_3100_mp4-24_jpg.rf.aa1cbaff65109302dccaaa9e...      0\nIMG_1493_mp4-21_jpg.rf.c5a3e237451e64e0674d5b0a...      1\nIMG_3100_mp4-27_jpg.rf.8bcc77cda09f4b865503adba...      0\nvideo_CDC-YOUTUBE_mp4-30_jpg.rf.a10191b12763868...      1\nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...      0\nIMG_4861_mp4-53_jpg.rf.507795a43df0b6ac554a94b9...      0\nIMG_4860_mp4-31_jpg.rf.19b88698cf8f74ee85b0e73b...      0\nIMG_4921-2_mp4-111_jpg.rf.92da68db4cb766bb76906...      1\nApple-Tests-Face-ID-Feature-While-Wearing-a-Mas...      0\nIMG_4920_mov-3_jpg.rf.d1fe4bf75ed7be7c97f85e4d1...      1\nIMG_0873_mp4-3_jpg.rf.46896bfd9b33387daf035555e...      0\nIMG_3100_mp4-17_jpg.rf.0ce2814d385f7d7e26535a4a...      1\nIMG_4861_mp4-4_jpg.rf.c34a8a22d84c641169322641c...      0\nvideoplayback-1-_mp4-3_jpg.rf.4688bf27f5538efb2...      0\nIMG_0871_MOV-22_jpg.rf.6ac73cf8114bae759970844e...      1\nIMG_4861_mp4-51_jpg.rf.42b1555d711c1936519f2fc7...      0\nIMG_4861_mp4-42_jpg.rf.cb9a4b243749f506b63bab24...      0\nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...      1\nIMG_0871_mp4-7_jpg.rf.4369c16792be03d03069063e5...      1\nvideoplayback-1-_mp4-34_jpg.rf.69baa5b11a706742...      0\nIMG_4860_mp4-44_jpg.rf.e2e6a8008d0cda580c49b57e...      0\nIMG_4869_mp4-6_jpg.rf.044ccf400303c0c5fc6a1aefd...      0\nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...      0\nIMG_4860_mp4-33_jpg.rf.df90e6f5b762f0500545210d...      0\nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n    </tr>\n    <tr>\n      <th>filename</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>IMG_4861_mp4-50_jpg.rf.7173e37ed9f62f8939af82323289faf2.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>video_CDC-YOUTUBE_mp4-58_jpg.rf.370d5f316397477da0ff4f44799b1da9.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>video_CDC-YOUTUBE_mp4-57_jpg.rf.de4856b9a314980e4113335576f453a8.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>IMG_3102_mp4-0_jpg.rf.6a18575fb4bf7f69cc9006b9a5f34e08.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_3094_mp4-34_jpg.rf.11eecb9601680286dc8338d5e8b9acb2.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_3100_mp4-22_jpg.rf.5cd5ee55e81838ff0a10d41a906e4811.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>IMG_4861_mp4-22_jpg.rf.287fa919e56ae28def30735639851d1c.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>012106_jpg_1140x855_jpg.rf.b784fe385fa3967de70f9d20c0c73429.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_0873_mp4-5_jpg.rf.221b34cf6b46d41a81d674baec581a14.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_3094_mp4-16_jpg.rf.dd59fa65d58061b3fd22e8b79eb3827e.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>IMG_4849_mp4-46_jpg.rf.c842cbb15d9565f19550e733ed736fee.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Apple-Tests-Face-ID-Feature-While-Wearing-a-Mask-Shorts_mp4-39_jpg.rf.c5e8c9e7da5c4833ebd428d97dd4cd33.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>video_CDC-YOUTUBE_mp4-39_jpg.rf.1c0863a2cdead89c96a5a00099001b5c.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_4860_mp4-16_jpg.rf.411ff5da0837aff460014ee8e5dc62a8.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_4860_mp4-12_jpg.rf.1781ffcadd06a00c8f5b114e4d23b5e9.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_3093_mp4-6_jpg.rf.e6968ac27aad22a56f05d8756bda12d1.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>How-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego-Health_mp4-97_jpg.rf.841de449bb52ecaa1ca2196e2f72cbc6.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>THE-HAVE-IT-ALL-TOUR-STARTS-TOMORROW-shorts-funny-standupcomedy-comedy-comedian-dating_mp4-9_jpg.rf.359da658b0999e912e729c38f5838f87.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1303078448-China-Coronavirus-Death-Toll-Hits-304_jpg.rf.a6f3455eb0648b7c226e3c91e329e943.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Apple-Tests-Face-ID-Feature-While-Wearing-a-Mask-Shorts_mp4-1_jpg.rf.b418c7f1d41950b7e19f55f334693169.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>IMG_0871_mp4-19_jpg.rf.1aa53957d0f368529eb995861f55adb3.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>IMG_4861_mp4-1_jpg.rf.7fe334aa62780ca9303dcde172d56410.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_0873_MOV-1_jpg.rf.5ca65a952f300fb65b415a7ac848f95a.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_4861_mp4-6_jpg.rf.e0d60552d526ccd84863173fb7470449.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_4860_mp4-19_jpg.rf.368a23fd16e38cce656dac90a6678dd4.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_3102_mp4-5_jpg.rf.41636f1d89a85858252bda24aa295060.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>videoplayback-1-_mp4-39_jpg.rf.c6b9de7158c88bef20d8a10530f88357.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_6622_mp4-13_jpg.rf.209806c6f85e84169f1fd7fb4327a2fc.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>IMG_1492_mp4-22_jpg.rf.a8691873b03c8255550cd10b525bb5e6.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_3093_mp4-17_jpg.rf.b545e49d12fd70bba4cc71be063bd7d0.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>IMG_3100_mp4-24_jpg.rf.aa1cbaff65109302dccaaa9ebec69e30.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_1493_mp4-21_jpg.rf.c5a3e237451e64e0674d5b0a6d556c25.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>IMG_3100_mp4-27_jpg.rf.8bcc77cda09f4b865503adba0492da64.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>video_CDC-YOUTUBE_mp4-30_jpg.rf.a10191b12763868462f995da60a12941.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>How-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego-Health_mp4-61_jpg.rf.e176cb291644e34d014d4391ff7c5e95.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_4861_mp4-53_jpg.rf.507795a43df0b6ac554a94b95896e6d8.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_4860_mp4-31_jpg.rf.19b88698cf8f74ee85b0e73b4e24f32c.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_4921-2_mp4-111_jpg.rf.92da68db4cb766bb76906d4b269b638d.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Apple-Tests-Face-ID-Feature-While-Wearing-a-Mask-Shorts_mp4-30_jpg.rf.9ae6cf145207b9047166b4e702e833c0.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_4920_mov-3_jpg.rf.d1fe4bf75ed7be7c97f85e4d1370eb18.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>IMG_0873_mp4-3_jpg.rf.46896bfd9b33387daf035555e8596720.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_3100_mp4-17_jpg.rf.0ce2814d385f7d7e26535a4a89b98033.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>IMG_4861_mp4-4_jpg.rf.c34a8a22d84c641169322641c10fa17a.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>videoplayback-1-_mp4-3_jpg.rf.4688bf27f5538efb2380ea2e81e70e84.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_0871_MOV-22_jpg.rf.6ac73cf8114bae759970844e54919a4c.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>IMG_4861_mp4-51_jpg.rf.42b1555d711c1936519f2fc7ed565aa0.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_4861_mp4-42_jpg.rf.cb9a4b243749f506b63bab242ce036f6.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>How-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego-Health_mp4-109_jpg.rf.f536f21562b648fa1711c5551cfe80b9.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>IMG_0871_mp4-7_jpg.rf.4369c16792be03d03069063e59a1c786.jpg</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>videoplayback-1-_mp4-34_jpg.rf.69baa5b11a7067427ab2218952047f7c.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_4860_mp4-44_jpg.rf.e2e6a8008d0cda580c49b57ea3c7e49d.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_4869_mp4-6_jpg.rf.044ccf400303c0c5fc6a1aefd6fc278b.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>How-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego-Health_mp4-62_jpg.rf.7a81ec327f644d6923c3f1a664458abb.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>IMG_4860_mp4-33_jpg.rf.df90e6f5b762f0500545210dc4c21438.jpg</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>How-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego-Health_mp4-115_jpg.rf.7fc798eb3d95d468a7d600a01a64febb.jpg</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":74},{"cell_type":"code","source":"submission[\"xmin\"] = bbox_preds[:, 0]*w_real  # Coordenada X mínima de la caja, escalada al ancho real de la imagen\nsubmission[\"ymin\"] = bbox_preds[:, 1]*h_real  # Coordenada Y mínima de la caja, escalada a la altura real de la imagen\nsubmission[\"xmax\"] = bbox_preds[:, 2]*w_real  # Coordenada X máxima de la caja, escalada al ancho real de la imagen\nsubmission[\"ymax\"] = bbox_preds[:, 3]*h_real  # Coordenada Y máxima de la caja, escalada a la altura real de la imagen\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:56:16.837451Z","iopub.execute_input":"2025-09-26T01:56:16.838335Z","iopub.status.idle":"2025-09-26T01:56:16.853919Z","shell.execute_reply.started":"2025-09-26T01:56:16.838303Z","shell.execute_reply":"2025-09-26T01:56:16.853018Z"},"papermill":{"duration":0.058243,"end_time":"2025-08-27T02:46:27.633517","exception":false,"start_time":"2025-08-27T02:46:27.575274","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":75},{"cell_type":"code","source":"submission['class'] = submission['class'].replace(id2obj)  # Reemplaza los IDs de clase numéricos por sus nombres/etiquetas reales usando el diccionario id2obj\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:56:16.854837Z","iopub.execute_input":"2025-09-26T01:56:16.855180Z","iopub.status.idle":"2025-09-26T01:56:16.877779Z","shell.execute_reply.started":"2025-09-26T01:56:16.855150Z","shell.execute_reply":"2025-09-26T01:56:16.876685Z"},"papermill":{"duration":0.058859,"end_time":"2025-08-27T02:46:27.74455","exception":false,"start_time":"2025-08-27T02:46:27.685691","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":76},{"cell_type":"code","source":"submission['class'].value_counts()  # Muestra la cantidad de predicciones por cada clase (frecuencia de cada etiqueta en el submission)\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:56:16.878845Z","iopub.execute_input":"2025-09-26T01:56:16.879136Z","iopub.status.idle":"2025-09-26T01:56:16.895353Z","shell.execute_reply.started":"2025-09-26T01:56:16.879113Z","shell.execute_reply":"2025-09-26T01:56:16.894444Z"},"papermill":{"duration":0.06454,"end_time":"2025-08-27T02:46:27.860011","exception":false,"start_time":"2025-08-27T02:46:27.795471","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"class\nno-mask    37\nmask       18\nName: count, dtype: int64"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"submission  # Muestra el DataFrame completo con las predicciones (clase y coordenadas de las cajas por imagen)\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:56:16.896125Z","iopub.execute_input":"2025-09-26T01:56:16.896419Z","iopub.status.idle":"2025-09-26T01:56:16.925651Z","shell.execute_reply.started":"2025-09-26T01:56:16.896395Z","shell.execute_reply":"2025-09-26T01:56:16.924921Z"},"papermill":{"duration":0.069181,"end_time":"2025-08-27T02:46:27.981064","exception":false,"start_time":"2025-08-27T02:46:27.911883","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"                                                      class        xmin  \\\nfilename                                                                  \nIMG_4861_mp4-50_jpg.rf.7173e37ed9f62f8939af8232...  no-mask  273.920013   \nvideo_CDC-YOUTUBE_mp4-58_jpg.rf.370d5f316397477...     mask  175.163544   \nvideo_CDC-YOUTUBE_mp4-57_jpg.rf.de4856b9a314980...     mask  209.645294   \nIMG_3102_mp4-0_jpg.rf.6a18575fb4bf7f69cc9006b9a...  no-mask  341.909332   \nIMG_3094_mp4-34_jpg.rf.11eecb9601680286dc8338d5...  no-mask  310.699402   \nIMG_3100_mp4-22_jpg.rf.5cd5ee55e81838ff0a10d41a...     mask  237.826996   \nIMG_4861_mp4-22_jpg.rf.287fa919e56ae28def307356...  no-mask  244.980850   \n012106_jpg_1140x855_jpg.rf.b784fe385fa3967de70f...  no-mask  300.765442   \nIMG_0873_mp4-5_jpg.rf.221b34cf6b46d41a81d674bae...  no-mask  301.980225   \nIMG_3094_mp4-16_jpg.rf.dd59fa65d58061b3fd22e8b7...     mask  277.256622   \nIMG_4849_mp4-46_jpg.rf.c842cbb15d9565f19550e733...  no-mask  282.658447   \nApple-Tests-Face-ID-Feature-While-Wearing-a-Mas...     mask  274.846466   \nvideo_CDC-YOUTUBE_mp4-39_jpg.rf.1c0863a2cdead89...  no-mask  233.358948   \nIMG_4860_mp4-16_jpg.rf.411ff5da0837aff460014ee8...  no-mask  354.323608   \nIMG_4860_mp4-12_jpg.rf.1781ffcadd06a00c8f5b114e...  no-mask  274.599182   \nIMG_3093_mp4-6_jpg.rf.e6968ac27aad22a56f05d8756...  no-mask  285.715271   \nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...  no-mask  355.169250   \nTHE-HAVE-IT-ALL-TOUR-STARTS-TOMORROW-shorts-fun...  no-mask  327.429352   \n1303078448-China-Coronavirus-Death-Toll-Hits-30...     mask  250.544434   \nApple-Tests-Face-ID-Feature-While-Wearing-a-Mas...     mask  248.159622   \nIMG_0871_mp4-19_jpg.rf.1aa53957d0f368529eb99586...     mask  215.980515   \nIMG_4861_mp4-1_jpg.rf.7fe334aa62780ca9303dcde17...  no-mask  275.717651   \nIMG_0873_MOV-1_jpg.rf.5ca65a952f300fb65b415a7ac...  no-mask  323.608398   \nIMG_4861_mp4-6_jpg.rf.e0d60552d526ccd84863173fb...  no-mask  320.395233   \nIMG_4860_mp4-19_jpg.rf.368a23fd16e38cce656dac90...  no-mask  362.015350   \nIMG_3102_mp4-5_jpg.rf.41636f1d89a85858252bda24a...  no-mask  309.002411   \nvideoplayback-1-_mp4-39_jpg.rf.c6b9de7158c88bef...  no-mask  403.379944   \nIMG_6622_mp4-13_jpg.rf.209806c6f85e84169f1fd7fb...     mask  255.973129   \nIMG_1492_mp4-22_jpg.rf.a8691873b03c8255550cd10b...  no-mask  292.805511   \nIMG_3093_mp4-17_jpg.rf.b545e49d12fd70bba4cc71be...     mask  248.043350   \nIMG_3100_mp4-24_jpg.rf.aa1cbaff65109302dccaaa9e...  no-mask  277.826935   \nIMG_1493_mp4-21_jpg.rf.c5a3e237451e64e0674d5b0a...     mask  272.847473   \nIMG_3100_mp4-27_jpg.rf.8bcc77cda09f4b865503adba...  no-mask  310.426758   \nvideo_CDC-YOUTUBE_mp4-30_jpg.rf.a10191b12763868...     mask  248.164886   \nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...  no-mask  333.929993   \nIMG_4861_mp4-53_jpg.rf.507795a43df0b6ac554a94b9...  no-mask  295.851562   \nIMG_4860_mp4-31_jpg.rf.19b88698cf8f74ee85b0e73b...  no-mask  278.412903   \nIMG_4921-2_mp4-111_jpg.rf.92da68db4cb766bb76906...     mask  268.514435   \nApple-Tests-Face-ID-Feature-While-Wearing-a-Mas...  no-mask  315.224823   \nIMG_4920_mov-3_jpg.rf.d1fe4bf75ed7be7c97f85e4d1...     mask  247.284103   \nIMG_0873_mp4-3_jpg.rf.46896bfd9b33387daf035555e...  no-mask  289.657166   \nIMG_3100_mp4-17_jpg.rf.0ce2814d385f7d7e26535a4a...     mask  193.867203   \nIMG_4861_mp4-4_jpg.rf.c34a8a22d84c641169322641c...  no-mask  253.070526   \nvideoplayback-1-_mp4-3_jpg.rf.4688bf27f5538efb2...  no-mask  397.312347   \nIMG_0871_MOV-22_jpg.rf.6ac73cf8114bae759970844e...     mask  204.989716   \nIMG_4861_mp4-51_jpg.rf.42b1555d711c1936519f2fc7...  no-mask  304.773285   \nIMG_4861_mp4-42_jpg.rf.cb9a4b243749f506b63bab24...  no-mask  273.509521   \nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...     mask  238.302338   \nIMG_0871_mp4-7_jpg.rf.4369c16792be03d03069063e5...     mask  209.090851   \nvideoplayback-1-_mp4-34_jpg.rf.69baa5b11a706742...  no-mask  378.388916   \nIMG_4860_mp4-44_jpg.rf.e2e6a8008d0cda580c49b57e...  no-mask  322.888641   \nIMG_4869_mp4-6_jpg.rf.044ccf400303c0c5fc6a1aefd...  no-mask  291.401947   \nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...  no-mask  311.613953   \nIMG_4860_mp4-33_jpg.rf.df90e6f5b762f0500545210d...  no-mask  296.681641   \nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...  no-mask  205.803238   \n\n                                                          ymin        xmax  \\\nfilename                                                                     \nIMG_4861_mp4-50_jpg.rf.7173e37ed9f62f8939af8232...  184.663666  579.523804   \nvideo_CDC-YOUTUBE_mp4-58_jpg.rf.370d5f316397477...  174.513687  246.200333   \nvideo_CDC-YOUTUBE_mp4-57_jpg.rf.de4856b9a314980...  192.422165  296.972473   \nIMG_3102_mp4-0_jpg.rf.6a18575fb4bf7f69cc9006b9a...  204.848999  624.976562   \nIMG_3094_mp4-34_jpg.rf.11eecb9601680286dc8338d5...  197.130920  710.530029   \nIMG_3100_mp4-22_jpg.rf.5cd5ee55e81838ff0a10d41a...  178.613312  518.731628   \nIMG_4861_mp4-22_jpg.rf.287fa919e56ae28def307356...  206.825272  567.567505   \n012106_jpg_1140x855_jpg.rf.b784fe385fa3967de70f...  145.194336  385.305908   \nIMG_0873_mp4-5_jpg.rf.221b34cf6b46d41a81d674bae...  187.595444  705.036682   \nIMG_3094_mp4-16_jpg.rf.dd59fa65d58061b3fd22e8b7...  176.870285  684.719116   \nIMG_4849_mp4-46_jpg.rf.c842cbb15d9565f19550e733...  230.155640  649.875732   \nApple-Tests-Face-ID-Feature-While-Wearing-a-Mas...   85.167351  357.545197   \nvideo_CDC-YOUTUBE_mp4-39_jpg.rf.1c0863a2cdead89...  218.847076  283.927124   \nIMG_4860_mp4-16_jpg.rf.411ff5da0837aff460014ee8...  222.433289  576.944031   \nIMG_4860_mp4-12_jpg.rf.1781ffcadd06a00c8f5b114e...  168.352692  540.666870   \nIMG_3093_mp4-6_jpg.rf.e6968ac27aad22a56f05d8756...  215.268921  708.283752   \nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...  184.194824  712.801392   \nTHE-HAVE-IT-ALL-TOUR-STARTS-TOMORROW-shorts-fun...  174.739548  524.889526   \n1303078448-China-Coronavirus-Death-Toll-Hits-30...   56.040276  353.918213   \nApple-Tests-Face-ID-Feature-While-Wearing-a-Mas...  104.785110  485.758972   \nIMG_0871_mp4-19_jpg.rf.1aa53957d0f368529eb99586...  112.058418  622.212463   \nIMG_4861_mp4-1_jpg.rf.7fe334aa62780ca9303dcde17...  224.146713  608.916992   \nIMG_0873_MOV-1_jpg.rf.5ca65a952f300fb65b415a7ac...  183.754196  709.778076   \nIMG_4861_mp4-6_jpg.rf.e0d60552d526ccd84863173fb...  185.141113  620.549194   \nIMG_4860_mp4-19_jpg.rf.368a23fd16e38cce656dac90...  221.667145  552.076355   \nIMG_3102_mp4-5_jpg.rf.41636f1d89a85858252bda24a...  221.029373  629.327026   \nvideoplayback-1-_mp4-39_jpg.rf.c6b9de7158c88bef...  160.813583  453.851562   \nIMG_6622_mp4-13_jpg.rf.209806c6f85e84169f1fd7fb...  161.696701  655.240845   \nIMG_1492_mp4-22_jpg.rf.a8691873b03c8255550cd10b...  198.995132  678.092407   \nIMG_3093_mp4-17_jpg.rf.b545e49d12fd70bba4cc71be...  169.044250  639.026306   \nIMG_3100_mp4-24_jpg.rf.aa1cbaff65109302dccaaa9e...  188.068909  682.259277   \nIMG_1493_mp4-21_jpg.rf.c5a3e237451e64e0674d5b0a...  191.637100  653.586426   \nIMG_3100_mp4-27_jpg.rf.8bcc77cda09f4b865503adba...  156.765015  561.598328   \nvideo_CDC-YOUTUBE_mp4-30_jpg.rf.a10191b12763868...  222.915787  391.890533   \nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...  161.358368  524.864868   \nIMG_4861_mp4-53_jpg.rf.507795a43df0b6ac554a94b9...  199.097214  592.106873   \nIMG_4860_mp4-31_jpg.rf.19b88698cf8f74ee85b0e73b...  177.218567  505.116913   \nIMG_4921-2_mp4-111_jpg.rf.92da68db4cb766bb76906...  196.966919  652.389648   \nApple-Tests-Face-ID-Feature-While-Wearing-a-Mas...  156.178986  548.743530   \nIMG_4920_mov-3_jpg.rf.d1fe4bf75ed7be7c97f85e4d1...  193.195724  644.505249   \nIMG_0873_mp4-3_jpg.rf.46896bfd9b33387daf035555e...  192.532684  694.468384   \nIMG_3100_mp4-17_jpg.rf.0ce2814d385f7d7e26535a4a...  146.078781  476.946472   \nIMG_4861_mp4-4_jpg.rf.c34a8a22d84c641169322641c...  216.787109  548.042358   \nvideoplayback-1-_mp4-3_jpg.rf.4688bf27f5538efb2...  163.250946  437.340240   \nIMG_0871_MOV-22_jpg.rf.6ac73cf8114bae759970844e...  119.782516  608.851746   \nIMG_4861_mp4-51_jpg.rf.42b1555d711c1936519f2fc7...  196.519028  619.744446   \nIMG_4861_mp4-42_jpg.rf.cb9a4b243749f506b63bab24...  228.016129  602.233337   \nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...  146.357300  504.492157   \nIMG_0871_mp4-7_jpg.rf.4369c16792be03d03069063e5...  117.682770  639.979248   \nvideoplayback-1-_mp4-34_jpg.rf.69baa5b11a706742...  149.369034  430.423157   \nIMG_4860_mp4-44_jpg.rf.e2e6a8008d0cda580c49b57e...  172.404190  484.032166   \nIMG_4869_mp4-6_jpg.rf.044ccf400303c0c5fc6a1aefd...  232.663101  619.348022   \nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...  195.567719  655.900024   \nIMG_4860_mp4-33_jpg.rf.df90e6f5b762f0500545210d...  189.413895  488.685272   \nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...  204.422546  484.862091   \n\n                                                          ymax  \nfilename                                                        \nIMG_4861_mp4-50_jpg.rf.7173e37ed9f62f8939af8232...  502.576965  \nvideo_CDC-YOUTUBE_mp4-58_jpg.rf.370d5f316397477...  287.917755  \nvideo_CDC-YOUTUBE_mp4-57_jpg.rf.de4856b9a314980...  321.693115  \nIMG_3102_mp4-0_jpg.rf.6a18575fb4bf7f69cc9006b9a...  494.601288  \nIMG_3094_mp4-34_jpg.rf.11eecb9601680286dc8338d5...  552.830566  \nIMG_3100_mp4-22_jpg.rf.5cd5ee55e81838ff0a10d41a...  496.446503  \nIMG_4861_mp4-22_jpg.rf.287fa919e56ae28def307356...  470.220398  \n012106_jpg_1140x855_jpg.rf.b784fe385fa3967de70f...  335.132568  \nIMG_0873_mp4-5_jpg.rf.221b34cf6b46d41a81d674bae...  538.885742  \nIMG_3094_mp4-16_jpg.rf.dd59fa65d58061b3fd22e8b7...  546.784912  \nIMG_4849_mp4-46_jpg.rf.c842cbb15d9565f19550e733...  544.886658  \nApple-Tests-Face-ID-Feature-While-Wearing-a-Mas...  199.425446  \nvideo_CDC-YOUTUBE_mp4-39_jpg.rf.1c0863a2cdead89...  310.159332  \nIMG_4860_mp4-16_jpg.rf.411ff5da0837aff460014ee8...  473.774872  \nIMG_4860_mp4-12_jpg.rf.1781ffcadd06a00c8f5b114e...  484.391510  \nIMG_3093_mp4-6_jpg.rf.e6968ac27aad22a56f05d8756...  539.457336  \nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...  554.693481  \nTHE-HAVE-IT-ALL-TOUR-STARTS-TOMORROW-shorts-fun...  372.227783  \n1303078448-China-Coronavirus-Death-Toll-Hits-30...  230.100586  \nApple-Tests-Face-ID-Feature-While-Wearing-a-Mas...  411.990051  \nIMG_0871_mp4-19_jpg.rf.1aa53957d0f368529eb99586...  463.523224  \nIMG_4861_mp4-1_jpg.rf.7fe334aa62780ca9303dcde17...  459.334167  \nIMG_0873_MOV-1_jpg.rf.5ca65a952f300fb65b415a7ac...  555.628357  \nIMG_4861_mp4-6_jpg.rf.e0d60552d526ccd84863173fb...  454.745178  \nIMG_4860_mp4-19_jpg.rf.368a23fd16e38cce656dac90...  454.475250  \nIMG_3102_mp4-5_jpg.rf.41636f1d89a85858252bda24a...  527.832947  \nvideoplayback-1-_mp4-39_jpg.rf.c6b9de7158c88bef...  198.787918  \nIMG_6622_mp4-13_jpg.rf.209806c6f85e84169f1fd7fb...  530.536255  \nIMG_1492_mp4-22_jpg.rf.a8691873b03c8255550cd10b...  546.485962  \nIMG_3093_mp4-17_jpg.rf.b545e49d12fd70bba4cc71be...  516.347229  \nIMG_3100_mp4-24_jpg.rf.aa1cbaff65109302dccaaa9e...  553.760254  \nIMG_1493_mp4-21_jpg.rf.c5a3e237451e64e0674d5b0a...  554.521606  \nIMG_3100_mp4-27_jpg.rf.8bcc77cda09f4b865503adba...  480.451935  \nvideo_CDC-YOUTUBE_mp4-30_jpg.rf.a10191b12763868...  319.432251  \nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...  467.984772  \nIMG_4861_mp4-53_jpg.rf.507795a43df0b6ac554a94b9...  468.912506  \nIMG_4860_mp4-31_jpg.rf.19b88698cf8f74ee85b0e73b...  366.790009  \nIMG_4921-2_mp4-111_jpg.rf.92da68db4cb766bb76906...  554.964722  \nApple-Tests-Face-ID-Feature-While-Wearing-a-Mas...  470.807343  \nIMG_4920_mov-3_jpg.rf.d1fe4bf75ed7be7c97f85e4d1...  540.303467  \nIMG_0873_mp4-3_jpg.rf.46896bfd9b33387daf035555e...  551.785522  \nIMG_3100_mp4-17_jpg.rf.0ce2814d385f7d7e26535a4a...  451.990295  \nIMG_4861_mp4-4_jpg.rf.c34a8a22d84c641169322641c...  463.866852  \nvideoplayback-1-_mp4-3_jpg.rf.4688bf27f5538efb2...  189.037033  \nIMG_0871_MOV-22_jpg.rf.6ac73cf8114bae759970844e...  459.132111  \nIMG_4861_mp4-51_jpg.rf.42b1555d711c1936519f2fc7...  492.846039  \nIMG_4861_mp4-42_jpg.rf.cb9a4b243749f506b63bab24...  459.890686  \nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...  521.689270  \nIMG_0871_mp4-7_jpg.rf.4369c16792be03d03069063e5...  473.552521  \nvideoplayback-1-_mp4-34_jpg.rf.69baa5b11a706742...  206.451691  \nIMG_4860_mp4-44_jpg.rf.e2e6a8008d0cda580c49b57e...  327.921875  \nIMG_4869_mp4-6_jpg.rf.044ccf400303c0c5fc6a1aefd...  500.059784  \nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...  543.688660  \nIMG_4860_mp4-33_jpg.rf.df90e6f5b762f0500545210d...  376.879822  \nHow-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego...  428.241547  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>xmax</th>\n      <th>ymax</th>\n    </tr>\n    <tr>\n      <th>filename</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>IMG_4861_mp4-50_jpg.rf.7173e37ed9f62f8939af82323289faf2.jpg</th>\n      <td>no-mask</td>\n      <td>273.920013</td>\n      <td>184.663666</td>\n      <td>579.523804</td>\n      <td>502.576965</td>\n    </tr>\n    <tr>\n      <th>video_CDC-YOUTUBE_mp4-58_jpg.rf.370d5f316397477da0ff4f44799b1da9.jpg</th>\n      <td>mask</td>\n      <td>175.163544</td>\n      <td>174.513687</td>\n      <td>246.200333</td>\n      <td>287.917755</td>\n    </tr>\n    <tr>\n      <th>video_CDC-YOUTUBE_mp4-57_jpg.rf.de4856b9a314980e4113335576f453a8.jpg</th>\n      <td>mask</td>\n      <td>209.645294</td>\n      <td>192.422165</td>\n      <td>296.972473</td>\n      <td>321.693115</td>\n    </tr>\n    <tr>\n      <th>IMG_3102_mp4-0_jpg.rf.6a18575fb4bf7f69cc9006b9a5f34e08.jpg</th>\n      <td>no-mask</td>\n      <td>341.909332</td>\n      <td>204.848999</td>\n      <td>624.976562</td>\n      <td>494.601288</td>\n    </tr>\n    <tr>\n      <th>IMG_3094_mp4-34_jpg.rf.11eecb9601680286dc8338d5e8b9acb2.jpg</th>\n      <td>no-mask</td>\n      <td>310.699402</td>\n      <td>197.130920</td>\n      <td>710.530029</td>\n      <td>552.830566</td>\n    </tr>\n    <tr>\n      <th>IMG_3100_mp4-22_jpg.rf.5cd5ee55e81838ff0a10d41a906e4811.jpg</th>\n      <td>mask</td>\n      <td>237.826996</td>\n      <td>178.613312</td>\n      <td>518.731628</td>\n      <td>496.446503</td>\n    </tr>\n    <tr>\n      <th>IMG_4861_mp4-22_jpg.rf.287fa919e56ae28def30735639851d1c.jpg</th>\n      <td>no-mask</td>\n      <td>244.980850</td>\n      <td>206.825272</td>\n      <td>567.567505</td>\n      <td>470.220398</td>\n    </tr>\n    <tr>\n      <th>012106_jpg_1140x855_jpg.rf.b784fe385fa3967de70f9d20c0c73429.jpg</th>\n      <td>no-mask</td>\n      <td>300.765442</td>\n      <td>145.194336</td>\n      <td>385.305908</td>\n      <td>335.132568</td>\n    </tr>\n    <tr>\n      <th>IMG_0873_mp4-5_jpg.rf.221b34cf6b46d41a81d674baec581a14.jpg</th>\n      <td>no-mask</td>\n      <td>301.980225</td>\n      <td>187.595444</td>\n      <td>705.036682</td>\n      <td>538.885742</td>\n    </tr>\n    <tr>\n      <th>IMG_3094_mp4-16_jpg.rf.dd59fa65d58061b3fd22e8b79eb3827e.jpg</th>\n      <td>mask</td>\n      <td>277.256622</td>\n      <td>176.870285</td>\n      <td>684.719116</td>\n      <td>546.784912</td>\n    </tr>\n    <tr>\n      <th>IMG_4849_mp4-46_jpg.rf.c842cbb15d9565f19550e733ed736fee.jpg</th>\n      <td>no-mask</td>\n      <td>282.658447</td>\n      <td>230.155640</td>\n      <td>649.875732</td>\n      <td>544.886658</td>\n    </tr>\n    <tr>\n      <th>Apple-Tests-Face-ID-Feature-While-Wearing-a-Mask-Shorts_mp4-39_jpg.rf.c5e8c9e7da5c4833ebd428d97dd4cd33.jpg</th>\n      <td>mask</td>\n      <td>274.846466</td>\n      <td>85.167351</td>\n      <td>357.545197</td>\n      <td>199.425446</td>\n    </tr>\n    <tr>\n      <th>video_CDC-YOUTUBE_mp4-39_jpg.rf.1c0863a2cdead89c96a5a00099001b5c.jpg</th>\n      <td>no-mask</td>\n      <td>233.358948</td>\n      <td>218.847076</td>\n      <td>283.927124</td>\n      <td>310.159332</td>\n    </tr>\n    <tr>\n      <th>IMG_4860_mp4-16_jpg.rf.411ff5da0837aff460014ee8e5dc62a8.jpg</th>\n      <td>no-mask</td>\n      <td>354.323608</td>\n      <td>222.433289</td>\n      <td>576.944031</td>\n      <td>473.774872</td>\n    </tr>\n    <tr>\n      <th>IMG_4860_mp4-12_jpg.rf.1781ffcadd06a00c8f5b114e4d23b5e9.jpg</th>\n      <td>no-mask</td>\n      <td>274.599182</td>\n      <td>168.352692</td>\n      <td>540.666870</td>\n      <td>484.391510</td>\n    </tr>\n    <tr>\n      <th>IMG_3093_mp4-6_jpg.rf.e6968ac27aad22a56f05d8756bda12d1.jpg</th>\n      <td>no-mask</td>\n      <td>285.715271</td>\n      <td>215.268921</td>\n      <td>708.283752</td>\n      <td>539.457336</td>\n    </tr>\n    <tr>\n      <th>How-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego-Health_mp4-97_jpg.rf.841de449bb52ecaa1ca2196e2f72cbc6.jpg</th>\n      <td>no-mask</td>\n      <td>355.169250</td>\n      <td>184.194824</td>\n      <td>712.801392</td>\n      <td>554.693481</td>\n    </tr>\n    <tr>\n      <th>THE-HAVE-IT-ALL-TOUR-STARTS-TOMORROW-shorts-funny-standupcomedy-comedy-comedian-dating_mp4-9_jpg.rf.359da658b0999e912e729c38f5838f87.jpg</th>\n      <td>no-mask</td>\n      <td>327.429352</td>\n      <td>174.739548</td>\n      <td>524.889526</td>\n      <td>372.227783</td>\n    </tr>\n    <tr>\n      <th>1303078448-China-Coronavirus-Death-Toll-Hits-304_jpg.rf.a6f3455eb0648b7c226e3c91e329e943.jpg</th>\n      <td>mask</td>\n      <td>250.544434</td>\n      <td>56.040276</td>\n      <td>353.918213</td>\n      <td>230.100586</td>\n    </tr>\n    <tr>\n      <th>Apple-Tests-Face-ID-Feature-While-Wearing-a-Mask-Shorts_mp4-1_jpg.rf.b418c7f1d41950b7e19f55f334693169.jpg</th>\n      <td>mask</td>\n      <td>248.159622</td>\n      <td>104.785110</td>\n      <td>485.758972</td>\n      <td>411.990051</td>\n    </tr>\n    <tr>\n      <th>IMG_0871_mp4-19_jpg.rf.1aa53957d0f368529eb995861f55adb3.jpg</th>\n      <td>mask</td>\n      <td>215.980515</td>\n      <td>112.058418</td>\n      <td>622.212463</td>\n      <td>463.523224</td>\n    </tr>\n    <tr>\n      <th>IMG_4861_mp4-1_jpg.rf.7fe334aa62780ca9303dcde172d56410.jpg</th>\n      <td>no-mask</td>\n      <td>275.717651</td>\n      <td>224.146713</td>\n      <td>608.916992</td>\n      <td>459.334167</td>\n    </tr>\n    <tr>\n      <th>IMG_0873_MOV-1_jpg.rf.5ca65a952f300fb65b415a7ac848f95a.jpg</th>\n      <td>no-mask</td>\n      <td>323.608398</td>\n      <td>183.754196</td>\n      <td>709.778076</td>\n      <td>555.628357</td>\n    </tr>\n    <tr>\n      <th>IMG_4861_mp4-6_jpg.rf.e0d60552d526ccd84863173fb7470449.jpg</th>\n      <td>no-mask</td>\n      <td>320.395233</td>\n      <td>185.141113</td>\n      <td>620.549194</td>\n      <td>454.745178</td>\n    </tr>\n    <tr>\n      <th>IMG_4860_mp4-19_jpg.rf.368a23fd16e38cce656dac90a6678dd4.jpg</th>\n      <td>no-mask</td>\n      <td>362.015350</td>\n      <td>221.667145</td>\n      <td>552.076355</td>\n      <td>454.475250</td>\n    </tr>\n    <tr>\n      <th>IMG_3102_mp4-5_jpg.rf.41636f1d89a85858252bda24aa295060.jpg</th>\n      <td>no-mask</td>\n      <td>309.002411</td>\n      <td>221.029373</td>\n      <td>629.327026</td>\n      <td>527.832947</td>\n    </tr>\n    <tr>\n      <th>videoplayback-1-_mp4-39_jpg.rf.c6b9de7158c88bef20d8a10530f88357.jpg</th>\n      <td>no-mask</td>\n      <td>403.379944</td>\n      <td>160.813583</td>\n      <td>453.851562</td>\n      <td>198.787918</td>\n    </tr>\n    <tr>\n      <th>IMG_6622_mp4-13_jpg.rf.209806c6f85e84169f1fd7fb4327a2fc.jpg</th>\n      <td>mask</td>\n      <td>255.973129</td>\n      <td>161.696701</td>\n      <td>655.240845</td>\n      <td>530.536255</td>\n    </tr>\n    <tr>\n      <th>IMG_1492_mp4-22_jpg.rf.a8691873b03c8255550cd10b525bb5e6.jpg</th>\n      <td>no-mask</td>\n      <td>292.805511</td>\n      <td>198.995132</td>\n      <td>678.092407</td>\n      <td>546.485962</td>\n    </tr>\n    <tr>\n      <th>IMG_3093_mp4-17_jpg.rf.b545e49d12fd70bba4cc71be063bd7d0.jpg</th>\n      <td>mask</td>\n      <td>248.043350</td>\n      <td>169.044250</td>\n      <td>639.026306</td>\n      <td>516.347229</td>\n    </tr>\n    <tr>\n      <th>IMG_3100_mp4-24_jpg.rf.aa1cbaff65109302dccaaa9ebec69e30.jpg</th>\n      <td>no-mask</td>\n      <td>277.826935</td>\n      <td>188.068909</td>\n      <td>682.259277</td>\n      <td>553.760254</td>\n    </tr>\n    <tr>\n      <th>IMG_1493_mp4-21_jpg.rf.c5a3e237451e64e0674d5b0a6d556c25.jpg</th>\n      <td>mask</td>\n      <td>272.847473</td>\n      <td>191.637100</td>\n      <td>653.586426</td>\n      <td>554.521606</td>\n    </tr>\n    <tr>\n      <th>IMG_3100_mp4-27_jpg.rf.8bcc77cda09f4b865503adba0492da64.jpg</th>\n      <td>no-mask</td>\n      <td>310.426758</td>\n      <td>156.765015</td>\n      <td>561.598328</td>\n      <td>480.451935</td>\n    </tr>\n    <tr>\n      <th>video_CDC-YOUTUBE_mp4-30_jpg.rf.a10191b12763868462f995da60a12941.jpg</th>\n      <td>mask</td>\n      <td>248.164886</td>\n      <td>222.915787</td>\n      <td>391.890533</td>\n      <td>319.432251</td>\n    </tr>\n    <tr>\n      <th>How-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego-Health_mp4-61_jpg.rf.e176cb291644e34d014d4391ff7c5e95.jpg</th>\n      <td>no-mask</td>\n      <td>333.929993</td>\n      <td>161.358368</td>\n      <td>524.864868</td>\n      <td>467.984772</td>\n    </tr>\n    <tr>\n      <th>IMG_4861_mp4-53_jpg.rf.507795a43df0b6ac554a94b95896e6d8.jpg</th>\n      <td>no-mask</td>\n      <td>295.851562</td>\n      <td>199.097214</td>\n      <td>592.106873</td>\n      <td>468.912506</td>\n    </tr>\n    <tr>\n      <th>IMG_4860_mp4-31_jpg.rf.19b88698cf8f74ee85b0e73b4e24f32c.jpg</th>\n      <td>no-mask</td>\n      <td>278.412903</td>\n      <td>177.218567</td>\n      <td>505.116913</td>\n      <td>366.790009</td>\n    </tr>\n    <tr>\n      <th>IMG_4921-2_mp4-111_jpg.rf.92da68db4cb766bb76906d4b269b638d.jpg</th>\n      <td>mask</td>\n      <td>268.514435</td>\n      <td>196.966919</td>\n      <td>652.389648</td>\n      <td>554.964722</td>\n    </tr>\n    <tr>\n      <th>Apple-Tests-Face-ID-Feature-While-Wearing-a-Mask-Shorts_mp4-30_jpg.rf.9ae6cf145207b9047166b4e702e833c0.jpg</th>\n      <td>no-mask</td>\n      <td>315.224823</td>\n      <td>156.178986</td>\n      <td>548.743530</td>\n      <td>470.807343</td>\n    </tr>\n    <tr>\n      <th>IMG_4920_mov-3_jpg.rf.d1fe4bf75ed7be7c97f85e4d1370eb18.jpg</th>\n      <td>mask</td>\n      <td>247.284103</td>\n      <td>193.195724</td>\n      <td>644.505249</td>\n      <td>540.303467</td>\n    </tr>\n    <tr>\n      <th>IMG_0873_mp4-3_jpg.rf.46896bfd9b33387daf035555e8596720.jpg</th>\n      <td>no-mask</td>\n      <td>289.657166</td>\n      <td>192.532684</td>\n      <td>694.468384</td>\n      <td>551.785522</td>\n    </tr>\n    <tr>\n      <th>IMG_3100_mp4-17_jpg.rf.0ce2814d385f7d7e26535a4a89b98033.jpg</th>\n      <td>mask</td>\n      <td>193.867203</td>\n      <td>146.078781</td>\n      <td>476.946472</td>\n      <td>451.990295</td>\n    </tr>\n    <tr>\n      <th>IMG_4861_mp4-4_jpg.rf.c34a8a22d84c641169322641c10fa17a.jpg</th>\n      <td>no-mask</td>\n      <td>253.070526</td>\n      <td>216.787109</td>\n      <td>548.042358</td>\n      <td>463.866852</td>\n    </tr>\n    <tr>\n      <th>videoplayback-1-_mp4-3_jpg.rf.4688bf27f5538efb2380ea2e81e70e84.jpg</th>\n      <td>no-mask</td>\n      <td>397.312347</td>\n      <td>163.250946</td>\n      <td>437.340240</td>\n      <td>189.037033</td>\n    </tr>\n    <tr>\n      <th>IMG_0871_MOV-22_jpg.rf.6ac73cf8114bae759970844e54919a4c.jpg</th>\n      <td>mask</td>\n      <td>204.989716</td>\n      <td>119.782516</td>\n      <td>608.851746</td>\n      <td>459.132111</td>\n    </tr>\n    <tr>\n      <th>IMG_4861_mp4-51_jpg.rf.42b1555d711c1936519f2fc7ed565aa0.jpg</th>\n      <td>no-mask</td>\n      <td>304.773285</td>\n      <td>196.519028</td>\n      <td>619.744446</td>\n      <td>492.846039</td>\n    </tr>\n    <tr>\n      <th>IMG_4861_mp4-42_jpg.rf.cb9a4b243749f506b63bab242ce036f6.jpg</th>\n      <td>no-mask</td>\n      <td>273.509521</td>\n      <td>228.016129</td>\n      <td>602.233337</td>\n      <td>459.890686</td>\n    </tr>\n    <tr>\n      <th>How-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego-Health_mp4-109_jpg.rf.f536f21562b648fa1711c5551cfe80b9.jpg</th>\n      <td>mask</td>\n      <td>238.302338</td>\n      <td>146.357300</td>\n      <td>504.492157</td>\n      <td>521.689270</td>\n    </tr>\n    <tr>\n      <th>IMG_0871_mp4-7_jpg.rf.4369c16792be03d03069063e59a1c786.jpg</th>\n      <td>mask</td>\n      <td>209.090851</td>\n      <td>117.682770</td>\n      <td>639.979248</td>\n      <td>473.552521</td>\n    </tr>\n    <tr>\n      <th>videoplayback-1-_mp4-34_jpg.rf.69baa5b11a7067427ab2218952047f7c.jpg</th>\n      <td>no-mask</td>\n      <td>378.388916</td>\n      <td>149.369034</td>\n      <td>430.423157</td>\n      <td>206.451691</td>\n    </tr>\n    <tr>\n      <th>IMG_4860_mp4-44_jpg.rf.e2e6a8008d0cda580c49b57ea3c7e49d.jpg</th>\n      <td>no-mask</td>\n      <td>322.888641</td>\n      <td>172.404190</td>\n      <td>484.032166</td>\n      <td>327.921875</td>\n    </tr>\n    <tr>\n      <th>IMG_4869_mp4-6_jpg.rf.044ccf400303c0c5fc6a1aefd6fc278b.jpg</th>\n      <td>no-mask</td>\n      <td>291.401947</td>\n      <td>232.663101</td>\n      <td>619.348022</td>\n      <td>500.059784</td>\n    </tr>\n    <tr>\n      <th>How-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego-Health_mp4-62_jpg.rf.7a81ec327f644d6923c3f1a664458abb.jpg</th>\n      <td>no-mask</td>\n      <td>311.613953</td>\n      <td>195.567719</td>\n      <td>655.900024</td>\n      <td>543.688660</td>\n    </tr>\n    <tr>\n      <th>IMG_4860_mp4-33_jpg.rf.df90e6f5b762f0500545210dc4c21438.jpg</th>\n      <td>no-mask</td>\n      <td>296.681641</td>\n      <td>189.413895</td>\n      <td>488.685272</td>\n      <td>376.879822</td>\n    </tr>\n    <tr>\n      <th>How-to-Properly-Wear-a-Face-Mask-_-UC-San-Diego-Health_mp4-115_jpg.rf.7fc798eb3d95d468a7d600a01a64febb.jpg</th>\n      <td>no-mask</td>\n      <td>205.803238</td>\n      <td>204.422546</td>\n      <td>484.862091</td>\n      <td>428.241547</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":78},{"cell_type":"code","source":"submission.to_csv('submission_own.csv')  # Exporta el DataFrame de submission a un archivo CSV con el nombre 'submission_vgg16.csv'\n","metadata":{"execution":{"iopub.status.busy":"2025-09-26T01:56:16.926189Z","iopub.execute_input":"2025-09-26T01:56:16.926476Z","iopub.status.idle":"2025-09-26T01:56:16.944336Z","shell.execute_reply.started":"2025-09-26T01:56:16.926452Z","shell.execute_reply":"2025-09-26T01:56:16.943363Z"},"papermill":{"duration":0.063727,"end_time":"2025-08-27T02:46:28.101643","exception":false,"start_time":"2025-08-27T02:46:28.037916","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":"# Conclusiones Generales","metadata":{}},{"cell_type":"markdown","source":"Arquitecturas\n=============\n\nSe evaluaron diferentes configuraciones arquitectónicas tomando como base **ResNet50\\_2** y **ResNet50\\_FPN**.\n\nEn el caso de **ResNet50\\_2**, se utilizaron las siguientes cabezas:\n\n*   **Clasificación**: Backbone → 768 → 256 → 2\n    \n*   **Regresión**: Backbone → 1024 → 512 → 256 → 4\n    \n\nEl modelo **ResNet50\\_FPN** mantuvo la misma cantidad de capas y neuronas por capa; las principales diferencias radicaron en el uso de **Dropout** y **BatchNorm**, además de cerrar la cabeza de regresión con una activación **Sigmoid**. Esta activación al final se agregó debido a que las coordenadas de los bounding boxes están normalizadas entre 0 y 1, por lo tanto, la función sigmoide evita que la red produzca coordenadas inválidas, además, puede mejorar la estabilidad en el entrenamiento pues las salidas pueden oscilar en rangos muy grandes al inicio del entrenamiento, lo que puede ralentizar o inestabilizar la convergencia.\n\nPor otro lado, para el modelo propuesto de diseño propio, la arquitectura inicial no proporcionó resultados satisfactorios. En consecuencia, se ajustó a la siguiente configuración:\n\n*   **Clasificación**: Backbone → 1024 → 512 → 256 → 2\n    \n*   **Regresión**: Backbone → 1024 → 512 → 256 → 128 → 4\n    \n\nEn ausencia de un backbone pre-entrenado, la inicialización de los pesos del modelo propio se realizó utilizando el método He Normal. Esta elección resultó en una mejora sustancial del desempeño respecto a las inicializaciones tradicionales en cero o con valores aleatorios, evidenciando la importancia de una estrategia de inicialización adecuada en el proceso de entrenamiento\n\nLa cantidad de capas en la arquitectura se definió como resultado de diferentes experimentos y comparación de resultados, buscando el mejor equilibrio entre capacidad de representación y generalización del modelo. Además, para los tres modelos siempre definimos una cabeza de clasificación con una menor cantidad de capas que la de regresión debido a que la segunda representa un problema más complejo que implica usar una arquitectura más profunda.\n\nFunción de pérdida\n==================\n\nSe implementaron variaciones en dos aspectos principales:\n\n1.  **Alpha (α):** factor de ponderación para equilibrar la importancia relativa entre la tarea de clasificación y la de regresión.\n    \n2.  **Pesos de clase:** ajuste de ponderaciones para compensar el desbalance del conjunto de datos.\n    \n\nEn relación con **α**, se evaluaron valores por debajo y por encima de 0.5. Para valores **< 0.5**, se observó un desempeño deficiente en la regresión, dado que la clasificación alcanzaba rápidamente un _accuracy_ cercano a 1.0 antes de obtener un IoU aceptable. Bajo estas condiciones, la regresión quedaba subentrenada.\n\nPara valores **≥ 0.9**, el rendimiento global del sistema disminuye: tanto la clasificación como la regresión presentaban dificultades para converger adecuadamente. Los mejores resultados se obtuvieron con valores de **α entre 0.6 y 0.7**, logrando un balance más estable entre ambas tareas.\n\nEn cuanto a los **pesos de clase** (_mask/no mask_), se realizaron experimentos en torno a las proporciones reales del dataset (38.4% / 61.6%). Las configuraciones que proporcionaron mejor desempeño fueron **35% / 65%**, mostrando que las variaciones en esta proporción afectan de manera significativa no sólo las métricas de clasificación, sino también las de regresión.\n\nTransformación de imágenes\n==========================\n\nPara el modelo basado en **ResNet50**, se emplearon inicialmente únicamente dos transformaciones sobre las imágenes de entrenamiento, a partir de las cuales se inició el proceso de optimización. Al incorporar transformaciones adicionales, el rendimiento del modelo se deterioró de manera sistemática, sin observar mejoras en las métricas.\n\nEn consecuencia, los dos modelos posteriores se entrenaron desde el inicio con **múltiples transformaciones**, evitando las limitaciones encontradas en el enfoque inicial.\n\nSe aplicó **MotionBlur** para simular imágenes borrosas, **RandomBrightnessContrast** para reflejar escenarios con mucha o poca iluminación y transformaciones de escalado para reconocer objetos en diferentes tamaños. Una opción descartada fueron las rotaciones, debido a que al analizar los resultados se identificaron inconsistencias en los _bounding boxes_.\n\nEntrenamiento y optimización\n============================\n\nSe habilitó explícitamente el uso de model.eval() y model.train() debido a la presencia de capas **Dropout** y **BatchNorm** en las arquitecturas evaluadas.\n\nSe utilizaron valores elevados de dropout en la cabeza de clasificación porque alcanzaba una alta precisión en pocas iteraciones, lo que indicaba un riesgo de sobreajuste. Al incrementar el dropout, se ralentiza el aprendizaje de la clasificación, permitiendo que la cabeza de regresión tenga más tiempo para optimizarse.  Así, se mejora la generalización del modelo y se evita que la clasificación domine el proceso de aprendizaje\n\nRespecto al **batch size**, se exploró un rango entre 8 y 126. Los tamaños extremos (pequeños o grandes) resultaron en problemas de memoria o en modelos poco estables. Los mejores resultados se obtuvieron consistentemente con un **batch size = 32**, mientras que con 16 también se lograron desempeños aceptables.\n\nEl **learning rate** se ajustó de manera diferenciada entre el _backbone_, la cabeza de clasificación y la de regresión, lo cual permitió un mejor control sobre el aprendizaje. Esto fue particularmente importante debido a que la clasificación tendía a converger significativamente más rápido que la regresión.\n\nFinalmente, se implementaron dos estrategias de optimización:\n\n1.  **Ciclos de entrenamiento dependientes del batch size**, siguiendo el planteamiento del documento base.\n    \n2.  **Control de ciclos basado en épocas definidas explícitamente**, estrategia que introdujo mejoras en estabilidad y resultados. Durante el entrenamiento observamos que, aunque el modelo completaba el número de épocas establecido, el resultado final no siempre correspondía con el mejor desempeño alcanzado en el proceso. Este comportamiento nos llevó a implementar la técnica de **Early Stopping**, con una paciencia de 10 iteraciones, utilizando como criterio la minimización de la función de pérdida (Loss). De esta manera, aseguramos conservar el modelo con mejor rendimiento y evitamos un sobreentrenamiento innecesario.\n    \n\nEn conclusión, tras realizar múltiples pruebas, el modelo que obtuvo el mejor desempeño —reflejado en el menor valor de loss y el mayor IoU— fue el **ResNet50\\_FPN**. En segundo lugar se ubicó el modelo **ResNet50\\_2**, mientras que el modelo propio presentó los resultados menos favorables.","metadata":{}}]}